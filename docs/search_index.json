[["index.html", "Fall 2020 EDAV Community Contributions Chapter 1 Instructions 1.1 Background 1.2 Preparing your .Rmd file 1.3 Submission steps 1.4 Optional tweaks 1.5 FAQ", " Fall 2020 EDAV Community Contributions 2020-11-26 Chapter 1 Instructions This chapter gives you all the information you need to upload your community contribution. Please read this entire document carefully before making your submission. Of particular note is the fact that bookdown requires a different .Rmd format than you’re used to, so you must make changes to the beginning of the file as described below before submitting. 1.1 Background This web site makes use of the bookdown package to render a collection of .Rmd files into a nicely formatted online book with chapters and subchapters. Your job will be to submit a slightly modified version of your community contribution .Rmd file to the GitHub repository where the source files for this web site are stored. On the backend, the admins will divide the chapters into book sections and order them. If your community contribution is in a different format, then create a short .Rmd file that explains what you did, and includes links to any relevant files, such as slides, etc. which you can post on your GitHub repo (or another online site.) 1.2 Preparing your .Rmd file You should only submit ONE Rmd file. After completing these modifications, your .Rmd should look like this sample .Rmd. Create a concise, descriptive name for your project. For instance, name it base_r_ggplot_graph or something similar if your work is about contrasting/working with base R graphics and ggplot2 graphics. Check the .Rmd filenames in the project repo to make sure your name isn’t already taken. Your project name should be words only and joined with underscores, no white space. In addition, all letters must be lowercase. Create a copy of your .Rmd file with the new name. Completely delete the YAML header (the section at the top of the .Rmd that includes name, title, date, output, etc.) including the --- line. Choose a short, descriptive, human readable title for your project as your title will show up in the table of contents – look at examples in the 2019 EDAV rendered book of community contributions https://jtr13.github.io/cc19. Capitalize the first letter only (“sentence case”). On the first line of your document, enter a single hashtag, followed by a single whitespace, and then your title. It is important to follow this format so that bookdown renders your title as a header. Do not use single # headers anywhere else in the document. Note: if your chapter is the first in a section, a second # header and section description will be requested during the pull-request and merge phase. The second line should be blank, followed by your name(s): # Base R vs. ggplot2 Aaron Burr and Alexander Hamilton Your content starts here. If your project requires data, please use a built-in dataset or read directly from a URL, such as: df &lt;- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\") If you absolutely must include a data file, please use a small one, as for many reasons it is desirable to keep the repository size as small as possible. If you have included a setup chunk in your Rmd file, please remember to remove the label setup in the chunk, i.e., use: {r, include=FALSE} instead of: {r setup, include=FALSE} If your project requires libraries to be installed and included in the document, please adhere to the following conventions. Do not evaluate any install.packages() statements in your document. Consumers of an Rmd file won’t want packages to get installed when they knit your document. Include any library() statements at the top of your Rmd file, below the title, name, and setup, but above any content. If your chapter requires the installation of a package from source (is a GitHub installation), please add a comment identifying it as such. Here is an example library() section with install statements that won’t be evaluated: install.packages(&quot;devtools&quot;) # used for installation from source install.packages(&quot;dplyr&quot;) install.packages(&quot;ggplot2&quot;) library(&quot;devtools&quot;) devtools::install_github(&quot;twitter/AnomalyDetection&quot;) library(&quot;AnomalyDetection&quot;) # must be installed from source library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) You could include both sections, or you could just include the second library() section and trust R users to install any packages themselves. If you developed your Rmd file before moving your library() statements above the rest of the file content, it is highly recommended to knit and review your document again. This may change the namespace that was available in each section of your code during development, causing a function not to work or to exhibit unexpected behavior. This common issue is mentioned in the EDAV R Basics “Functions stop working” section. Your file should not contain getwd() / setwd() calls (you should never use these in scripts anyway!) nor write statements. Want to get fancy? See the optional tweaks section below. 1.3 Submission steps To submit your work, we will be following “Workflow #4” – that is submitting a pull request to someone else’s repository to which you do not have write access. Instructions are available in this tutorial. They are repeated below in abbreviated form, with specific instructions on naming conventions, content information, and other important details. Fork cc20 repo (this repo) to your GitHub account. Clone/download the forked repo to your local computer. Create a new branch and name it with your project name, in our case sample_project. Do not skip this step. We will not merge your PR if it doesn’t come from a branch. If you already forgot to do this, check this tutorial for how to fix it. Copy your modified .Rmd file with the same name into the root directory on the branch. In our example, it is sample_project.Rmd. Do not include an .html file. (In order for the bookdown package to work, all .Rmd files will be rendered behind the scenes.) [OPTIONAL] If you have other resources (such as images) included in your project, create a folder under resources/. In our example, it is resources/sample_project/. Put the resources files there. Be sure to change all the links in your .Rmd to include resources/.../, for example: ![Test Photo](resources/sample_project/election.jpg) When you are ready to submit your project, push your branch to your remote repo. Follow this tutorial to create a pull request. At this point a back and forth will begin with the team managing the pull requests. If you are asked to make changes, simply make the changes on your local branch, save, commit, and push to GitHub. The new commits will be added to your pull request; you do not need to, and should not, create a new pull request. (If, based on the circumstances, it does make sense to close the pull request and start a new one, we will tell you.) Once your pull request is merged, it’s fine to delete your local clone (folder) as well as the forked repository in your GitHub account. 1.4 Optional tweaks If you prefer for links from your chapter to open in new tabs, add {target=\"_blank\"} after the link, such as: [edav.info](edav.info){target=\"_blank\"} Note that your headers (##, ###, etc.) will be converted to numbered headings as such: ## –&gt; 3.1 ### –&gt; 3.1.1 These headings will appear as chapter subheadings and sub-subheadings in the navigation panel on the left. Think about a logical structure for users to navigate your chapter. We recommend using only ## and ### headings since “sub-sub-subheadings” such as 4.1.3.4 are generally unnecessary and look messy. Unfortunately, there’s no simple way to preview your chapter before it’s actually merged into the project. (bookdown has preview_chapter() option but it only works after the entire book has been rendered at least once and that will become more and more complex and require more and more packages as the project grows.) If you really want to preview it, fork and clone this minimal bookdown repo, add your .Rmd file, click the “Build book” button on the Build tab (next to Git), and then open any of the .html files in the _book folder in a web browser to see the rendered book. (Do not click the Knit button as it will not build a bookdown book.) If you’re interested in more bookdown options, see the official reference book. Have more useful tweaks to share? Submit an issue or PR. 1.5 FAQ 1.5.1 What should I expect after creating a pull request? Within a week after you create a pull request, we will apply a label to it and assign an administrator who will review all the files you submit to see if they meet the requirements. It will take some time before we can process all the pull requests, so as long as you see your pull request has been labeled and assigned to an admin, don’t worry. However, if the admin contacts you regarding the pull request, that usually means your files fail to meet some requirements. The admin will clearly state what is wrong, so please fix them as soon as possible. 1.5.2 What if I catch mistakes before my pull request is merged? Just make the changes on your branch, commit and push them to GitHub. They will automatically be added to the pull request. 1.5.3 What if I catch mistakes after my pull request is merged? You may submit additional pull requests to fix material on the site. If the edits are small, such as fixing typos, it is easiest to make the edits directly on GitHub, following these instructions. We will merge first pull requests before edits, so please be patient. 1.5.4 Other questions If you encounter other problems, please submit an issue and we will look into it. Thank you for your contributions! "],["sample-project.html", "Chapter 2 Sample project", " Chapter 2 Sample project Joe Biden and Donald Trump This chapter gives a sample layout of your Rmd file. Test Photo "],["data-transformation-in-r.html", "Chapter 3 Data transformation in R 3.1 Introduction 3.2 Basics 3.3 Function Usage 3.4 Quick EDA: median arrival delays 3.5 External Resource", " Chapter 3 Data transformation in R Jiongxin Ye and Zhuoyan Ma 3.1 Introduction Data visualizations are nice and insightful, but we usually spend more time formatting, cleaning and wrangling the data. Sometimes, we need to transform the data to perform a better visualization, or maybe we just want to rename the variables and get summaries. No matter for detect factual information or implicit relationships, data transformation plays an important role, helping us to dig deeper and wider and thus telling a better story from data. As a result, we want share some useful methods of data transformation to let you play with data more efficiently. Specifically, we want to provide a detailed instruction of package dplyr. We hope that you can know the various methods in changing the data frame and function in selecting the data which you want after reading this article. 3.2 Basics As said before, we will mainly use dplyr package, which will be automatically installed if you install the tidyverse. filter: select observations by their values arrange: reorder observations select: pick variables by their names mutate: create or rename variables summarize: aggregate observations group_by: group observations by variables All dplyr “verbs” are functions that take a data frame and return a data frame after the operation To explore the basic data manipulation of dplur, we will demonstrate using nycflights13::flights. It’s a dataset which contains information of 336,776 flights that departed from New York City in 2013. You can access it by installing the packages ‘nycflights13’. library(tidyverse) library(nycflights13) nycflights13::flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 3.3 Function Usage 3.3.1 1. Filter( ) To select observations we can use filter: filter (.data, condition1, condition2, …, conditionN) where each condition evaluates to a logical vector and only TRUE entries are kept. Example: we want to focus on the flight whose carrier is UA. UA_flight &lt;- filter(flights,carrier==&#39;UA&#39;) head(UA_flight) ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 554 558 -4 740 728 ## 4 2013 1 1 558 600 -2 924 917 ## 5 2013 1 1 558 600 -2 923 937 ## 6 2013 1 1 559 600 -1 854 902 ## # … with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; UA_num = nrow(UA_flight) What we found: There are UA_num, 58665, flights whose carrier is UA in nycflights13 dataset. We also can use filter to remove rows that associated with NA values of certain variables like dep_time. Tidy_flight&lt;-filter(flights,!is.na(dep_time)) dep_num = nrow(Tidy_flight) What we found: we remove over 8,000 rows whose dep_time is NA. The total number of observations after removing the NA objects in dep_time is dep_num, which is 328521. More importantly, we can cooperate with logical operators ! (not), | (or), &amp; (and) and some statistical rules such as De Morgan’s Law, to add more conditions in the filter function in a way you like. Below three approaches are equivalent to find flights in January and Feburary. filter(flights, month == 1 | month == 2) ## # A tibble: 51,955 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 51,945 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, month %in% c(1, 2)) ## # A tibble: 51,955 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 51,945 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, month &lt;=2 ) ## # A tibble: 51,955 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 51,945 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 3.3.1.1 More exercises: Find flights that: Were delayed by at least an hour, but made up over 30 minutes in flight filter(flights, dep_delay &gt; 60, dep_delay - arr_delay &gt;= 30) ## # A tibble: 2,046 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 1716 1545 91 2140 2039 ## 2 2013 1 1 2205 1720 285 46 2040 ## 3 2013 1 1 2326 2130 116 131 18 ## 4 2013 1 3 1503 1221 162 1803 1555 ## 5 2013 1 3 1821 1530 171 2131 1910 ## 6 2013 1 3 1839 1700 99 2056 1950 ## 7 2013 1 3 1850 1745 65 2148 2120 ## 8 2013 1 3 1923 1815 68 2036 1958 ## 9 2013 1 3 1941 1759 102 2246 2139 ## 10 2013 1 3 1950 1845 65 2228 2227 ## # … with 2,036 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Flew to Boston operated by United, American or Delta in Summer (June to August) filter(flights, dest == &quot;BOS&quot;, carrier == &quot;UA&quot; | carrier == &quot;AA&quot; | carrier == &quot;DL&quot;, month %in% c(6, 7, 8)) ## # A tibble: 1,663 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 6 1 816 820 -4 920 930 ## 2 2013 6 1 1022 1025 -3 1130 1150 ## 3 2013 6 1 1240 1245 -5 1343 1350 ## 4 2013 6 1 1519 1530 -11 1705 1702 ## 5 2013 6 1 1524 1445 39 1634 1615 ## 6 2013 6 1 1555 1600 -5 1705 1720 ## 7 2013 6 1 1954 1955 -1 2116 2110 ## 8 2013 6 1 2010 2000 10 2115 2130 ## 9 2013 6 1 2124 2125 -1 2224 2256 ## 10 2013 6 1 2152 2159 -7 2252 2328 ## # … with 1,653 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 3.3.2 2. Arrange( ) Arrange( ) function lets us to reorder the rows in a order that we want: arrange (.data, variable1, variable2,…, .by_group = FALSE) It’s default in increasing order. To reorder decreasingly, use desc. You can also reorder the rows by group, using .by_group. Example: we can reorder the flight by the delay in departure in a increasing order. arrange(flights, dep_delay) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 7 2040 2123 -43 40 2352 ## 2 2013 2 3 2022 2055 -33 2240 2338 ## 3 2013 11 10 1408 1440 -32 1549 1559 ## 4 2013 1 11 1900 1930 -30 2233 2243 ## 5 2013 1 29 1703 1730 -27 1947 1957 ## 6 2013 8 9 729 755 -26 1002 955 ## 7 2013 10 23 1907 1932 -25 2143 2143 ## 8 2013 3 30 2030 2055 -25 2213 2250 ## 9 2013 3 2 1431 1455 -24 1601 1631 ## 10 2013 5 5 934 958 -24 1225 1309 ## # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; What we found: The data has been reorder by dep_delay value from small to large. we can reorder the flight by the delay in departure in a decreasing order. arrange(flights, desc(dep_delay)) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 9 641 900 1301 1242 1530 ## 2 2013 6 15 1432 1935 1137 1607 2120 ## 3 2013 1 10 1121 1635 1126 1239 1810 ## 4 2013 9 20 1139 1845 1014 1457 2210 ## 5 2013 7 22 845 1600 1005 1044 1815 ## 6 2013 4 10 1100 1900 960 1342 2211 ## 7 2013 3 17 2321 810 911 135 1020 ## 8 2013 6 27 959 1900 899 1236 2226 ## 9 2013 7 22 2257 759 898 121 1026 ## 10 2013 12 5 756 1700 896 1058 2020 ## # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; What we found: The data has been reorder by dep_delay value from large to small. we can reorder the flight by the month and day. arrange(flights, month, day) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; What we found: The data has been reorder by 2 variables: month and day. For the object with same month, they are reordering by day. we can reorder the flight by the delay in departure in a decreasing order group by carrier. flights%&gt;%group_by(carrier)%&gt;%arrange(desc(dep_delay),.by_group = TRUE) ## # A tibble: 336,776 x 19 ## # Groups: carrier [16] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 2 16 757 1930 747 1013 2149 ## 2 2013 7 24 1525 815 430 1808 1030 ## 3 2013 11 27 1503 815 408 1628 952 ## 4 2013 2 27 1529 845 404 1639 1015 ## 5 2013 7 7 2228 1559 389 NA 1828 ## 6 2013 6 25 1421 805 376 1602 950 ## 7 2013 1 25 15 1815 360 208 1958 ## 8 2013 12 14 1425 825 360 1604 938 ## 9 2013 9 12 2159 1600 359 2400 1818 ## 10 2013 7 22 2216 1620 356 116 1853 ## # … with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; What we found: The data has been reorder by grouping the carrier and then order by dep_delay value from large to small. Since the carrier is nominal variable, the carrier order is default. Attention: If we don’t use the .by_group in arrange( ), the data will be reorder just by dep_delay value although we have group_by the data in previous. 3.3.3 3. Select( ) It subsets a dataset containing only selected variables. select (.data, variable_expression) Some helper functions: Negative indices remove variables from the selection. A:B, from column A to column B starts_with(“abc”): matches names starting with “abc” ends_with(“xyz”): matches names ending with “xyz” contains(“ijk”): matches names containing “ijk” num_range(“x”, 1:3): matches x1, x2, x3 We can use the operators in selecting the variables easily and use c( ) to combine selections. Example: Select year, month, day, destination columns from flights. select(flights, year:day, dest) ## # A tibble: 336,776 x 4 ## year month day dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2013 1 1 IAH ## 2 2013 1 1 IAH ## 3 2013 1 1 MIA ## 4 2013 1 1 BQN ## 5 2013 1 1 ATL ## 6 2013 1 1 ORD ## 7 2013 1 1 FLL ## 8 2013 1 1 IAD ## 9 2013 1 1 MCO ## 10 2013 1 1 ORD ## # … with 336,766 more rows we can also use the columns number to represent the columns name. select(flights, 1:3, 14) ## # A tibble: 336,776 x 4 ## year month day dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2013 1 1 IAH ## 2 2013 1 1 IAH ## 3 2013 1 1 MIA ## 4 2013 1 1 BQN ## 5 2013 1 1 ATL ## 6 2013 1 1 ORD ## 7 2013 1 1 FLL ## 8 2013 1 1 IAD ## 9 2013 1 1 MCO ## 10 2013 1 1 ORD ## # … with 336,766 more rows we can also use | to union the variable set. select(flights, 1:3|14) ## # A tibble: 336,776 x 4 ## year month day dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2013 1 1 IAH ## 2 2013 1 1 IAH ## 3 2013 1 1 MIA ## 4 2013 1 1 BQN ## 5 2013 1 1 ATL ## 6 2013 1 1 ORD ## 7 2013 1 1 FLL ## 8 2013 1 1 IAD ## 9 2013 1 1 MCO ## 10 2013 1 1 ORD ## # … with 336,766 more rows Select all columns except year, month, day, destination from flights. select(flights, !year:day, !dest) ## # A tibble: 336,776 x 19 ## dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 517 515 2 830 819 11 UA ## 2 533 529 4 850 830 20 UA ## 3 542 540 2 923 850 33 AA ## 4 544 545 -1 1004 1022 -18 B6 ## 5 554 600 -6 812 837 -25 DL ## 6 554 558 -4 740 728 12 UA ## 7 555 600 -5 913 854 19 B6 ## 8 557 600 -3 709 723 -14 EV ## 9 557 600 -3 838 846 -8 B6 ## 10 558 600 -2 753 745 8 AA ## # … with 336,766 more rows, and 12 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt; We can also use the c( ) to combine the variables. select(flights, !c(year:day,dest)) ## # A tibble: 336,776 x 15 ## dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 517 515 2 830 819 11 UA ## 2 533 529 4 850 830 20 UA ## 3 542 540 2 923 850 33 AA ## 4 544 545 -1 1004 1022 -18 B6 ## 5 554 600 -6 812 837 -25 DL ## 6 554 558 -4 740 728 12 UA ## 7 555 600 -5 913 854 19 B6 ## 8 557 600 -3 709 723 -14 EV ## 9 557 600 -3 838 846 -8 B6 ## 10 558 600 -2 753 745 8 AA ## # … with 336,766 more rows, and 8 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; Select columns end with ‘times’. select(flights, ends_with(&quot;time&quot;)) ## # A tibble: 336,776 x 5 ## dep_time sched_dep_time arr_time sched_arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 830 819 227 ## 2 533 529 850 830 227 ## 3 542 540 923 850 160 ## 4 544 545 1004 1022 183 ## 5 554 600 812 837 116 ## 6 554 558 740 728 150 ## 7 555 600 913 854 158 ## 8 557 600 709 723 53 ## 9 557 600 838 846 140 ## 10 558 600 753 745 138 ## # … with 336,766 more rows Select columns end with ‘times’ but without the one starting with ‘sched’. select(flights, ends_with(&quot;time&quot;), -starts_with(&quot;sched&quot;)) ## # A tibble: 336,776 x 3 ## dep_time arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 830 227 ## 2 533 850 227 ## 3 542 923 160 ## 4 544 1004 183 ## 5 554 812 116 ## 6 554 740 150 ## 7 555 913 158 ## 8 557 709 53 ## 9 557 838 140 ## 10 558 753 138 ## # … with 336,766 more rows Select columns contains ‘dep’. select(flights, contains(&quot;dep&quot;)) ## # A tibble: 336,776 x 3 ## dep_time sched_dep_time dep_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 2 ## 2 533 529 4 ## 3 542 540 2 ## 4 544 545 -1 ## 5 554 600 -6 ## 6 554 558 -4 ## 7 555 600 -5 ## 8 557 600 -3 ## 9 557 600 -3 ## 10 558 600 -2 ## # … with 336,766 more rows Select year, month, day, destination columns from flights. Also columns end with ‘times’ but without the one starting with ‘sched’. select(flights, year:day, dest, ends_with(&quot;time&quot;), -starts_with(&quot;sched&quot;)) ## # A tibble: 336,776 x 7 ## year month day dest dep_time arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 IAH 517 830 227 ## 2 2013 1 1 IAH 533 850 227 ## 3 2013 1 1 MIA 542 923 160 ## 4 2013 1 1 BQN 544 1004 183 ## 5 2013 1 1 ATL 554 812 116 ## 6 2013 1 1 ORD 554 740 150 ## 7 2013 1 1 FLL 555 913 158 ## 8 2013 1 1 IAD 557 709 53 ## 9 2013 1 1 MCO 557 838 140 ## 10 2013 1 1 ORD 558 753 138 ## # … with 336,766 more rows Attention: For without condition, we can also use ! to replace the -. We can also use &amp; to intersect the variables set together, which is equivalent with the previous method. select(flights, year:day, dest, ends_with(&quot;time&quot;)&amp; -starts_with(&quot;sched&quot;)) ## # A tibble: 336,776 x 7 ## year month day dest dep_time arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 IAH 517 830 227 ## 2 2013 1 1 IAH 533 850 227 ## 3 2013 1 1 MIA 542 923 160 ## 4 2013 1 1 BQN 544 1004 183 ## 5 2013 1 1 ATL 554 812 116 ## 6 2013 1 1 ORD 554 740 150 ## 7 2013 1 1 FLL 555 913 158 ## 8 2013 1 1 IAD 557 709 53 ## 9 2013 1 1 MCO 557 838 140 ## 10 2013 1 1 ORD 558 753 138 ## # … with 336,766 more rows This is an example of using operations in select( ). It also approaches to the same result as before. select(flights, year:day|dest|ends_with(&quot;time&quot;)&amp;!starts_with(&quot;sched&quot;)) ## # A tibble: 336,776 x 7 ## year month day dest dep_time arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 IAH 517 830 227 ## 2 2013 1 1 IAH 533 850 227 ## 3 2013 1 1 MIA 542 923 160 ## 4 2013 1 1 BQN 544 1004 183 ## 5 2013 1 1 ATL 554 812 116 ## 6 2013 1 1 ORD 554 740 150 ## 7 2013 1 1 FLL 555 913 158 ## 8 2013 1 1 IAD 557 709 53 ## 9 2013 1 1 MCO 557 838 140 ## 10 2013 1 1 ORD 558 753 138 ## # … with 336,766 more rows 3.3.4 4. Mutate( ) It changes variables to the format that we want. If we want to only keep the variables that we create, we should use transmute( ) mutate (.data,…, .keep = c(“all”, “used”, “unused”, “none”), .before = NULL, .after = NULL) Some helper functions： Arithmetic operators: +, -, *, /, ^, %/% (integer division), and %% (remainder) Mathematical functions such as log, exp, sin, cos, and others Logical operators, e.g. !=, |, as we saw when discussing filter Offsets: lead and lag, e.g., to compute running difference x - lag(x) Aggregators: mean, sum, min, max and their respective cummulators cummean, cumsum, cummin, cummax Example: We create three variables here: gain which measures in-flight time gain, duration which is flight duration in hours and speed which equals to distance divided by duration. mutate(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration) ## # A tibble: 336,776 x 22 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # … with 336,766 more rows, and 14 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # gain &lt;dbl&gt;, duration &lt;dbl&gt;, speed &lt;dbl&gt; Attention: Since the default value of .keep = “all”, we will get a new data frame with all variables and new variables. we can also use .keep = “used” to get the data frame contain the new variables and variables we used in generate the new variables. mutate(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration, .keep = &quot;used&quot;) ## # A tibble: 336,776 x 7 ## dep_delay arr_delay air_time distance gain duration speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 227 1400 -9 3.78 370. ## 2 4 20 227 1416 -16 3.78 374. ## 3 2 33 160 1089 -31 2.67 408. ## 4 -1 -18 183 1576 17 3.05 517. ## 5 -6 -25 116 762 19 1.93 394. ## 6 -4 12 150 719 -16 2.5 288. ## 7 -5 19 158 1065 -24 2.63 404. ## 8 -3 -14 53 229 11 0.883 259. ## 9 -3 -8 140 944 5 2.33 405. ## 10 -2 8 138 733 -10 2.3 319. ## # … with 336,766 more rows we can also use .keep = “unused” to get the data frame contain the new variables and all variables which don’t used in calulate the new variables. This transform is equal to replace the variables using new variable related to it in this data frame. mutate(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration, .keep = &quot;unused&quot;) ## # A tibble: 336,776 x 18 ## year month day dep_time sched_dep_time arr_time sched_arr_time carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 830 819 UA ## 2 2013 1 1 533 529 850 830 UA ## 3 2013 1 1 542 540 923 850 AA ## 4 2013 1 1 544 545 1004 1022 B6 ## 5 2013 1 1 554 600 812 837 DL ## 6 2013 1 1 554 558 740 728 UA ## 7 2013 1 1 555 600 913 854 B6 ## 8 2013 1 1 557 600 709 723 EV ## 9 2013 1 1 557 600 838 846 B6 ## 10 2013 1 1 558 600 753 745 AA ## # … with 336,766 more rows, and 10 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ## # gain &lt;dbl&gt;, duration &lt;dbl&gt;, speed &lt;dbl&gt; we can also use .keep = “none” to get the data frame only contain the new variables. This transform is same as using default transmute. mutate(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration, .keep = &quot;none&quot;) ## # A tibble: 336,776 x 3 ## gain duration speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -9 3.78 370. ## 2 -16 3.78 374. ## 3 -31 2.67 408. ## 4 17 3.05 517. ## 5 19 1.93 394. ## 6 -16 2.5 288. ## 7 -24 2.63 404. ## 8 11 0.883 259. ## 9 5 2.33 405. ## 10 -10 2.3 319. ## # … with 336,766 more rows transmute(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration) ## # A tibble: 336,776 x 3 ## gain duration speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -9 3.78 370. ## 2 -16 3.78 374. ## 3 -31 2.67 408. ## 4 17 3.05 517. ## 5 19 1.93 394. ## 6 -16 2.5 288. ## 7 -24 2.63 404. ## 8 11 0.883 259. ## 9 5 2.33 405. ## 10 -10 2.3 319. ## # … with 336,766 more rows Attention: The difference between Mutate( ) and Transmute( ) is that: the default return from Mutate( ) are all variables with new variables. However, the default return from transmute( ) is only the new variables. We can change the parameter in each functions and get the same result. We create a new boolean variable as_scheduled which indicates whether the flight delays in arrival time or departure time. And we use transmute here to only keep three variables, including the one we just created. flights %&gt;% transmute(arr_delay, dep_delay, as_scheduled = arr_delay &lt; 0 &amp; dep_delay &lt; 0) ## # A tibble: 336,776 x 3 ## arr_delay dep_delay as_scheduled ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 11 2 FALSE ## 2 20 4 FALSE ## 3 33 2 FALSE ## 4 -18 -1 TRUE ## 5 -25 -6 TRUE ## 6 12 -4 FALSE ## 7 19 -5 FALSE ## 8 -14 -3 TRUE ## 9 -8 -3 TRUE ## 10 8 -2 FALSE ## # … with 336,766 more rows We use .before and .after to determine the position of new variables. Create three variables here: gain which measures in-flight time gain, duration which is flight duration in hours and speed which equals to distance divided by duration. Then we want to place the variables in front of dep_time. mutate(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration, .before = dep_time) ## # A tibble: 336,776 x 22 ## year month day gain duration speed dep_time sched_dep_time dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 -9 3.78 370. 517 515 2 ## 2 2013 1 1 -16 3.78 374. 533 529 4 ## 3 2013 1 1 -31 2.67 408. 542 540 2 ## 4 2013 1 1 17 3.05 517. 544 545 -1 ## 5 2013 1 1 19 1.93 394. 554 600 -6 ## 6 2013 1 1 -16 2.5 288. 554 558 -4 ## 7 2013 1 1 -24 2.63 404. 555 600 -5 ## 8 2013 1 1 11 0.883 259. 557 600 -3 ## 9 2013 1 1 5 2.33 405. 557 600 -3 ## 10 2013 1 1 -10 2.3 319. 558 600 -2 ## # … with 336,766 more rows, and 13 more variables: arr_time &lt;int&gt;, ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; We create three variables here: gain which measures in-flight time gain, duration which is flight duration in hours and speed which equals to distance divided by duration. Then we want to place the variables in after of dep_time. mutate(flights,gain = dep_delay - arr_delay, duration = air_time / 60, speed = distance / duration, .after = dep_time) ## # A tibble: 336,776 x 22 ## year month day dep_time gain duration speed sched_dep_time dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 517 -9 3.78 370. 515 2 ## 2 2013 1 1 533 -16 3.78 374. 529 4 ## 3 2013 1 1 542 -31 2.67 408. 540 2 ## 4 2013 1 1 544 17 3.05 517. 545 -1 ## 5 2013 1 1 554 19 1.93 394. 600 -6 ## 6 2013 1 1 554 -16 2.5 288. 558 -4 ## 7 2013 1 1 555 -24 2.63 404. 600 -5 ## 8 2013 1 1 557 11 0.883 259. 600 -3 ## 9 2013 1 1 557 5 2.33 405. 600 -3 ## 10 2013 1 1 558 -10 2.3 319. 600 -2 ## # … with 336,766 more rows, and 13 more variables: arr_time &lt;int&gt;, ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 3.3.5 5. Group_by( ) and Summarize( ) Summarize( ) is used to aggregated data. We usually use it after grouping the observations with group_by. In other words, when we want to know counts of certain combinations of variables, we can take advantage of group_by and summarize. (Hint: summarise () is the same as summarize( )). Examples: For each original airports, find their median delay time and associated counts. na.rm = TRUE: remove missing values summarize(group_by(flights, origin),median_delay = median(arr_delay, na.rm = TRUE), count = n()) ## # A tibble: 3 x 3 ## origin median_delay count ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 EWR -4 120835 ## 2 JFK -6 111279 ## 3 LGA -5 104662 For each original airports and each carriers, find the proportion of flights that had a delayed departure, but by no longer than 30 minutes. summarize(group_by(flights, origin, carrier), prop_delayed = mean(between(dep_delay, 0, 30), na.rm = TRUE)) ## # A tibble: 35 x 3 ## # Groups: origin [3] ## origin carrier prop_delayed ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EWR 9E 0.134 ## 2 EWR AA 0.214 ## 3 EWR AS 0.268 ## 4 EWR B6 0.236 ## 5 EWR DL 0.240 ## 6 EWR EV 0.264 ## 7 EWR MQ 0.245 ## 8 EWR OO 0.333 ## 9 EWR UA 0.422 ## 10 EWR US 0.183 ## # … with 25 more rows We can also use group_by in front of summarize. flights %&gt;% group_by(origin, carrier) %&gt;% summarize(prop_delayed = mean(between(dep_delay, 0, 30), na.rm = TRUE)) ## # A tibble: 35 x 3 ## # Groups: origin [3] ## origin carrier prop_delayed ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EWR 9E 0.134 ## 2 EWR AA 0.214 ## 3 EWR AS 0.268 ## 4 EWR B6 0.236 ## 5 EWR DL 0.240 ## 6 EWR EV 0.264 ## 7 EWR MQ 0.245 ## 8 EWR OO 0.333 ## 9 EWR UA 0.422 ## 10 EWR US 0.183 ## # … with 25 more rows 3.4 Quick EDA: median arrival delays Here, we utilize the tools of dyplr to conduct a quick exploratory data analysis of median arrival delays of flights. First, we clean the dataset a little bit by filtering out the flights whose arr_delay and dep_delay is NA. Therefore, we can find flights that are not cancelled and then it’s meaningful to analyze their arrival delay. not_canceled &lt;- flights %&gt;% filter(!is.na(arr_delay), !is.na(dep_delay)) Then, we want to see if the arrival delays of flights are different at each airport, so we use group_by( ) and summarise( ) to get a basic statistics summary of arrival delays.Here, we display counts, min, 25th quantile, median, 75th quantile and max of arrival delays. not_canceled %&gt;% group_by(origin) %&gt;% summarize(count = n(),min_delay = min(arr_delay), q1_delay = quantile(arr_delay, .25), # first quartile median_delay = median(arr_delay), q3_delay = quantile(arr_delay, .75), # third quartile max_delay = max(arr_delay)) ## # A tibble: 3 x 7 ## origin count min_delay q1_delay median_delay q3_delay max_delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EWR 117127 -86 -16 -4 16 1109 ## 2 JFK 109079 -79 -18 -6 13 1272 ## 3 LGA 101140 -68 -17 -5 12 915 Through the summary, we discover that the flights distributes pretty evenly in three airports, which avoids selection bias affecting our analysis negatively. Also, EWR has highest median delay and LGA has the lowest one. But, we have to say that the differences of median delay at three airpots don’t seem to be very significant, which suggests that the arrival delay may not depend on the airports. Then, we want to see if the arrival delay depends on the flight. Thus, let us plot histogram median arrival delay for each flight: plot_theme&lt;-theme(panel.grid.major.y = element_line(), panel.grid.major.x = element_line(), panel.grid.minor = element_line(), plot.background = element_rect(fill=&#39;#FAFAFA&#39;), axis.line = element_line(color=&#39;black&#39;), plot.title = element_text(hjust=0.25)) not_canceled %&gt;% group_by(flight) %&gt;% summarize(delay = median(arr_delay)) %&gt;% ggplot(aes(x=delay)) + geom_histogram()+ labs(x=&#39;Median Arrival Delay&#39;, y=&#39;Frequency&#39;, title=&#39;Histogram of Median Arrival Delay of each flight&#39;)+ plot_theme We discover that most flights’ median arrival delay is around 10,while only a very few number of flights have median arrival delay around 75 to 90 minutes.And one or two flights even have median delay over 200 miniutes. Now let’s plot median arrival delay against number of trips for flight: not_canceled %&gt;% group_by(flight) %&gt;% summarize(count = n(), delay = median(arr_delay)) %&gt;% ggplot(aes(x = count, y = delay)) + geom_point(alpha = 0.1)+ labs(x=&#39;Number of trips&#39;,y=&#39;median arrival delay&#39;,title=&#39;Median Arrival Delay V.s Number of trips&#39;)+ plot_theme Like the previous plot, we notice that the “outliers” seem to be due to flights with small number of trips. Probably, we should remove them and take a closer look at ones that have more than 25 trips. We also add a regression line with a smoother, hoping to indicate something about relationship between median arrival delay and number of trips of flights. not_canceled %&gt;% group_by(flight) %&gt;% summarize(count = n(), delay = median(arr_delay)) %&gt;% filter(count &gt; 25) %&gt;% ggplot(aes(x = count, y = delay)) + geom_point(alpha = .1) + geom_smooth()+ plot_theme In the plot, we find a very weak negative relationship between number of trips and median arrival delay. 3.5 External Resource dplyr: Excellent resource about the overview of dplyr package. Linking to URL in markdown and R markdown Cheatsheet: Tools to typeset in markdown. "],["data-preprocessing-and-feature-engineering-in-r.html", "Chapter 4 Data Preprocessing and Feature Engineering in R 4.1 Overview 4.2 Missing Values 4.3 Feature Selection 4.4 Dimentionality Reduction", " Chapter 4 Data Preprocessing and Feature Engineering in R Kenny Jin library(naniar) library(zoo) library(dplyr) library(VIM) library(caret) library(factoextra) 4.1 Overview Data proprocessing and feature engineering is usually a necessary step for data visualization and machine learning. This article will introduce several data preprocessing and feature engineering techniques and how to implement these techniques in R. 4.2 Missing Values Real world datasets usually contain missing values. Hence, it is important to properly handle these missing values before we continue to perform any data related tasks. 4.2.1 Exploring the dataset We use R’s airquality dataset as an example. The first thing to do for handling missing values is to explore the dataset and find out how many values are missing, and where are these values. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 Using head() function, we can clearly see that there are missing values (denoted as NA) in this dataset. But how many values are missing? We can find this out using the n_miss() function from the “naniar” package. n_miss(airquality) ## [1] 44 The function tells us that there are 44 missing values in total. We can also pass a single column to n_miss() to find out how many missing values are in the column. n_miss(airquality$Ozone) ## [1] 37 We see that the “Ozone” column alone has 37 missing values. Usually we want to find out the proportion of missing values, rather than a single number. This can be easily achieved using prop_miss(). Note that you can also pass a single column to this function. prop_miss(airquality) ## [1] 0.04793028 prop_miss(airquality$Ozone) ## [1] 0.2418301 The results above shows that there are about 4.79% of the values missing for the whole dataset, and 24.18% of the values are missing for the Ozone column. We can also find out the count and proportion of non-missing values using n_complete(), prop_complete(). n_complete(airquality) ## [1] 874 prop_complete(airquality) ## [1] 0.9520697 n_complete(airquality$Ozone) ## [1] 116 prop_complete(airquality$Ozone) ## [1] 0.7581699 We can also get a summary for the whole dataset using miss_var_summary(). Note this is a summary for each column, or variable. n_miss is the number of missing values in that column, and pct_miss is the percentage. miss_var_summary(airquality) ## # A tibble: 6 x 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Ozone 37 24.2 ## 2 Solar.R 7 4.58 ## 3 Wind 0 0 ## 4 Temp 0 0 ## 5 Month 0 0 ## 6 Day 0 0 Getting the summary for each row, or each case, can be achieved using miss_case_summary(). “case” is the row number of the observation. n_miss is the number of missing values in that row, and pct_miss is the percentage. miss_case_summary(airquality) ## # A tibble: 153 x 3 ## case n_miss pct_miss ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 5 2 33.3 ## 2 27 2 33.3 ## 3 6 1 16.7 ## 4 10 1 16.7 ## 5 11 1 16.7 ## 6 25 1 16.7 ## 7 26 1 16.7 ## 8 32 1 16.7 ## 9 33 1 16.7 ## 10 34 1 16.7 ## # … with 143 more rows We can also visualize the count of missing values for each column using gg_miss_var(). gg_miss_var(airquality) 4.2.2 Handling Missing Values and Imputation The easiest way to handle the missing values is simply to drop all instances(rows) with NA. This can be achieved using na.omit(). airquality_clean = na.omit(airquality) head(airquality_clean) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 n_miss(airquality_clean) ## [1] 0 Using n_miss, we see that all rows with NAs are dropped. Sometimes we want to fill in the NA values rather than simply dropping the row. The process of filling in NA entries is called “Imputation”. 4.2.2.1 Mean Imputation There are many imputation methods, and one of the most popular is “mean imputation”, to fill in all the missing values with the mean of that column. To implement mean imputation, we can use the mutate_all() from the package dplyr. air_imp &lt;- airquality %&gt;% mutate_all(~ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)) n_miss(air_imp) ## [1] 0 head(air_imp) ## Ozone Solar.R Wind Temp Month Day ## 1 41.00000 190.0000 7.4 67 5 1 ## 2 36.00000 118.0000 8.0 72 5 2 ## 3 12.00000 149.0000 12.6 74 5 3 ## 4 18.00000 313.0000 11.5 62 5 4 ## 5 42.12931 185.9315 14.3 56 5 5 ## 6 28.00000 185.9315 14.9 66 5 6 We can see that all NAs are replaced by the mean value of that column. We can also achieve the mean imputation using na.aggregate() from package zoo. air_imp_1 &lt;- na.aggregate(airquality) n_miss(air_imp_1) ## [1] 0 head(air_imp_1) ## Ozone Solar.R Wind Temp Month Day ## 1 41.00000 190.0000 7.4 67 5 1 ## 2 36.00000 118.0000 8.0 72 5 2 ## 3 12.00000 149.0000 12.6 74 5 3 ## 4 18.00000 313.0000 11.5 62 5 4 ## 5 42.12931 185.9315 14.3 56 5 5 ## 6 28.00000 185.9315 14.9 66 5 6 The results are the same as above. It is worth noting that mean imputation might be problematic if the variable we are imputing is correlated with other variables. In this case, the relationships between variables might be destroyed. 4.2.2.2 KNN Imputation We can also fill in the missing values using the k-nearest neighbor methods. Instead of computing the overall mean for the whole column, The algorithm will only compute the average of the k nearest data points when performing inference regarding the missing values. KNN imputation can be achieved using kNN() function from package VIM. air_imp_knn &lt;- kNN(airquality, k = 5, variable = &quot;Ozone&quot;) n_miss(air_imp_knn$Ozone) ## [1] 0 head(air_imp_knn) ## Ozone Solar.R Wind Temp Month Day Ozone_imp ## 1 41 190 7.4 67 5 1 FALSE ## 2 36 118 8.0 72 5 2 FALSE ## 3 12 149 12.6 74 5 3 FALSE ## 4 18 313 11.5 62 5 4 FALSE ## 5 18 NA 14.3 56 5 5 TRUE ## 6 28 NA 14.9 66 5 6 FALSE k is the number of neighbors for inference. In this example we specify k = 5. A modifed KNN method is to compute the distance-weighted mean. The weights are inverted distanced from each neighbor. This can also be achieved using kNN(). air_imp_knn_1 &lt;- kNN(airquality, k = 5, variable = &quot;Ozone&quot;, numFun = weighted.mean, weightDist = TRUE) n_miss(air_imp_knn_1$Ozone) ## [1] 0 head(air_imp_knn_1) ## Ozone Solar.R Wind Temp Month Day Ozone_imp ## 1 41 190 7.4 67 5 1 FALSE ## 2 36 118 8.0 72 5 2 FALSE ## 3 12 149 12.6 74 5 3 FALSE ## 4 18 313 11.5 62 5 4 FALSE ## 5 18 NA 14.3 56 5 5 TRUE ## 6 28 NA 14.9 66 5 6 FALSE 4.3 Feature Selection Reference: https://learn.datacamp.com/courses/machine-learning-with-caret-in-r Before performing any supervised machine learning tasks, it is ususally necessary to select features. One common technique for feature selection is to remove features with low variance. If a feature has low variance, it is likely that it contains “low information”, thus might not be very helpful for predictive analysis. We use the BloodBrain dataset from caret as an example. data(BloodBrain) bloodbrain_x = bbbDescr We can identify the features that have low variance with the nearZeroVar() function from caret library. # Identify near zero variance predictors: remove_cols remove_cols &lt;- nearZeroVar(bloodbrain_x, names = TRUE, freqCut = 2, uniqueCut = 20) remove_cols ## [1] &quot;negative&quot; &quot;peoe_vsa.3&quot; &quot;peoe_vsa.4&quot; ## [4] &quot;peoe_vsa.5&quot; &quot;peoe_vsa.2.1&quot; &quot;peoe_vsa.3.1&quot; ## [7] &quot;peoe_vsa.4.1&quot; &quot;peoe_vsa.5.1&quot; &quot;peoe_vsa.6.1&quot; ## [10] &quot;a_acid&quot; &quot;vsa_acid&quot; &quot;vsa_base&quot; ## [13] &quot;vsa_pol&quot; &quot;slogp_vsa2&quot; &quot;slogp_vsa6&quot; ## [16] &quot;slogp_vsa8&quot; &quot;smr_vsa4&quot; &quot;frac.anion7.&quot; ## [19] &quot;rule.of.5violations&quot; &quot;alert&quot; &quot;inthb&quot; ## [22] &quot;chdh3&quot; # Get all column names from bloodbrain_x: all_cols all_cols &lt;- names(bloodbrain_x) # Remove from data: bloodbrain_x_small bloodbrain_x_small &lt;- bloodbrain_x[ , setdiff(all_cols, remove_cols)] dim(bloodbrain_x) ## [1] 208 134 dim(bloodbrain_x_small) ## [1] 208 112 After removing the features with low variance, we are left with only 112 features instead of the original 134 features. We can also remove the features with low variance using caret’s preProcess() function, just to specify the method as “nzv”. Note that a separate predict() step is needed to get the transformed dataset. preproc = preProcess(bloodbrain_x, method = &quot;nzv&quot;, freqCut = 2, uniqueCut = 20) bloodbrain_x_small_1 = predict(preproc, bloodbrain_x) dim(bloodbrain_x_small_1) ## [1] 208 112 4.4 Dimentionality Reduction 4.4.1 Principal Component Analysis (PCA) We usually need to do dimensionality reduction for data analysis. PCA is one of the most commonly used method, and can be used simply with base R’s function prcomp(). We use base R’s mtcars dataset as an example. mtcars.pca &lt;- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE) predict(mtcars.pca, mtcars) ## PC1 PC2 PC3 PC4 PC5 ## Mazda RX4 -0.66422351 1.1734476 -0.20431724 -0.12601751 0.75200784 ## Mazda RX4 Wag -0.63719807 0.9769448 0.11077779 -0.08567709 0.65668822 ## Datsun 710 -2.29973601 -0.3265893 -0.21014955 -0.10862524 -0.07622329 ## Hornet 4 Drive -0.21529670 -1.9768101 -0.32946822 -0.30806225 -0.24391787 ## Hornet Sportabout 1.58697405 -0.8287285 -1.03299254 0.14738418 -0.22270405 ## Valiant 0.04960512 -2.4466838 0.11177774 -0.87154914 -0.12574876 ## Duster 360 2.71439677 0.3610529 -0.65206041 0.09633337 0.29674234 ## Merc 240D -2.04370658 -0.8006412 0.84898795 -0.27451338 -0.26307848 ## Merc 230 -2.29506729 -1.3056004 1.96848450 0.05055875 -0.45988113 ## Merc 280 -0.38252133 0.5811211 0.88632274 0.07026946 0.45835852 ## Merc 280C -0.36652708 0.4121971 1.14860950 0.06150898 0.48309076 ## Merc 450SE 1.88466875 -0.7241198 -0.20604588 -0.21856675 0.27996207 ## Merc 450SL 1.67107231 -0.7144354 -0.32644071 -0.28933625 0.28061777 ## Merc 450SLC 1.77692371 -0.8411687 -0.08557921 -0.28421711 0.34961695 ## Cadillac Fleetwood 3.64958983 -0.9480878 0.88315862 0.21645793 -0.34788247 ## Lincoln Continental 3.71033756 -0.8426945 0.93230325 0.34099021 -0.34260485 ## Chrysler Imperial 3.33196300 -0.4805609 0.67061959 0.65189724 -0.43940743 ## Fiat 128 -3.45236266 -0.4327074 -0.22604214 0.10018032 -0.33470301 ## Honda Civic -3.85477722 0.7084152 -0.22670973 1.19340342 0.53954318 ## Toyota Corolla -3.85488283 -0.3872111 -0.25488964 0.21962306 -0.30372397 ## Toyota Corona -1.90375523 -1.5725638 0.06620424 0.07989679 0.50126570 ## Dodge Challenger 1.80402354 -1.1340965 -1.00776416 -0.58796239 0.09903732 ## AMC Javelin 1.46483534 -0.9777629 -0.76680342 -0.03308788 0.26871378 ## Camaro Z28 2.60135738 0.7649595 -0.48915140 0.95247550 0.53065965 ## Pontiac Firebird 1.87424485 -0.9791561 -0.89787633 0.22438738 -0.50770999 ## Fiat X1-9 -3.14830645 -0.2552569 -0.36230545 0.06406082 0.03361267 ## Porsche 914-2 -2.77939557 1.6373369 -0.35969974 0.31886540 -0.43251030 ## Lotus Europa -2.90895427 1.3962368 -0.91635036 -0.90254314 -0.75861156 ## Ford Pantera L 1.54812696 3.0206982 -0.51945216 0.86560850 -0.86048411 ## Ferrari Dino 0.08049995 2.8346567 0.34481747 -1.14659658 0.29944552 ## Maserati Bora 2.96252801 3.9993896 0.70296512 -0.73000448 -0.22756074 ## Volvo 142E -1.90443632 0.1084190 0.39906976 0.31285789 0.11738974 ## PC6 PC7 PC8 PC9 ## Mazda RX4 -0.12506777 -0.42357334 -0.003259165 -0.167051112 ## Mazda RX4 Wag -0.06619437 -0.44849307 0.056643244 -0.071592094 ## Datsun 710 -0.56693648 0.38612406 -0.202035744 0.114505030 ## Hornet 4 Drive 0.08382435 0.03299362 -0.023714111 -0.145255757 ## Hornet Sportabout 0.18280435 -0.05793795 0.152342587 -0.154646072 ## Valiant -0.23043022 0.22451528 0.098663134 -0.004233901 ## Duster 360 0.27763557 0.44227307 -0.306373481 -0.186980810 ## Merc 240D -0.19042527 -0.39416400 -0.187088365 -0.010461330 ## Merc 230 0.20443847 0.53713423 0.413455512 -0.169005773 ## Merc 280 -0.07984989 -0.26113412 0.204105964 0.110461785 ## Merc 280C -0.16066456 -0.07979514 0.352641772 0.027108266 ## Merc 450SE 0.17135058 -0.08914480 0.092140434 0.396034809 ## Merc 450SL 0.33682412 0.03346598 0.182323579 0.196526577 ## Merc 450SLC 0.13926264 0.20632469 0.295340402 0.147796262 ## Cadillac Fleetwood -0.24002207 -0.31053111 -0.171865268 -0.251117818 ## Lincoln Continental -0.22646211 -0.28589695 -0.239313268 -0.028994385 ## Chrysler Imperial 0.31045750 -0.38304409 -0.359765688 0.223097923 ## Fiat 128 0.57303421 -0.24650594 -0.066340528 0.220271421 ## Honda Civic 0.37207104 -0.20055288 0.087333576 -0.241702175 ## Toyota Corolla 0.83750899 -0.10186868 0.104053562 0.042833437 ## Toyota Corona -0.07212137 0.74680802 -0.408144457 -0.082722856 ## Dodge Challenger -0.33920894 -0.14045443 0.156086022 -0.050247532 ## AMC Javelin -0.31479492 0.03753417 0.370979414 -0.043466032 ## Camaro Z28 0.05970074 0.38212238 -0.289612990 0.082069840 ## Pontiac Firebird 0.20785973 -0.32709161 0.027471038 -0.130958896 ## Fiat X1-9 -0.09586730 0.10352270 -0.020876499 0.021084764 ## Porsche 914-2 -0.69006515 -0.26313120 -0.105695694 0.085027267 ## Lotus Europa 0.05473409 -0.03491081 -0.236552376 -0.046341050 ## Ford Pantera L -0.50704173 0.37940892 0.548070377 0.053196712 ## Ferrari Dino -0.08124583 -0.26924964 -0.123537656 -0.047915313 ## Maserati Bora 0.65580986 0.49422807 -0.082329298 -0.053112079 ## Volvo 142E -0.48091826 0.31102454 -0.315146031 0.165790892 We can plot the percentages of explained variances using fviz_eig() from the library factoextra. fviz_eig(mtcars.pca) In practice, we usually use “elbow” method to select the number of components. In this case, the best number of component is 3. "],["apply-family.html", "Chapter 5 Apply family", " Chapter 5 Apply family Jie Mei and Ningxin Li library(ggplot2) ##Introduction The apply family consists of functions which minimize our need to create loops. It will apply an R function or some R functions to an R data object. The biggest difference between using a function alone and using through apply family is the object class on which the function is applied and which will be returned. We will explain the most common forms of apply functions (apply, sapply, lapply, tapply), and there are also some other apply functions (mapply, rapply, and vapply) which will not be included here. 5.0.1 apply The apply() function is most often used to apply a function to the rows or columns of matrices, arrays and data frames. Apply can be used on lists too. First, let’s see the usage of apply: &gt; apply(X, MARGIN, FUN...) X: an array, a matrix or a dataframe. MARGIN: dimcode. 1 means apply to rows, 2 means apply to columns, and if they have names, we can use a character vector to select. FUN: function …: fargs(the optional parameter set of the function) Then, let’s see the examples: For example: print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/2 } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(z,1,f) ## [,1] [,2] [,3] ## [1,] 0.5 1.0 1.5 ## [2,] 2.0 2.5 3.0 Basicly, we set a simple 3x2 matrix, and a simple function :deviding by 2. And we got a 2x3 matrix, whose three columns means three results after applying the function to each row, and since each row has two values, there are also two values in a result, so the number of rows is two. And, what if we make it more complex: print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/c(2,4) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(z,1,f) ## [,1] [,2] [,3] ## [1,] 0.5 1.00 1.5 ## [2,] 1.0 1.25 1.5 Now, we changed the function to deviding by c(2,4). And we still got a 2x3 matrix, but with different values in the second row. This is because we changed the function to two values, and when it applied, the first value was applied to the first row, and the second value was applied to the second row. What if we have three values in the function: print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/c(2,4,8) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(z,1,f) ## [,1] [,2] [,3] ## [1,] 0.500 1.00 1.500 ## [2,] 1.000 1.25 1.500 ## [3,] 0.125 0.25 0.375 Now, we got a 3x3 matrix, with exactly the same answer we saw in b) and an additional row with some new values. It is easy to find that the third row is the result applying the third function value to the first column, which means when the amount of function value is larger than the amount of objects we want to apply, the objects will be applied cyclically.(like here, we actually applied the function /c(2,4,8) to 1,4,1 in the first row, where 1 is used twice, and we got 0.5,1,0.125) If we try applying to the column: print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/c(2,4,8) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(z,2,f) ## [,1] [,2] ## [1,] 0.500 2.00 ## [2,] 0.500 1.25 ## [3,] 0.375 0.75 Since there are exactly three values in the function, and three values in each column, we got the same size of applied matrix, which is 3*2. The columns in the result matrix just means the results. What if the amount of function values is less than object values: print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/c(2,4) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(z,2,f) ## [,1] [,2] ## [1,] 0.5 2.00 ## [2,] 0.5 1.25 ## [3,] 1.5 3.00 The function values will be used cyclically. Let’s try applying to c(1,2): print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/c(2,4) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(z,c(1,2),f) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 0.50 1.0 1.50 ## [2,] 0.25 0.5 0.75 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 2 2.50 3.0 ## [2,] 1 1.25 1.5 We will get two tables, one is applying to rows, and another is applying to columns. What about dataframe: data.matrix&lt;-matrix(1:12,c(3,4)) d &lt;- data.frame(data.matrix) d ## X1 X2 X3 X4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 f &lt;- function(x) { x*c(1,2,3) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(d,2,f) ## X1 X2 X3 X4 ## [1,] 1 4 7 10 ## [2,] 4 10 16 22 ## [3,] 9 18 27 36 Here we can see that the application on dataframe is quite like matrix. Let’s try some other functions: By columns: print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(d,2,quantile) ## X1 X2 X3 X4 ## 0% 1.0 4.0 7.0 10.0 ## 25% 1.5 4.5 7.5 10.5 ## 50% 2.0 5.0 8.0 11.0 ## 75% 2.5 5.5 8.5 11.5 ## 100% 3.0 6.0 9.0 12.0 By rows: print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(d,1,quantile) ## [,1] [,2] [,3] ## 0% 1.00 2.00 3.00 ## 25% 3.25 4.25 5.25 ## 50% 5.50 6.50 7.50 ## 75% 7.75 8.75 9.75 ## 100% 10.00 11.00 12.00 By using some built-in functions, we can easily do data summary. We can also select which column or row we want to apply on: Select columns, apply column: print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(d[,3:4],2,quantile) ## X3 X4 ## 0% 7.0 10.0 ## 25% 7.5 10.5 ## 50% 8.0 11.0 ## 75% 8.5 11.5 ## 100% 9.0 12.0 Select rows, apply column: print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(d[1:2,],2,quantile) ## X1 X2 X3 X4 ## 0% 1.00 4.00 7.00 10.00 ## 25% 1.25 4.25 7.25 10.25 ## 50% 1.50 4.50 7.50 10.50 ## 75% 1.75 4.75 7.75 10.75 ## 100% 2.00 5.00 8.00 11.00 Select both, apply row: print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(d[1:2,3:4],1,quantile) ## 1 2 ## 0% 7.00 8.00 ## 25% 7.75 8.75 ## 50% 8.50 9.50 ## 75% 9.25 10.25 ## 100% 10.00 11.00 What about array, we use a built-in dataset ‘Titanic’ as an example. Titanic ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0 0 ## 2nd 0 0 ## 3rd 35 17 ## Crew 0 0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 118 4 ## 2nd 154 13 ## 3rd 387 89 ## Crew 670 3 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 5 1 ## 2nd 11 13 ## 3rd 13 14 ## Crew 0 0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 57 140 ## 2nd 14 80 ## 3rd 75 76 ## Crew 192 20 Since this array has more than 2 dimensions, we can use bigger number for MARGIN now. Let’s try apply to different dimensions: apply(Titanic, 1, sum) ## 1st 2nd 3rd Crew ## 325 285 706 885 apply(Titanic, 2, mean) ## Male Female ## 108.1875 29.3750 apply(Titanic, 3, quantile) ## Age ## Child Adult ## 0% 0.0 3.0 ## 25% 0.0 18.5 ## 50% 0.5 78.0 ## 75% 13.0 143.5 ## 100% 35.0 670.0 apply(Titanic, 4, max) ## No Yes ## 670 192 As we can see, there are four dimensions, and we can apply functions to each dimension. Can we apply to more than 1 dimension: apply(Titanic, c(1,2), mean) ## Sex ## Class Male Female ## 1st 45.00 36.25 ## 2nd 44.75 26.50 ## 3rd 127.50 49.00 ## Crew 215.50 5.75 apply(Titanic, c(3,4), sum) ## Survived ## Age No Yes ## Child 52 57 ## Adult 1438 654 apply(Titanic, c(1,2,3), mean) ## , , Age = Child ## ## Sex ## Class Male Female ## 1st 2.5 0.5 ## 2nd 5.5 6.5 ## 3rd 24.0 15.5 ## Crew 0.0 0.0 ## ## , , Age = Adult ## ## Sex ## Class Male Female ## 1st 87.5 72.0 ## 2nd 84.0 46.5 ## 3rd 231.0 82.5 ## Crew 431.0 11.5 f &lt;- function(x) { x/2 } apply(Titanic, c(1,2,3,4), f) ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0.0 0.0 ## 2nd 0.0 0.0 ## 3rd 17.5 8.5 ## Crew 0.0 0.0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 59.0 2.0 ## 2nd 77.0 6.5 ## 3rd 193.5 44.5 ## Crew 335.0 1.5 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 2.5 0.5 ## 2nd 5.5 6.5 ## 3rd 6.5 7.0 ## Crew 0.0 0.0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 28.5 70 ## 2nd 7.0 40 ## 3rd 37.5 38 ## Crew 96.0 10 Of course, we can apply functions to more than one dimensions. 5.0.2 Lapply The lapply function works on a list. It applies the chosen function on a list interating through each elements and returns a list in the end. First, let’s see the usage of lapply: &gt; lapply(X, FUN, ???) X: the targeted list. FUN: the function we want to perform on X. Then, let’s see a sample code: In this example, the data_frame is a data frame and we want to get the total number of rows in lists 2 to 5. Notice here that data frames are consisted of lists. lapply(data_frame[, 2:5], function(x) nrow(x)) %&gt;% unlist() %&gt;% sum() 5.0.3 Sapply The sapply() function is quite like lapply(). The only difference is in the return value. Sapply() will try to simplify the result of lapply() if possible(that is why it called ’s’apply). Actually, sapply() functions like lapply() and then does the following algorithm: ??? If the result is a list with each element’s length equals 1, then it will return a vector. ??? If the result is a list with vectors of the same length, then it will return a matrix. ??? Otherwise, it will return a list. First, let’s see the usage of sapply: &gt; sapply(X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE) If we set simplify = FALSE, and USE.NAMES = FALSE, then it will be the same as lapply(x,FUN) simplify(cannot be abbreviated): logical/character; a vector, matrix or higher dimensional array. TRUE: returns a vector or matrix, whereas “array”: returns an array of “rank??? (=length(dim(.))) one higher than the result of FUN(X[[i]]). USE.NAMES(cannot be abbreviated): logical; TRUE and X is character: use X as names for the result unless it had names already. Then, let’s see the examples: We just use the same dataset, and same function to see the different. print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(Titanic, c(1,2,3,4), mean) ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0 0 ## 2nd 0 0 ## 3rd 35 17 ## Crew 0 0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 118 4 ## 2nd 154 13 ## 3rd 387 89 ## Crew 670 3 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 5 1 ## 2nd 11 13 ## 3rd 13 14 ## Crew 0 0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 57 140 ## 2nd 14 80 ## 3rd 75 76 ## Crew 192 20 print(&quot;This is what we got after sapplying the function:&quot;) ## [1] &quot;This is what we got after sapplying the function:&quot; sapply(Titanic, mean) ## [1] 0 0 35 0 0 0 17 0 118 154 387 670 4 13 89 3 5 11 13 ## [20] 0 1 13 14 0 57 14 75 192 140 80 76 20 We can see very clearly that the ‘apply’ function returns a array, while the ‘sapply’ function returns a vector. If we do not simplify: print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; apply(Titanic, c(1,2,3,4), mean) ## , , Age = Child, Survived = No ## ## Sex ## Class Male Female ## 1st 0 0 ## 2nd 0 0 ## 3rd 35 17 ## Crew 0 0 ## ## , , Age = Adult, Survived = No ## ## Sex ## Class Male Female ## 1st 118 4 ## 2nd 154 13 ## 3rd 387 89 ## Crew 670 3 ## ## , , Age = Child, Survived = Yes ## ## Sex ## Class Male Female ## 1st 5 1 ## 2nd 11 13 ## 3rd 13 14 ## Crew 0 0 ## ## , , Age = Adult, Survived = Yes ## ## Sex ## Class Male Female ## 1st 57 140 ## 2nd 14 80 ## 3rd 75 76 ## Crew 192 20 print(&quot;This is what we got after sapplying the function:&quot;) ## [1] &quot;This is what we got after sapplying the function:&quot; c&lt;-sapply(Titanic, mean, simplify = FALSE, USE.NAMES = FALSE) is.list(c) ## [1] TRUE Then, we will get a list. Let’s go back to the matrix: print(&quot;This is the matrix:&quot;) ## [1] &quot;This is the matrix:&quot; z &lt;- matrix(1:6, nrow = 3) z ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 f &lt;- function(x) { x/c(2,4,8) } print(&quot;This is what we got after applying the function:&quot;) ## [1] &quot;This is what we got after applying the function:&quot; a &lt;-apply(z,c(1,2),f) a ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 0.500 1.00 1.500 ## [2,] 0.250 0.50 0.750 ## [3,] 0.125 0.25 0.375 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 2.0 2.500 3.00 ## [2,] 1.0 1.250 1.50 ## [3,] 0.5 0.625 0.75 print(&quot;This is what we got after sapplying the function:&quot;) ## [1] &quot;This is what we got after sapplying the function:&quot; b &lt;-sapply(z,f) b ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.500 1.00 1.500 2.0 2.500 3.00 ## [2,] 0.250 0.50 0.750 1.0 1.250 1.50 ## [3,] 0.125 0.25 0.375 0.5 0.625 0.75 is.array(a) ## [1] TRUE is.matrix(b) ## [1] TRUE Since the sapply function do not have a MARGIN parameter, we can only apply this to all the dimensions. Here the apply result becomes an array, because we applied the function to both row and column, and the sapply result remains a matrix, which simplified the format. 5.0.4 tapply tapply() is used to apply a function over subsets of a vector: in cases we can group the datasets based on some variables. First, let’s see the usage of tapply: &gt; tapply(x, INDEX, FUN, ..., simplify = TRUE) x: the target numerical or logical vector. INDEX: a factor or a list of factors that divides the dataset into groups FUN: the function we want to apply to each group Then, let’s see an example: In this example, we wanted to examine the average price of diamonds based on cut. The target dataset/vector x is diamonds$price and the groupping is identified by diamonds$cut. head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 tapply(diamonds$price, diamonds$cut, mean, na.rm = TRUE) ## Fair Good Very Good Premium Ideal ## 4358.758 3928.864 3981.760 4584.258 3457.542 "],["writing-sql-in-r.html", "Chapter 6 Writing SQL in R 6.1 1. Introduction 6.2 2. What is SQL? 6.3 3. R Packages 6.4 sqldf 6.5 DPLYR 6.6 DBI 6.7 Conclusion 6.8 Work Cited:", " Chapter 6 Writing SQL in R Jingyuan Liu 6.1 1. Introduction SQL has been one of the go-to solutions for data scientists to perform data analysis and manage data pipelines. This is because SQL is fairly easy to read and learn –almost everybody knows it. As a result, it would be helpful for some to work with SQL under the R framework. This post will explore methods to incorporate SQL in R. 6.2 2. What is SQL? SQL stands for Structured Query Language which let you access and manipulate databases. SQL is designed for managing data in relational database management systems(RDBMS). 6.3 3. R Packages We will be trying out three R packages in this post in the following order: sqldf, DBI, dplyr. 6.3.1 Load packages library(proto) library(gsubfn) library(RSQLite) library(sqldf) library(dplyr) library(DBI) 6.3.2 Dataset We will be using the dataset iris throughout this project. First we use DBI package to copy iris from Dataframe into the database. head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa connection &lt;- dbConnect(SQLite(), &quot;&quot;) dbWriteTable(connection, &#39;iris&#39;, iris) iris_tbl = dbReadTable(connection, &#39;iris&#39;) 6.4 sqldf sqldf is the simplest way to use SQL in R among the three packages. It contains only one function sqldf() which we just pass in the SQL query we want. 6.4.1 SQL Queries sqldf(&quot;SELECT * FROM iris LIMIT 10&quot;) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 6.4.2 Where/And/Or Notice that sqldf does not allow . in column names, so in this case we need to put column name in backticks. sqldf(&quot;SELECT Sepal.Length, Species FROM iris WHERE Species=&#39;setosa&#39; LIMIT 10 &quot; ) ## Error: no such column: Sepal.Length sqldf(&quot;SELECT `Sepal.Length`, Species FROM iris WHERE Species=&#39;setosa&#39; LIMIT 10 &quot; ) ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa 6.4.3 More sqldf Examples Use SELECT COUNT(). sqldf(&quot;SELECT COUNT() FROM iris WHERE Species = &#39;setosa&#39;&quot;) ## COUNT() ## 1 50 Creating new table for data aggregation df = sqldf(&quot;SELECT Species, AVG(`Sepal.Length`) FROM iris GROUP BY Species&quot;) df ## Species AVG(`Sepal.Length`) ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 6.4.4 sqldf Summary: We see that sqldf provided a surprisingly easy way to write SQL in R. The user simply specifies an SQL statement in R using data frame names in place of table names and a database with appropriate table layouts/schema is automatically created. Currently, sqdl supports (1) the SQLite backend database, (2) the H2 java database, (3) the PostgreSQL database and (4) MySQL. 6.5 DPLYR DPLYR provides a way to write your code in consistent with R and translate to SQL with the show_query() function. 6.5.1 DPLYR Queries Select sepal.length and species with species Virginica. iris_tbl &lt;- tbl(connection, &#39;iris&#39;) q1 &lt;- iris_tbl %&gt;% select(Sepal.Length, Species) %&gt;% filter(Species == &#39;virginica&#39;) %&gt;% head(10) show_query(q1) ## &lt;SQL&gt; ## SELECT * ## FROM (SELECT `Sepal.Length`, `Species` ## FROM `iris`) ## WHERE (`Species` = &#39;virginica&#39;) ## LIMIT 10 q1 ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.33.0 [] ## Sepal.Length Species ## &lt;dbl&gt; &lt;chr&gt; ## 1 6.3 virginica ## 2 5.8 virginica ## 3 7.1 virginica ## 4 6.3 virginica ## 5 6.5 virginica ## 6 7.6 virginica ## 7 4.9 virginica ## 8 7.3 virginica ## 9 6.7 virginica ## 10 7.2 virginica 6.5.2 More Examples Count number of rows with species setosa q2 &lt;- iris_tbl %&gt;% select(Sepal.Length, Species) %&gt;% filter(Species == &#39;setosa&#39;) %&gt;% count() show_query(q2) ## &lt;SQL&gt; ## SELECT COUNT(*) AS `n` ## FROM (SELECT `Sepal.Length`, `Species` ## FROM `iris`) ## WHERE (`Species` = &#39;setosa&#39;) q2 ## # Source: lazy query [?? x 1] ## # Database: sqlite 3.33.0 [] ## n ## &lt;int&gt; ## 1 50 q3 &lt;- iris_tbl %&gt;% select(Sepal.Length, Species) %&gt;% group_by(Species) %&gt;% summarize(mean(Sepal.Length, na.rm = TRUE)) show_query(q3) ## &lt;SQL&gt; ## SELECT `Species`, AVG(`Sepal.Length`) AS `mean(Sepal.Length, na.rm = TRUE)` ## FROM (SELECT `Sepal.Length`, `Species` ## FROM `iris`) ## GROUP BY `Species` q3 ## # Source: lazy query [?? x 2] ## # Database: sqlite 3.33.0 [] ## Species `mean(Sepal.Length, na.rm = TRUE)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 6.5.3 DPLYR Summary Dplyr provides a convenient way to translate R syntax into SQL. This feature is particular useful for some who are more familiar with R than SQL. However, Dplyr sometimes generate sub-optimal SQL queries that does not perform well with massive data set. Moreover, it takes sometime to get familiar with Dplyr functions. 6.6 DBI DBI provides a most comprehensive way to use database in R. It allows the user to connect, write, read, and manage database directly with functions includes in the package. In this post, we mainly focus on the query part. 6.6.1 DBI Queries: Again, select first ten rows from iris res &lt;- dbSendQuery(connection, &quot;SELECT * FROM iris LIMIT 10&quot;) dbFetch(res) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa dbClearResult(res) Select sepal.length and species with species setosa. res &lt;- dbSendQuery(connection, &quot;SELECT `Sepal.Length`, Species FROM iris WHERE Species=&#39;setosa&#39; LIMIT 10 &quot;) dbFetch(res) ## Sepal.Length Species ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa ## 7 4.6 setosa ## 8 5.0 setosa ## 9 4.4 setosa ## 10 4.9 setosa dbClearResult(res) Count number of rows with species setosa res &lt;- dbSendQuery(connection, &quot;SELECT COUNT() FROM iris WHERE Species = &#39;setosa&#39;&quot;) dbFetch(res) ## COUNT() ## 1 50 dbClearResult(res) Select the average Sepal.Length for each of the species. res &lt;- dbSendQuery(connection, &quot;SELECT Species, AVG(`Sepal.Length`) FROM iris GROUP BY Species&quot;) dbFetch(res) ## Species AVG(`Sepal.Length`) ## 1 setosa 5.006 ## 2 versicolor 5.936 ## 3 virginica 6.588 dbClearResult(res) dbDisconnect(connection) 6.7 Conclusion In this project we explored different methods to work with SQL in R. The sqldf provides the most intuitive way by the single function sqldf which works directly with R’s dataframe object. Dplyr allows user to translate their R syntax into SQL which is really helpful for some SQL newbies. DBI, on the other hand, is the most powerful database tool at this time. It allows the user to connect and work with database directly. However, it seems not as easy to use as sqldf. 6.8 Work Cited: Dplyr Relational Databases, Alberto Munguia and Chengyi Chen https://jtr13.github.io/cc19/dplyr-relational-databases.html Using DBI https://db.rstudio.com/dbi "],["customized-plot-matrix-pairs-and-ggpairs.html", "Chapter 7 Customized Plot Matrix: pairs and ggpairs 7.1 Overview: Things we can do with pairs() and ggpairs() 7.2 Scatterplot matrix for continuous variables 7.3 Categorical variables 7.4 Outside sources", " Chapter 7 Customized Plot Matrix: pairs and ggpairs Yibai Liu library(GGally) library(ggplot2) library(vcd) 7.1 Overview: Things we can do with pairs() and ggpairs() When our data contains multivariate variables, it is important to evaluate associations between these variables before modeling. We can create scatterplot matrices, correlation matrix, as well as mosaic pairs plots to get an idea about if and how these variables are correlated with each other. In this tutorial, I would plot using a base r function pairs() and a function ggpairs() from the GGally package, which both functions provide methods to generate customized plot matrices. Plots for different purposes: - Scatterplot matrix: correlations between continuous variables - Mosaic pairs plot: correlations between categorical variables 7.2 Scatterplot matrix for continuous variables 7.2.1 Plot with pairs() 7.2.1.1 Basic scatterplot matrix of the `mtcars dataset (all numeric variables) data(mtcars) pairs(~., data = mtcars, main = &quot;Scatterplot Matrix of `mtcars`&quot;) We notice that there are some numeric variables actually discrete or representing categories, so we can trim all discrete and categorical variables, and only plot continuous variables in the matrix. 7.2.1.2 Continuous variables only pairs(mtcars[, c(1,3:7)], main = &quot;Scatterplot Matrix of `mtcars`&quot;) 7.2.1.3 Change color, shape, size of points, as well as labels and gaps of the plot pairs(mtcars[, c(1,3:7)], col = &quot;blue&quot;, # Change color pch = 19, # Change shape of points cex = 0.8, # Change size of points labels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), # Change labels gap = 0.3, # Change gaps in between main = &quot;Scatterplot Matrix of `mtcars`&quot;) 7.2.1.4 Add a smoother to each scatterplot pairs(mtcars[, c(1,3:7)], lower.panel = panel.smooth, # Add a smoother for the lower panel col = &quot;blue&quot;, pch = 19, cex = 0.8, labels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), gap = 0.3, main = &quot;Scatterplot Matrix of `mtcars`&quot;) 7.2.1.5 Separate groups using different colors Tip: You can also highlight a certain level of a categorical variable by simply turn other levels to grey. mtcars$vs &lt;- as.factor(mtcars$vs) pairs(mtcars[, c(1,3:7)], col = c(&quot;blue&quot;,&quot;red&quot;)[mtcars$vs], # Group by variable `vs` pch = 19, cex = 0.8, labels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), gap = 0.3, main = &quot;Scatterplot Matrix of `mtcars` Grouped by Engine&quot;) By separating data points by vs or the engine type, we can see that two groups form distinct clusters for many of the variables. 7.2.1.6 Choose panel display If the plot seems dominated by too many points, you can turn off one of the panels. pairs(mtcars[, c(1,3:7)], col = c(&quot;blue&quot;,&quot;red&quot;)[mtcars$vs], pch = 19, cex = 0.8, labels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), gap = 0.3, upper.panel = NULL, # Turn off the upper panel above the diagonal main = &quot;Scatterplot Matrix of `mtcars`&quot;) 7.2.1.7 Customize your own plot matrix The plot matrix is consisted of multiple panels, e.g. the upper panel, lower panel, diagonal panel, etc. You can customize each panel and make your own plot. #Panel of correlations panel.corr &lt;- function(x, y){ usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- round(cor(x, y), digits=3) txt &lt;- paste0(&quot;Corr: &quot;, r) text(0.5, 0.5, txt, cex = 1) } #Panel of histograms panel.hist &lt;- function(x, ...){ usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(usr[1:2], 0, 1.5) ) h &lt;- hist(x, plot = FALSE) breaks &lt;- h$breaks len &lt;- length(breaks) y &lt;- h$counts/max(h$counts) rect(breaks[-len], 0, breaks[-1], y, col = &quot;lightblue&quot;) } #Panel of scatterplots panel.scat &lt;- function(x, y){ points(x,y, pch = 19, cex = 1, col = &quot;coral&quot;) } #Plot pairs(mtcars[, c(1,3:7)], lower.panel = panel.scat, upper.panel = panel.corr, diag.panel = panel.hist, labels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), gap = 0.3, main = &quot;Scatterplot matrix of `mtcars`&quot;) 7.2.2 Plot with ggpairs() from GGally package 7.2.2.1 Basic ggpairs() plot # You need both ggplot2 and GGally packages loaded to use ggpairs() ggpairs(mtcars[, c(1,3:7)], columnLabels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), upper = list(continuous = wrap(&#39;cor&#39;, size = 4)), title = &quot;Scatterplot matrix of `mtcars`&quot;) 7.2.2.2 Separate groups using different colors ggpairs(mtcars[, c(1,3:7)], columnLabels = c(&quot;Miles&quot;,&quot;Displacement&quot;,&quot;Horsepower&quot;, &quot;Rear axle ratio&quot;,&quot;Weight&quot;,&quot;1/4 mile time&quot;), aes(color = mtcars$vs), # Separate data by levels of vs upper = list(continuous = wrap(&#39;cor&#39;, size = 3)), lower = list(combo = wrap(&quot;facethist&quot;, bins = 30)), diag = list(continuous = wrap(&quot;densityDiag&quot;, alpha = 0.5)), title = &quot;Scatterplot matrix of `mtcars` Grouped by Engine&quot;) 7.3 Categorical variables There are some categorical variables in the dataset mtcars. We can turn these variables and also discrete variables into factors ### Factorize discrete/categorical variables mtcars$cyl &lt;- as.factor(mtcars$cyl) mtcars$am &lt;- as.factor(mtcars$am) mtcars$gear &lt;- as.factor(mtcars$gear) Now we can create a mosaic pairs plot with pairs_diagonal_mosaic() in the vcd package 7.3.1 pairs() plot for categorical variables # You need to load the `vcd` package p &lt;- mtcars[, c(2,8:10)] pairs(table(p), # Here the data needs to be in a table format diag_panel = pairs_diagonal_mosaic(offset_varnames=-2.5)) #move down variable labels 7.3.2 Highlight correlations with shade colors pairs(table(p), diag_panel = pairs_diagonal_mosaic(offset_varnames=-2.5), #move down variable names upper_panel_args = list(shade = TRUE), #set shade colors lower_panel_args = list(shade = TRUE)) 7.4 Outside sources You can check out the following links to find more interesting ways to customize your plot matrix. [R pairs &amp; ggpairs Plot Functions] https://statisticsglobe.com/r-pairs-plot-example/#:~:text=The%20pairs%20R%20function%20returns,pairs%20command%20is%20shown%20above. [ggpairs() r documentation] https://www.rdocumentation.org/packages/GGally/versions/1.5.0/topics/ggpairs [Pairs plot for contingency tables] http://finzi.psych.upenn.edu/R/library/vcd/html/pairs.table.html "],["a-brief-introduction-to-seaborn.html", "Chapter 8 A brief introduction to seaborn", " Chapter 8 A brief introduction to seaborn Rui Qin In the class, we learned how to create graphs of data by ggplot2, a data visualization package in R. Likewise, there are some libraries in Python able to do the same job as ggplot2. Seaborn is a data visualization tool based on Python library, matplotlib. Like ggplot2 in R, seaborn can create multiple kinds of statistical graphs for exploratory and explanatory purpose. In this file I will show some examples of graphs that we have learned in class and I will use three languages, English, Chinese, and Japanese, to briefly explain them. 课上，我们学习并掌握了R语言中通过ggplot2绘制统计图表。Python中也有同样的数据可视化库包括matplotlib和seaborn。他们均可以将数据框中的数据以图标的形式表现出来，以此达到数据分析的目的。这里我将用seaborn展示几组课上学过的常用图表绘制方法。 クラスでggplot2とRを利用し、統計グラフを作るのが勉強しました。同様に、Pythonのデータ可視化ライブラリもあります。よく使われるのはmatplotlibとseabornです。ggplot2のように、seabornは色々な統計グラフを作れます。これから、私はseabornの使い方を説明したいと思います。 Here is the link! "],["geommlbstadiums.html", "Chapter 9 GeomMLBStadiums 9.1 Overview 9.2 Setup 9.3 geom_mlb_stadium 9.4 geom_spraychart 9.5 Other types of Visualizations with GeomMLBStadiums", " Chapter 9 GeomMLBStadiums James Ding 9.1 Overview Baseball is one of the most data driven sports in the world due to the number of games played and the format of the sport. However, since the majority of the people involved in baseball are not statisticians, there is a need for data visualization for easier consumption. One type of data commonly used in the sport is location of batted balls in play. Due to the unique shape of Major League Baseball (MLB) Stadiums, the plotting of this data needs a specialized package, GeomMLBStadiums, developed by Ben Dilday at https://github.com/bdilday/GeomMLBStadiums. 9.2 Setup To install the GeomMLBStadiumspackage, first install devtools. Then use ‘devtools’ to install GeomMLBStadiums from Github, as shown in the code below. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;bdilday/GeomMLBStadiums&quot;) Once the package is downloaded, open up the GeomMLBStadiums library, as well as ggplot2 and dplyr. library(GeomMLBStadiums) library(ggplot2) library(dplyr) 9.3 geom_mlb_stadium To just see the stadium, you can use geom_mlb_stadium with ggplot. The plots are drawn from the data in MLBStadiumsPathData, which is a dataframe included in the package. Below, I show the dimensions of Yankee Stadium. Notice that the parameter stadium_ids takes the team name rather than the stadium name. For example, if you want Wrigley Field, you would set stadium_ids = \"cubs\". Make sure to set stadium_segments = \"all\". By default it is set to \"outfield outer\", which will only show the outer dimension of the stadium. I would also recommend using coord_fixed() so the graph doesn’t get stretched out and theme_void so that the background doesn’t affect the readability of the graph. ggplot() + geom_mlb_stadium(stadium_ids = &quot;yankees&quot;, stadium_segments = &quot;all&quot;) + coord_fixed() + theme_void() If you wanted to see all the stadiums in one plot, you can set stadium_ids = \"all_mlb\" and use facets. ggplot() + geom_mlb_stadium(stadium_ids = &quot;all_mlb&quot;, stadium_segments = &quot;all&quot;) + facet_wrap(~team) + coord_fixed() + theme_void() While this may seem fine at first, it appears that this function accidentally mirrors the stadiums. This is most notable in the Red Sox’s stadium (Fenway Park), which is known for having a short left field with a large wall (known as “The Green Monster”). However, in this plot the Green Monster is clearly in right field. Don’t worry, as this mistake is easily corrected by setting the parameter stadium_transform = TRUE. This also has the added benefit of changing home plate to be at the bottom of each plot, which is a more natural representation of the stadiums for baseball fans. ggplot() + geom_mlb_stadium(stadium_ids = &quot;all_mlb&quot;, stadium_segments = &quot;all&quot;, stadium_transform_coords = TRUE) + facet_wrap(~team) + coord_fixed() + theme_void() 9.4 geom_spraychart This is the most important part of the package, as geom_spraychart allows for the plotting of batted ball data. Nowadays, Major League Baseball teams are moving their defensive players around more and more. These shifts are usually to put defenders in areas where the batter tends to hit the ball, thereby getting more outs. geom_spraychart allows for easy visualizations of batters’ hitting tendencies. To demonstrate this part of the package, I will use real data from MLB’s Statcast database (data from MLB’s Baseball Savant Website, https://baseballsavant.mlb.com/statcast_search). However, if you do not feel like using this data, you can create your own data; all you need is a dataframe of x and y coordinates. 9.4.1 mlbam_xy_transformation mlbam_xy_transformation is a function in the GeomMLBStadiums package that will change the x and y coordinates of the data such that home plate will appear at the bottom and the coordinates of home plate will be (0,0). This function is useful because Statcast presents its data with home plate at the top. Calling this function on a dataframe will add two columns to the dataframe: one for the adjusted x-values and one for the adjusted y-values. Keep in mind that these values must be numeric; in later examples you’ll see me convert these columns to numbers in order to use this function. By default, these columns are named by adding an underscore to the end of the names for the x and y coordinate columns. To see this function in action, take a look at the next section. 9.4.2 Spraycharts For this section, I will be using data from DJ LeMahieu’s 2020 regular season. I only used data on balls in play, as it wouldn’t make sense to plot other pitches. This data was downloaded from the link above as a CSV. DJLeMahieu &lt;- read.csv(&quot;https://raw.githubusercontent.com/dncamp/GeomMLBStadiums-Tutorial/main/DJLeMahieu.csv&quot;) head(DJLeMahieu %&gt;% select(game_date, events, hc_x, hc_y, balls, strikes, outs_when_up, inning)) ## game_date events hc_x hc_y balls strikes outs_when_up inning ## 1 2020-09-27 field_out 113.82 135.79 1 0 2 7 ## 2 2020-09-27 single 81.61 117.79 3 2 1 5 ## 3 2020-09-27 single 127.53 188.21 0 1 1 3 ## 4 2020-09-26 single 135.01 90.69 1 1 2 7 ## 5 2020-09-26 double 78.35 139.04 0 1 1 6 ## 6 2020-09-26 field_out 128.37 195.82 0 1 1 5 The important columns here are hc_x and hc_y, as these are the coordinates where LeMahieu hit the ball. However, since these values are the Statcast values, we will need to use mlbam_xy_transformation to adjust these values to use with our plot. The adjusted values are in columns hc_x_ and hc_y_. DJLeMahieu &lt;- mlbam_xy_transformation(DJLeMahieu) head(DJLeMahieu %&gt;% select(hc_x, hc_y, hc_x_, hc_y_)) ## hc_x hc_y hc_x_ hc_y_ ## 1 113.82 135.79 -27.901602 157.751364 ## 2 81.61 117.79 -108.287165 202.673442 ## 3 127.53 188.21 6.314048 26.928290 ## 4 135.01 90.69 24.981667 270.306126 ## 5 78.35 139.04 -116.423052 149.640433 ## 6 128.37 195.82 8.410411 7.936234 This data is then easily plotted using ggplot2 and geom_spraychart. For the mapping, make sure to set x and y to the adjusted columns. Note that if the parameter stadium_ids is left blank for geom_spraychart, the data will be plotted on a generic field. ggplot(DJLeMahieu, aes(x = hc_x_, y = hc_y_)) + geom_spraychart(stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + coord_fixed() + theme_void() From here there are almost infinite possibilities. For example, we can change the color of each point to represent the type of batted ball (called bb_type in the dataframe). DJLeMahieu$bb_type &lt;- factor(DJLeMahieu$bb_type, levels = c(&quot;ground_ball&quot;, &quot;line_drive&quot;, &quot;fly_ball&quot;, &quot;popup&quot;)) ggplot(DJLeMahieu, aes(x = hc_x_, y = hc_y_, color = bb_type)) + geom_spraychart(stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + coord_fixed() + theme_void() This data helps to show what types of batted balls LeMahieu hits in different locations. For example, LeMahieu the vast majority of balls that LeMahieu hits into the infield are ground balls. This is abnormal because you would expect more infield pop ups if this were an average player. Another option is to change color based on the result of the batted ball (called events in the dataframe). DJLeMahieu$events &lt;- factor(DJLeMahieu$events, levels = c(&quot;single&quot;, &quot;double&quot;, &quot;triple&quot;, &quot;home_run&quot;, &quot;sac_fly&quot;, &quot;field_error&quot;, &quot;field_out&quot;, &quot;force_out&quot;, &quot;fielders_choice_out&quot;, &quot;grounded_into_double_play&quot;, &quot;double_play&quot;)) ggplot(DJLeMahieu, aes(x = hc_x_, y = hc_y_, color = events)) + geom_spraychart(stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + scale_color_brewer(palette = &quot;Set3&quot;) + coord_fixed() + theme_void() While most of this data is fairly standard, one data point seems to stick out in particular. One home run in the top right of the plot is well inside the boundary of the stadium, and it seems to be further from the wall than some of Lemahieu’s non-home run hits. To see an explanation for this, we can plot Lemahieu’s data in his home stadium, Yankee Stadium. ggplot(DJLeMahieu, aes(x = hc_x_, y = hc_y_, color = events)) + geom_spraychart(stadium_ids = &quot;yankees&quot;, stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + scale_color_brewer(palette = &quot;Set3&quot;) + coord_fixed() + theme_void() From this plot we can see that the outlier home run in may be the result of Yankee Stadium’s “short porch”. GeomMLBStadiums allows for this easy comparison between stadiums. 9.5 Other types of Visualizations with GeomMLBStadiums While these spray charts may be the good for the data used above, it sometimes these plots, which are basically a variant of scatter plots, may not be the best solution. For example the data used above was taken during the 2020 MLB season which was shortened to 60 games due to COVID-19. If the data were over a full MLB season (162 games) or even over multiple seasons, the resulting spray chart could get quite messy. For example, below I plotted a spraychart for the entire New York Yankees team in 2019. yankees &lt;- read.csv(&quot;https://raw.githubusercontent.com/dncamp/GeomMLBStadiums-Tutorial/main/Yankees.csv&quot;) yankees &lt;- yankees %&gt;% filter(hc_x != &quot;null&quot;) yankees$hc_x &lt;- as.numeric(yankees$hc_x) yankees$hc_y &lt;- as.numeric(yankees$hc_y) yankees &lt;- mlbam_xy_transformation(yankees) ggplot(yankees, aes(x = hc_x_, y = hc_y_)) + geom_spraychart(stadium_ids = &quot;yankees&quot;, stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + coord_fixed() + theme_void() There is too much data, so a simple spraychart is too hard to read. In these cases, a different visualization may be necessary. 9.5.1 Heatmaps with GeomMLBStadiums As with other geoms in ggplot2, you can add multiple geoms to the plot. Below I used both geom_hex and geom_mlb_stadium to create a heatmap of the data. Keep in mind that the last geom is the one that will appear on top, so I would suggest plotting geom_hex first and geom_mlb_stadium second. ggplot(yankees, aes(x = hc_x_, y = hc_y_)) + geom_hex() + scale_fill_distiller(palette = &quot;Reds&quot;, direction = 1) + geom_mlb_stadium(stadium_ids = &quot;yankees&quot;, stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + coord_fixed() + theme_void() This resulting visualization is much easier to read compared to the original spraychart. This is because geom_hex summarizes the amount of data points within each hexagon, so the clutter of the spray chart is avoided. 9.5.2 stat_summary_hex with GeomMLBStadiums Another great way to use the package is in conjunction with stat_summary_hex. Recently, the term “launch angle” has started to gain popularity with fans and analysts, but what constitutes a good launch angle? Using stat_summary_hex we can graph hit locations like we have done before, but we can also use color to indicate the launch angle of balls hit in that location as well. To do so, in the mapping we just need to set z = launch_angle. yankees$launch_angle &lt;- as.numeric(yankees$launch_angle) ggplot(yankees, aes(x = hc_x_, y = hc_y_, z = launch_angle)) + stat_summary_hex() + scale_fill_distiller(palette = &quot;YlOrRd&quot;, direction = 1) + geom_mlb_stadium(stadium_ids = &quot;yankees&quot;, stadium_transform_coords = TRUE, stadium_segments = &quot;all&quot;) + coord_fixed() + theme_void() From the above visualization, it seems that the ideal launch angle is between 25 and 35 degrees if the goal is to hit a home run. Batted balls with high launch angles tend to end up in foul territory or in the shallow part of the outfield. Batted balls with low launch angles ended up in the infield. "],["radar-chart.html", "Chapter 10 Radar Chart 10.1 Introduction of Radar Chart 10.2 Basic Radar Chart 10.3 Customize Radar Chart 10.4 Radar Chart with several individuals 10.5 Reference", " Chapter 10 Radar Chart Mohan Duan and Run Zhang Here we introduce two kinds of plot that not covered in our class: radar chart. 10.1 Introduction of Radar Chart A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. Pros: great tool to compare different entities easily. easier for reader to understood than a column diagram. useful in drawing comparisons on the basis of different parameters. Cons: if there are so many variables to compare, radar chart can be over-crowded. not ideal for making trade-off decisions or comparing vastly distinctive variables. radar charts can distort data to some extent. Here we explain a tool for drawing radar chart in R 10.2 Basic Radar Chart Using package ‘fmsb’ #install.packages(&#39;fmsb&#39;) library(fmsb) creating data of the nine enneagram type descriptions for person A personA &lt;- as.data.frame(matrix(sample(2:9, 9, replace = T), ncol = 9)) colnames(personA) &lt;- c(&quot;Reformer&quot;, &quot;Helper&quot;, &quot;Achiever&quot;, &quot;Individual&quot;, &quot;Investigator&quot;, &quot;Loyalist&quot;, &quot;Enthusiast&quot;, &quot;Challenger&quot;, &quot;Peacemaker&quot;) personA ## Reformer Helper Achiever Individual Investigator Loyalist Enthusiast ## 1 5 4 5 7 3 9 8 ## Challenger Peacemaker ## 1 9 8 set the min and max of each personality to show on the plot personA &lt;- rbind(rep(10,9) , rep(0,9) , personA) Using ‘radarchart()’ to plot radar graph radarchart(personA) 10.3 Customize Radar Chart The ‘radarchart()’ function offers several options to customize the chart: Polygon features: ‘pcol’ → line color ‘pfcol’ → fill color ‘plwd’ → line width Grid features: ‘cglcol’ → color of the net ‘cglty’ → net line type (see possibilities) ‘axislabcol’ → color of axis labels ‘caxislabels’ → vector of axis labels to display ‘cglwd’ → net width Labels: ‘vlcex’ → group labels size radarchart( personA , axistype=1 , ##custom polygon pcol=rgb(0.2,0.5,0.5,0.9) , pfcol=rgb(0.2,0.5,0.5,0.5) , plwd=4 , ##custom the grid cglcol=&quot;grey&quot;, cglty=1, axislabcol=&quot;grey&quot;, caxislabels=seq(0,10,2.5), cglwd=0.8, ##custom labels vlcex=0.8 ) 10.4 Radar Chart with several individuals 10.4.1 basic plot For next application, we are going to measure data scientists based on the skills required for data scientists. * Required Skills for a Data Scientist: * Programming: SQL,Python, R, JAVA,MATLAB * Machine Learning(ML): Natural Language Processing, Classification, Clustering,Ensemble methods, Deep Learning * Visualization: Tableau, SAS, D3.js, Python, Java, R libraries * Big Data: MongoDB, Oracle, Microsoft Azure, Cloudera * Communication Reference: https://www.mastersindatascience.org/careers/data-scientist/ create data ds_data &lt;- as.data.frame(matrix( sample( 0:20 , 15 , replace=F) , ncol=5)) colnames(ds_data) &lt;- c(&quot;Programming&quot; , &quot;ML&quot; , &quot;Visulization&quot; , &quot;Big Data&quot;,&quot;Communication&quot; ) rownames(ds_data) &lt;- paste(&quot;data scientist&quot; , letters[1:3] , sep=&quot;-&quot;) ds_data ## Programming ML Visulization Big Data Communication ## data scientist-a 7 4 16 2 9 ## data scientist-b 20 15 5 14 19 ## data scientist-c 12 1 17 6 18 ds_bind &lt;- rbind(rep(20,5) , rep(0,5) , ds_data) ## plot with default options: radarchart(ds_bind) 10.4.2 customize The ‘radarchart()’ function offers several options to customize the chart: Polygon features: ‘pcol’ → line color ‘pfcol’ → fill color ‘plwd’ → line width Grid features: ‘cglcol’ → color of the net ‘cglty’ → net line type (see possibilities) ‘axislabcol’ → color of axis labels ‘caxislabels’ → vector of axis labels to display ‘cglwd’ → net width Labels: ‘vlcex’ → group labels size ## Color vector colors_border=c( rgb(0.2,0.5,0.5,0.9), rgb(0.8,0.2,0.5,0.9) , rgb(0.7,0.5,0.1,0.9) ) colors_in=c( rgb(0.2,0.5,0.5,0.4), rgb(0.8,0.2,0.5,0.4) , rgb(0.7,0.5,0.1,0.4) ) ## plot with default options: b&lt;- radarchart( ds_bind , axistype=1 , ##custom polygon pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1, ##custom the grid cglcol=&quot;grey&quot;, cglty=1, axislabcol=&quot;grey&quot;, caxislabels=seq(0,20,5), cglwd=0.8, ##custom labels vlcex=0.8 ) b ## NULL ## Add a legend legend(x=0.7, y=1, legend = rownames(ds_bind[-c(1,2),]), bty = &quot;n&quot;, pch=20 , col=colors_in , text.col = &quot;grey&quot;, cex=1.2, pt.cex=3) 10.4.3 Radar vs. Bar Chart library(ggplot2) library(tidyverse) library(dplyr) tidy_ds &lt;- ds_data %&gt;% tibble::rownames_to_column( &quot;scientist&quot;) tidy_ds &lt;- pivot_longer(tidy_ds,!c(&quot;scientist&quot; ), names_to = &quot;skill&quot;, values_to = &quot;score&quot;) ggplot(tidy_ds, aes(x = skill, y = score, fill = scientist)) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) 10.4.4 Using package ‘ggradar’ devtools::install_github(&quot;ricardo-bion/ggradar&quot;, dependencies = TRUE) library(ggradar) library(dplyr) library(scales) library(tibble) data(USArrests) new_data = USArrests%&gt;% as_tibble(rownames = &quot;group&quot;) %&gt;% mutate_at(vars(-group), rescale)%&gt;% filter(group %in% c(&quot;California&quot;,&quot;New York&quot;,&quot;New Jersey&quot;,&quot;Maryland&quot;)) ggradar(new_data) 10.5 Reference https://www.r-graph-gallery.com/142-basic-radar-chart.html "],["likert-scale-definition-examples-and-visualization.html", "Chapter 11 Likert Scale: Definition, Examples, and Visualization 11.1 What is Likert Scale? 11.2 Test Your Understanding of Likert Scale 11.3 Visualization of a Sample Likert Scale Data: Explore Climate Change in the American Mind 11.4 Arguments on each type of charts 11.5 Reference", " Chapter 11 Likert Scale: Definition, Examples, and Visualization Jingyi An, Tingyi Lu library(foreign) # use `foreign` to import SPSS files library(likert) library(tidyverse) library(ggthemes) library(ggplot2) library(HH) 11.1 What is Likert Scale? Likert scale is a psychometric scale that is mostly applied for scaling responses in surveys / questionnaires. One can apply Likert-type questions to measure people’s feeling or perception on a variety of things like Satisfaction, Frequency, Agreement, likelihood, experience, and so on. Importantly, Likert scaling requires that distances between each response option are equal. This means, for instance, one cannot form a Likert-scale responses like the following: Strongly Disagree, Neutral, Somehow Agree, Strongly Agree. Another important property of Likert scale is its bipolarity, which means it measures either the positive or negative side of a particular statement. Another way to explain this: one should observe symmetry and balance in a well-designed Likert scaling. The number of positive and negative responses should be symmetric about the “neutral” option. A 3-level Likert scale that satisfies the properties could be: Disagree Neutral Agree. A 5-level Likert scale could be: Strongly Disagree Somehow Disagree Neutral Somehow Agree Strongly Agree Sometimes it’s not necessary to have a “neutral” option while one can still design a symmetric and balance Liker Scale. For instance, a 4-level Likert Scale could be: Very Unlikely Somehow Unlikely Somehow Likely Very Likely 11.2 Test Your Understanding of Likert Scale Which of the following is a well-designed Likert scale? Why or why not? A. Not Satisfied, Somehow Satisfied, Very Satisfied B. Do more harm than good, Make no difference, Do more good than harm C. No Impact, Moderate Impact, Huge Impact 11.3 Visualization of a Sample Likert Scale Data: Explore Climate Change in the American Mind (You can find the data, codebook, and other documentation related to the data at: https://osf.io/jw79p/) There are many ways to visualize Likert-scale data, and one of the most used would be bar charts. Among all kinds of bar charts, stacked bar charts, faceted bar charts, and diverging stacked bar charts are widely used based on one’s unique visualization needs. In this section, we would introduce how to visualize the sample dataset using these three kinds of bar charts, by ggplot2 and HH package, and what are some choices one would face when making visualization decisions. 11.3.1 Overview of the Data # import dataset CCAM &lt;- read.spss(&quot;https://osf.io/gb8v4/download&quot;, to.data.frame = TRUE) Description on the dataset: The dataset contains 19 rounds of nationally representative surveys, measuring global warming beliefs and attitude, risk perceptions, policy preferences and information acquisition behaviors, of U.S. adults aged 18 and older, conducted by the Yale Program on Climate Change COmmunication (YPCCC) and the George Mason University Center for Climate Change Communication (Mason 4C) between 2008 and 2018. It contains 20,024 observations and 102 variables. Variables to be observed: For the purpose of this module, we\"ll be selecting 3 variables of Likert Scale: reg_CO2_pollutant, reg_utilities, fund_research. reg_CO2_pollutant: How much do you support or oppose the following policies? Regulate carbon dioxide (the primary greenhouse gas) as a pollutant. reg_utilities: How much do you support or oppose the following policies? Require electric utilities to produce at least 20% of their electricity from wind, solar, or other renewable energy sources, even if it costs the average household an extra $100 a year. fund_research: How much do you support or oppose the following policies? Fund more research into renewable energy sources, such as solar and wind power. The possible responses to each of the 3 questions are: Strongly oppose, Somewhat oppose, Somewhat agree, Strongly agree, and Refused. Before visualize the data, one can create a summary table before doing the visualization to gain a better understanding of each question (can be either about frequency or proportion): # subset the data to selected variables CCAM_sub = CCAM[, c(&quot;reg_CO2_pollutant&quot;, &quot;reg_utilities&quot;, &quot;fund_research&quot;)] # Two ways to create a summary table # Use `table()` # frequency / count summary_q &lt;- rbind(table(CCAM_sub$reg_CO2_pollutant), table(CCAM_sub$reg_utilities), table(CCAM_sub$fund_research)) rownames(summary_q) &lt;- c(&quot;Require producing\\n20% of electricity\\nfrom clean energy&quot;, &quot;Regulate CO2\\nas a pollutant&quot;, &quot;Fund more research\\ninto renewable energy&quot;) summary_q ## Refused ## Require producing\\n20% of electricity\\nfrom clean energy 540 ## Regulate CO2\\nas a pollutant 486 ## Fund more research\\ninto renewable energy 542 ## Strongly oppose ## Require producing\\n20% of electricity\\nfrom clean energy 2147 ## Regulate CO2\\nas a pollutant 2676 ## Fund more research\\ninto renewable energy 1371 ## Somewhat oppose ## Require producing\\n20% of electricity\\nfrom clean energy 3065 ## Regulate CO2\\nas a pollutant 3479 ## Fund more research\\ninto renewable energy 2209 ## Somewhat support ## Require producing\\n20% of electricity\\nfrom clean energy 9605 ## Regulate CO2\\nas a pollutant 6706 ## Fund more research\\ninto renewable energy 9198 ## Strongly support ## Require producing\\n20% of electricity\\nfrom clean energy 6049 ## Regulate CO2\\nas a pollutant 4043 ## Fund more research\\ninto renewable energy 9096 # proportion summary_q_prop &lt;- rbind(prop.table(table(CCAM_sub$reg_CO2_pollutant)), prop.table(table(CCAM_sub$reg_utilities)), prop.table(table(CCAM_sub$fund_research))) rownames(summary_q_prop) &lt;- c(&quot;Regulate Carbon Dioxide as a pollutant&quot;, &quot;Produce 20% of electricity from clean energy&quot;, &quot;Fund more research into renewable energy&quot;) summary_q_prop ## Refused Strongly oppose ## Regulate Carbon Dioxide as a pollutant 0.02522657 0.10029898 ## Produce 20% of electricity from clean energy 0.02794710 0.15388154 ## Fund more research into renewable energy 0.02417916 0.06116167 ## Somewhat oppose Somewhat support ## Regulate Carbon Dioxide as a pollutant 0.14318415 0.4487060 ## Produce 20% of electricity from clean energy 0.20005750 0.3856239 ## Fund more research into renewable energy 0.09854568 0.4103319 ## Strongly support ## Regulate Carbon Dioxide as a pollutant 0.2825843 ## Produce 20% of electricity from clean energy 0.2324899 ## Fund more research into renewable energy 0.4057816 11.3.2 Stacked Bar Chart ggplot2 has done a good job for basic visualization needs like stacked bar charts. In this subsection, we would visualize using ggplot2 for stacked bar charts with 2 choices: with and without the “refused/neutral” option. # prepare data for plotting CCAM_tidy &lt;- CCAM_sub %&gt;% pivot_longer(c(1:3), names_to = &quot;Question&quot;, values_to = &quot;Response&quot;) CCAM_tidy &lt;- CCAM_tidy %&gt;% drop_na(Response) CCAM_summary &lt;- CCAM_tidy %&gt;% group_by(Question, Response) %&gt;% summarize(Freq = n()) %&gt;% mutate(prop = Freq/sum(Freq)) CCAM_summary$Response &lt;- factor(CCAM_summary$Response, levels = c(&quot;Strongly oppose&quot;, &quot;Somewhat oppose&quot;, &quot;Refused&quot;, &quot;Somewhat support&quot;, &quot;Strongly support&quot;)) # create a theme for plotting likert data likert_theme &lt;- theme_gray() + theme(text = element_text(size = 11), plot.title = element_text(size = 13, face = &quot;bold&quot;, margin = margin(10, 0, 10, 0)), plot.margin = unit(c(2.4,0,2.4,.4), &quot;cm&quot;), plot.subtitle = element_text(face = &quot;italic&quot;), legend.title = element_blank(), legend.key.size = unit(.7, &quot;line&quot;), legend.background = element_rect(fill = &quot;grey90&quot;), panel.grid = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.background = element_blank()) # create x labels for questions q_lab &lt;- c(&quot;reg_utilities&quot; = &quot;Require producing\\n20% of electricity\\nfrom clean energy&quot;, &quot;reg_CO2_pollutant&quot; = &quot;Regulate CO2\\nas a pollutant&quot;, &quot;fund_research&quot; = &quot;Fund more research\\ninto renewable energy&quot;) # ggplot2 stacked bar chart with refused ggplot(data = CCAM_summary, aes(Question, prop, fill = fct_rev(Response))) + likert_theme + theme(legend.position = &quot;bottom&quot;) + geom_col(position = &quot;fill&quot;) + geom_text(aes(Question, prop, label = as.integer(100*prop)), # add percentage position = position_stack(vjust = .5), fontface = &quot;bold&quot;) + # center the label scale_fill_brewer(type = &quot;div&quot;, palette = &quot;RdBu&quot;) + # use a diverging fill coord_flip() + scale_x_discrete(labels = q_lab) + ggtitle(&quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, subtitle = &#39;% (Attitude) on &quot;The government should __&quot;&#39;) # ggplot2 stacked bar chart with refused removed CCAM_rmrf &lt;- CCAM_tidy[CCAM_tidy$Response!=&quot;Refused&quot;,] CCAM_rmrf_summary &lt;- CCAM_rmrf %&gt;% group_by(Question, Response) %&gt;% summarize(Freq = n()) %&gt;% mutate(prop = Freq/sum(Freq)) ggplot(data = CCAM_rmrf_summary, aes(Question, prop, fill = fct_rev(Response))) + likert_theme + theme(legend.position = &quot;bottom&quot;) + geom_col(position = &quot;fill&quot;) + geom_text(aes(Question, prop, label = as.integer(100*prop)), # add percentage position = position_stack(vjust = .5), fontface = &quot;bold&quot;) + # center the label scale_fill_brewer(type = &quot;div&quot;, palette = &quot;RdBu&quot;) + # use a diverging fill scale_x_discrete(labels = q_lab) + coord_flip() + ggtitle(&quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, subtitle = &#39;% (Attitude) on &quot;The government should __&quot;, with &quot;Refused&quot; removed&#39;) From the above plots, one may observe that, though stacked bar charts are easily created, the major disadvantage is that the comparison of non-polar responses are not straightforward. The comparison of response “Strongly Disagree” and “Strongly Agree” are obvious between questions but not for “Somewhat Disagree” and “Somewhat Agree”. Thus, it is usually practical to include percentage labels for each bar to overcome this problem, like what’s done in both charts. 11.3.3 Faceted Bar Charts A faceted stacked bar chart would create a bar chart that facets on each response option. This makes it easier to compare each response option across different questions than a simple stacked bar chart. We will still use ggplot2 to create a faceted bar chart. ggplot(data = CCAM_summary) + likert_theme + theme(strip.background = element_blank(), axis.title = element_blank(), panel.background = element_blank(), legend.position = &quot;none&quot;) + geom_col(aes(Question, prop, fill = Response)) + geom_text(aes(Question, prop, label = as.integer(100*prop)), # add percentage fontface = &quot;bold&quot;, position = position_stack(vjust = 0.5)) + # center the label coord_flip() + facet_wrap(.~Response, nrow = 1) + scale_x_discrete(labels = q_lab) + scale_fill_brewer(type = &quot;div&quot;) + ggtitle(&quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, subtitle = &#39;% (Attitude) on &quot;The government should __&quot;&#39;) One can also have the “Refused / Neutral” response option removed if including the choice doesn’t have too much meaning. In our case, the percentage of “Refused” is relatively little and is the same for each question, so we feel comfortable to remove the “Refused” result for all questions. Below is the graph without the “Refused”. ggplot(data = CCAM_rmrf_summary) + likert_theme + theme(strip.background = element_blank(), axis.title = element_blank(), panel.background = element_blank(), legend.position = &quot;none&quot;) + geom_col(aes(Question, prop, fill = Response)) + geom_text(aes(Question, prop, label = as.integer(100*prop)), # add percentage position = position_stack(vjust = .5), fontface = &quot;bold&quot;) + # center the label scale_fill_brewer(type = &quot;div&quot;) + # use a diverging fill scale_x_discrete(labels = q_lab) + facet_wrap(.~Response, nrow = 1) + coord_flip() + ggtitle(&quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, subtitle = &#39;% (Attitude) on &quot;The government should __&quot;, with &quot;Refused&quot; removed&#39;) 11.3.4 Diverging Stacked Bar Chart Last but not least, one may visualize Likert scale by a diverged stacked bar chart, which has its positive and negative options heading on different directions with neutral option at the center. This type of charts make it more straightforward to compare the overall positive and negative opinions on a certain matter. An easy way to create a diverged stacked bar chart is the likert function of the HH package. summary_freq &lt;- as.data.frame(summary_q) # likert function uses a summary table summary_freq &lt;- rownames_to_column(summary_freq, &quot;Question&quot;) summary_freq &lt;- relocate(summary_freq, &quot;Refused&quot;, .after = &quot;Somewhat oppose&quot;) # Plot diverging stacked bar chart centered at &quot;Refused&quot; likert(Question ~., data = summary_freq, as.percent = &quot;noRightAxis&quot;, main = &quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, ReferenceZero = 3, ylab = NULL) Similarly, one can have “Refused” removed if it doesn’t have too much meaning to the visualization. summary_freq &lt;- as.data.frame(summary_q) # likert function uses a summary table summary_freq &lt;- rownames_to_column(summary_freq, &quot;Question&quot;) summary_freq &lt;- relocate(summary_freq, &quot;Refused&quot;, .after = &quot;Somewhat oppose&quot;) summary_freq_rmrf &lt;- summary_freq[c(-4)] # Plot diverging stacked bar chart with &quot;Refused&quot; removed likert(Question ~., data = summary_freq_rmrf, as.percent = &quot;noRightAxis&quot;, main = &quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, ylab = NULL) The likert function also allows to order from the most positive by adjusting one of its variables. # setting the positive.order to TRUE likert(Question ~., data = summary_freq_rmrf, as.percent = &quot;noRightAxis&quot;, main = &quot;U.S. Adults Opinions on Climate Change-Relevant Regulations&quot;, ylab = NULL, positive.order = TRUE) Again, the diverging stacked bar chart makes it really easy to compare the positive against the negative on each question and compare the positive and negative across different questions. The cons, however, is that it doesn’t do a good job in showing the comparison of each individual response option. For instance, it is not straightforward to tell whether more people strongly support the government to regulate CO2 or to require producing at least 20% of electricity from clean energy. 11.4 Arguments on each type of charts It’s not surprised that people may find 100% stacked bar charts are inferior in visualizing Likert scale than the other two since it doesn’t work quite well in either comparing the positive against the negative or examining each response option. Perhaps it is only good when one has only 2 groups of results to compare against each other, and this way 100% stacked bar chart would be nice comparing differences between the two. When one wants to look at the differences between overall positive and negative results, diverging stacked bar charts would stand out, and they usually work better when the neutral/refused category doesn’t stand in the middle. Thus, it is also practical to either remove the neutral/refused category or put it aside. When one wants to take a closer look on each individual result, faceted bar charts definitely work the best. Some of the articles we think are useful to closely examine the advantages and disadvantages of each type of chart we mentioned in this short tutorial: A blog by Stephen Few on three types of charts: https://www.perceptualedge.com/blog/?p=2239 The case against diverging stacked bars: https://blog.datawrapper.de/divergingbars/ 11.5 Reference Yale Program on Climate Change Communication (YPCCC) &amp; George Mason University Center for Climate Change Communication (Mason 4C). (2020). Climate Change in the American Mind: National survey data on public opinion (2008-2018) [Data file and codebook]. doi: 10.17605/OSF.IO/JW79PBallew, M. T., Leiserowitz, A., Roser-Renouf, C., Rosenthal, S. A., Kotcher, J. E., Marlon, J. R., Lyon, E., Goldberg, M. H., &amp; Maibach, E. W. (2019). Climate Change in the American Mind: Data, tools, and trends. Environment: Science and Policy for Sustainable Development, 61(3), 4-18. doi: 10.1080/00139157.2019.1589300 "],["a-simple-way-to-visualize-geographic-data.html", "Chapter 12 A simple way to visualize geographic data 12.1 Overview 12.2 Introduction 12.3 Application 12.4 Conclusion", " Chapter 12 A simple way to visualize geographic data Haoyu Liu and Zhiheng Jiang 12.1 Overview During lectures and class projects, we encountered several cases that we need to analyze and visualize the data based on states or counties in United States. However, we noticed that, with the methods that we have learned, there exists a limitation that the underlying geometric influence are often ignored. Taking the Cleveland Dot plot as an example, although it is a common way to visualize categorical data, it is hard to reflect the factors that regions contribute when we deal with data associated with states or counties. Facet by region can, of course, show some geometric factors. However, the real scenarios maybe more complex than categorization can reveal. For instance, east and west coast may share some similarities due to their similarities in geometry, while a states and its neighbor states may have some correlation. These geometric factors may play a significant part in the data we collect, but it’s hard to discover these factors without looking into the map. Therefore, we think it is important to implement map into visualizing geometric data, and we will discuss a simple and useful method to achieve this goal. 12.2 Introduction 12.2.1 Load Packages plot_usmap, a useful extension under ggplot2, can visualize the states and the counties of United States. plot_usmap and ggplot2 are the two packages that we need. library(usmap) library(ggplot2) 12.2.2 Boarder Visualization It can plot, as its name suggested, a U.S map with boarders of states and counties. This feature is very helpful in visualizing state or county based data. The following code demonstrates the basic functions of this package. As we can see, it plots the U.S. map with state boarders by default, and it also takes “regions” as an input parameter. “regions” can take a character string in c(“states”,“counties”,“state”,“county”). When regions = “counties”, plot_usmap will visualize map with state boarders and county boarders. plot_usmap() plot_usmap(regions = &quot;states&quot;) plot_usmap(regions = &quot;counties&quot;) ### Input data The input dataframe or table must have a column named as “states” or “fips”, and this column must be a column of strings, which can either be the full name of each state or the capital abbreviation of each state. If a dataframe contains name that is not a U.S. states, this row needs to be removed for this function to work. For example, Puerto Rico is not considered as a U.S. state. 12.2.3 Boarders We can customize the color of boarders in plot_usmap by the parameter “color”, for example: plot_usmap(regions = &quot;states&quot;, color=&quot;red&quot;) We can also label each state by its abbreviation. This can be achieved by simply put the parameter “labels” to TRUE usmap::plot_usmap(&quot;states&quot;, labels = TRUE) 12.2.4 Parts of the Map It can also plot part of the states with or without county boarders. The state selection is achieved by the parameter “include”, which takes a list of character strings. The character strings need to be the capital abbreviation of each state. For example, here is a plot for the western states of the United States: plot_usmap(regions = &quot;counties&quot;,include = c(&quot;CA&quot;, &quot;NV&quot;, &quot;ID&quot;, &quot;OR&quot;, &quot;WA&quot;)) + labs(title = &quot;Counties of Western States&quot;) 12.3 Application Now we understand how to use this library, let us try an example to see how plot_usmap can help visualizing geometric data. In this part, we will use the “Median Housing Price” data of U.S. states in 2020. We found this data from https://worldpopulationreview.com/state-rankings/median-home-price-by-state. This website used the Cleveland Dot plot to visualize the median housing price. As we mentioned, Cleveland Dot plot overlooks any potential geometric factors. Although we can see each state’s value relative to others, we cannot see how their locations effect their values. 12.3.1 Data Collection library(forcats) library(tidyverse) library(rvest) url &lt;- read_html( &quot;https://worldpopulationreview.com/state-rankings/median-home-price-by-state&quot; ) tables &lt;- url %&gt;% html_nodes(&quot;table&quot;) %&gt;% html_table() df &lt;- tables[[1]] df &lt;- setNames(df, c(&quot;state&quot;, &quot;median&quot;)) df &lt;- df[-c(1),] df$median &lt;- as.numeric(gsub(&quot;[\\\\$,]&quot;,&quot;&quot;,df$median)) 12.3.2 Comparison First we show the Cleveland Dot Plot. Here we use plot_usmap for the same data plot_usmap(regions=&quot;states&quot;,data = df, values = &quot;median&quot;, color = &quot;red&quot;) + scale_fill_continuous(name = &quot;median housing price (2020)&quot;, label = scales::comma) + theme(legend.position = &quot;right&quot;) + ggtitle(&quot;The Median Housing Price by States in U.S. (2020)&quot;) 12.4 Conclusion Based on the layers of ggplot2, plot_usmap can allow some unique and powerful visualization on U.S. map. From the example we demonstrated, we noticed the trend that the coastal states tend to have higher median housing price, and southern states have lower median housing price. Hawaii, an island state which is far away from U.S. continent has the highest median housing price, and this may due to it being one of the most popular touring destinations. "],["d3-js-intereactive-choropleth-map.html", "Chapter 13 d3.js Intereactive Choropleth Map", " Chapter 13 d3.js Intereactive Choropleth Map Heather Zhu There are many practical tools to make visualizations. In addition to using R as we learned in class, D3.js is a JavaScript library for producing dynamic, interactive data visualizations in web browsers. I uploaded an example of creating an interactive choropleth map using D3.js and geojson in this GitHub Page. Here’s the rendered interactive map on heroku. To make the code easier to understand, I created a walkthrough video tutorial. "],["introduction-to-violin-plots.html", "Chapter 14 Introduction to violin plots 14.1 Overview 14.2 Theory 14.3 Basic violin plot 14.4 Grouped violin plot 14.5 Grouped violin plot with split violins 14.6 violin plot versus box plot", " Chapter 14 Introduction to violin plots Yiwen Cai, Xuanhao Wu library(sm) library(zoo) library(ggplot2) library(vioplot) 14.1 Overview A violin plot is a method of plotting the distribution of numeric data across different categories. For example, temperature distribution compared between day and night, or distribution of car prices compared across different car makers. In this tutorial, we will talk about how to plot violin plots in R using both the ggplot and the vioplot package. Also, we will have a detailed discussion on differences between box plots and violin plots and things to consider when using violin plots for visualization. 14.2 Theory Violin plots show the probability density of the data at different values, usually smoothed by a kernel density estimator. Wider sections in violin plots represent a higher probability;the skinnier sections represent a lower probability. The choice of the kernel bandwidth will control the smoothness of the graph. Lower kernel bandwidth generates lumpier plots. However, graphs with lower kernel bandwidth perform better in identifying minor clusters. Therefore, experiments are needed in order to get the ideal kernel bandwidth used for smoothing. The ordering of the group shown in the violin plot is also important as the ordering will give insights to the graph. 14.3 Basic violin plot chickdata &lt;- read.csv(file = &quot;resources/introduction_vioplot_data/chick_weights.csv&quot;) chickdata$feed &lt;- as.factor(chickdata$feed) chickdata$sex &lt;- as.factor(chickdata$sex) head(chickdata) ## id weight sex feed ## 1 1 179 male Horsebean ## 2 2 160 male Horsebean ## 3 3 136 female Horsebean ## 4 4 227 male Horsebean ## 5 5 217 female Horsebean ## 6 6 168 male Horsebean ggplot(chickdata, aes(x = feed, y = weight)) + geom_violin(aes(fill = feed), trim = FALSE) + geom_boxplot(width = 0.1) + theme_classic() + theme(legend.position = &quot;none&quot;) Or we can flip the axes to get a horizontal violin plot. Like horizontal bar charts, swapping axes gives the category labels more room to breathe. And when the sample size is not large, we can present the sample points instead of the box plots. ggplot(chickdata, aes(x = feed, y = weight)) + geom_violin(aes(fill = feed), trim = FALSE, bw = 10) + geom_point()+ coord_flip() + theme_classic() + theme(legend.position = &quot;none&quot;) 14.4 Grouped violin plot Violin plots can also illustrate a second-order categorical variable. You can create groups within each category. ggplot(chickdata, aes(x = feed, y = weight, fill = sex)) + geom_violin(position = position_dodge(0.7), trim = FALSE) + geom_boxplot(position = position_dodge(0.7), color = &quot;white&quot;, width = 0.1, show.legend = FALSE) + theme_classic() 14.5 Grouped violin plot with split violins Instead of drawing separate plots for each group within a category, you can also create split violins.The split violins can help you compare the distributions of each group more directly. Here we use the vioplot package in R to create split violins.To get a one sided plotting of violin plots,you can change the side parameter from the default one “both” to “left” or “right” fchick = chickdata[chickdata$sex == &quot;female&quot;,] mchick = chickdata[chickdata$sex == &quot;male&quot;,] vioplot(weight~feed, data= fchick, col = &quot;palevioletred&quot;, plotCentre = &quot;line&quot;, side = &quot;left&quot;,ylim = c(0,500),names=levels(fchick$feed)) vioplot(weight~feed, data= mchick, col = &quot;lightblue&quot;, plotCentre = &quot;line&quot;, side = &quot;right&quot;,ylim = c(0, 500) ,add = T) legend(&quot;bottomright&quot;, fill = c(&quot;palevioletred&quot;, &quot;lightblue&quot;), legend = c(&quot;female&quot;, &quot;male&quot;), title = &quot;Sex&quot;) Here the horizontal lines inside the one-sided graph represents the median line in different groups. 14.6 violin plot versus box plot While similar to box plot, violin plot is more informative of the variations of the data especially when the data itself is multimodal. In this case a violin plot shows the presence of different peaks, their position and relative amplitude. mu&lt;-2 si&lt;-0.6 bimodal&lt;-c(rnorm(1000,-mu,si),rnorm(1000,mu,si)) uniform&lt;-runif(2000,-4,4) normal&lt;-rnorm(2000,0,3) vioplot(bimodal,uniform,normal,names=c(&quot;bimodal&quot;,&quot;uniform&quot;,&quot;normal&quot;)) boxplot(bimodal,uniform,normal,names=c(&quot;bimodal&quot;,&quot;uniform&quot;,&quot;normal&quot;)) As we can see above, the boxplots are convenient for comparing summary statistics (such as range and quartiles). And violin plots shows the full distribution of the data better. However, violin plots are less popular. Because of their unpopularity, their meaning can be harder to grasp for many readers not familiar with data visualization. In this case,plotting a series of stacked histograms or kernel density distributions can be used as alternatives. "],["base-r-vs-ggplot2.html", "Chapter 15 Base R vs. ggplot2", " Chapter 15 Base R vs. ggplot2 Winnie Gao One of the biggest reasons why more and more people choose to use R nowadays is due to a extremely powerful package called ggplot2 perfectly integrated in R for producing pretty plots with relatively short codes. A lot of people apply ggplot2 in their code to conveniently create exploratory plots. However, sometimes complains are also heard about ggplot2, such as being enforced to create data frame before plotting, a number of graph types unable to be neatly fitted in ggplot2, etc. At the same time, R itself can produce plots without installing packages. Since R basics has fewer restrictions for producing plots, people can create various plots according to their own needs. But for the same reason, its shortcomings are also due to the lack of ready frameworks, so R basics usually requires more code to adjust if people want to draw plots as good-looking as ggplot2. On this page, let us discuss if R basics can also complete what ggplot2 do for some types of graphs that is not that common in our daily practice: Cleveland Dot plot, Parallel Coordinate plot and Mosaic plot. Cleveland Dot Plot Similar with the more common scatter plots, Cleveland dot plots also map two variables of interest onto x-axis and y-axis. The only difference is that, in Cleveland dot plots, on the y-axis are usually some texts, resulting that each line contains only one dot. But we can apply the same procedure for scatter plot here to plot a Cleveland Dot Plot. As an example, let’s use the data set game metascore that is also used in Problem Set2: ggplot(score, aes(x = metascore, y = reorder(title, metascore))) + geom_point() + xlab(&quot;Metascore&quot;) + ylab(&quot;Game Title&quot;) + ggtitle(&quot;Game Metascore&quot;) Let’s see if we can plot with r basics. Luckily, there is a function written in R that can directly plot the Cleveland plot called dotchart. There are several parameters dotchart: 1. x: the values that you want to put in the plot; 2. labels: the values that you want to put on y-axis, usually the texts or string values; 3. cex: how big you want to circle or labels to be. Here is the example: dotchart(score$metascore, labels=score$title, cex=.7) With in one line, we can roughly get a Cleveland dot plot. However, we always want the circles to be ordered with largest values ont the top. To fulfill this, what we need to do is only to reorder our data before we plot it. order_score = score[order(score$metascore),] dotchart(order_score$metascore, labels=order_score$title, cex=.7) title(&quot;Game Metascore&quot;, xlab=&quot;Metascore&quot;, ylab=&quot;Game Title&quot;) Even though the two plots by ggplot2 and r-basics are not exactly the same, both of them convey the some information. And surprisingly, the code using R basics is not longer than that using ggplot2. Parallel Coordinates Plot Then, let’s try parallel Coordinate plots in ggplot2. There is some other useful packages such as GGally, parcoords and MASS that can also create parallel coordinate plots conveniently, but today let’s first use ggplot2 because sometimes we don’t want to load to many packages. As an example, we will use mtcars data set. At the beginning, there is an important step: rescaling. Since different variables might be in different scales, rescaling help better present our data. Here, let’s use std. Also, in order to plot the brand name on the x-axis and other variables on vertical axies, we have to reshape our data frame first. On purpose of a better visualization, let’s group data points with same key together: mtcars = mtcars %&gt;% arrange(key) Now, we have shaped our data in an order that is ready to plot. ggplot(mtcars, aes(x = key, y = value, group = factor(brand))) + geom_path(position = &quot;identity&quot;) + geom_point() Since we do not have too many car brands here, we can also color each line to see which brand it belongs to. ggplot(mtcars, aes(x = key, y = value, color = brand, group = factor(brand))) + geom_path(position = &quot;identity&quot;) + geom_point() Even though there isn’t a function in ggplot2 that can directly produce a parallel coordinate plot, reshaping our data frame and use geom_path can also complete the plot. Next, let’s see how we should do using r basics only. First, let’s read the data frame again. data(mtcars) mtcars = data.frame(scale(mtcars)) mtcars = rownames_to_column(mtcars, var = &quot;brand&quot;) With r basics, we could use line() function to create lines that connect adjacent values for each variables. And for loop can help run through all variables and add the line plots onto the same plot. plot(as.numeric(mtcars[1, 2:ncol(mtcars)]), type = &quot;b&quot;, ylim=c(-2, 3.1), xlab = &quot;key&quot;, ylab = &quot;value&quot;, xaxt = &quot;n&quot;) axis(1, at=1:11, labels=colnames(mtcars)[2:12]) title(xlab = &quot;key&quot;, ylab = &quot;value&quot;) for (i in 2:nrow(mtcars)) { lines(as.numeric(mtcars[i, 2:ncol(mtcars)]), type = &quot;b&quot;) } We can also create a parallel coordinate plot using basic r functions. Basically, as long as we understand what relationships between variables we want to present using our plot, we can make one even only using a single line() function. But obviously, it takes much longer procedure to create. For your interest, we can also apply color to this line plot: plot(as.numeric(mtcars[1, 2:ncol(mtcars)]), type = &quot;b&quot;, ylim=c(-2, 3.1), xlab = &quot;key&quot;, ylab = &quot;value&quot;, xaxt = &quot;n&quot;, col = 1) axis(1, at=1:11, labels=colnames(mtcars)[2:12]) title(xlab = &quot;key&quot;, ylab = &quot;value&quot;) for (i in 2:nrow(mtcars)) { lines(as.numeric(mtcars[i, 2:ncol(mtcars)]), type = &quot;b&quot;, col = i) } However, after careful observation, we can see that there are fewer number of points in this plot. This is because some of our data points have values very close to each other. When we plot these data points in circle and line, one will cover others. To avoid this problem, we can apply some jittering on our plot. Comparing plotting with ggplot2 and r basics, I would recommend you use ggplot2. And it is even more convenient to plot with other packages as I mentioned above. But if you only have a few data points to plot and do not want to load more packages for a single plot, r baiscs is indeed a choice. Mosaic Plot Finally, let’s try one more type of plot: mosaic plot. As a example, we will use the diamond data set in r and try to explore the relationship between cut and clarity. Therefore, our first step is to group the data frame by cut to calculate its corresponding proportion for each level of clarity. data(diamonds) df = diamonds %&gt;% group_by(cut, clarity) %&gt;% summarise(count = n()) %&gt;% mutate(cut.count = sum(count), prop = count/sum(count)) After reshaping the data into the form we want, we can now start to draw using geom_bar. (External resource: https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2) ggplot(df, aes(x = cut, y = prop, width = cut.count, fill = clarity)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;, colour = &quot;black&quot;) + geom_text(aes(label = scales::percent(prop)), position = position_stack(vjust = 0.5)) + # if labels are desired facet_grid(~cut, scales = &quot;free_x&quot;, space = &quot;free_x&quot;) + scale_fill_brewer(palette = &quot;RdYlGn&quot;) + theme(panel.spacing.x = unit(0, &quot;npc&quot;)) + # if no spacing preferred between bars theme_void() R has its own written function for masic plot called mosaicplot(), which can directly produce a mosaic plot by pluging the varaibles you want to plot. t = table(diamonds$cut, diamonds$clarity) mosaicplot(t, main = &quot;Mosaic: cut vs. clarity&quot;, xlab = &quot;cut&quot;, ylab = &quot;clarity&quot;, las = 1) As required, we can change the direction of cutting by specifying the parameter dir inside the function: mosaicplot(t, main = &quot;Mosaic: cut vs. clarity&quot;, dir = c(&quot;h&quot;, &quot;v&quot;), xlab = &quot;clarity&quot;, ylab = &quot;cut&quot;, las = 1) Surprisingly, mosaic plot by r basics takes less time and fewer lines of code. There are some other useful packages that can produce beautiful mosaic plots such as vcd. Conclusion Today, I introduced how to plot Cleveland Dot Plots, Parallel Coordinate Plots and Mosaic Plots using both ggplot2 and written-in R functions. And as I mentioned above, there are always some fantastic package that are built specifically for some types of plots. However, if you do not want to load too many packages in your working directory, R basics is always a choice to work with. From the procedure of building nice plots using written-in functions, we can see that we do not need to stick on a specific function. Instead, as long as we fully understand the relationship and logic behind the data we want to present, we can always free style and use any types of code and any functions we would like. So, after reading this page, why not give a try! External Resource: Cleveland Dot Plot using R: https://stat.ethz.ch/R-manual/R-patched/library/graphics/html/dotchart.html Cleveland Dot Plot using ggplot2: https://edav.info/cleveland.html Parallel Coordinate Plot using ggplot2: https://www.r-graph-gallery.com/parallel-plot-ggally.html Mosaic Plot using r: https://www.tutorialgateway.org/mosaic-plot-in-r/ Mosaic Plot using ggplot2: https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2 "],["visualiztion-tools-in-real-work.html", "Chapter 16 Visualiztion tools in real work", " Chapter 16 Visualiztion tools in real work Gaoyi Shi and Xiaonan Guo We will Introduce several data visualization tools currently used in industry. Companies use different data visualization tools based on their business purposes and requirements. Based on our daily work and some research on our friends’ companies, we have picked the 4 most popular data visualization tools to demonstrate how the industries are using different tools for graphical data analysis. We strongly suggest that our classmates take some time to make themselves familiar with those tools which will be beneficial for seeking data scientist related jobs. Contents Part1: R/Python Plotly Part2: R Shiny App Part3: AWS QuickSight Part4: Tableau Appendix: Python code Here is the link! "],["color-in-r.html", "Chapter 17 Color in R 17.1 Overview 17.2 Discrete Colors 17.3 Continuous Colors 17.4 Resources", " Chapter 17 Color in R Xiaoyu Su library(ggplot2) library(ggplot2movies) library(tidyverse) library(RColorBrewer) library(viridis) library(ggplot2) library(colorspace) library(reshape2) library(dichromat) library(vcd) 17.1 Overview This section covers the syntax of using colors in R (which reviews some of the materials in class) as well as the related packages and tools that I found interesting. Color deployments might not always add analytic sense to the graph, but maybe a pleasant color representation brings comfort and so we have more patience towards it and stair a it longer. It’s always better to look at a graph nicely colored. 17.1.1 Sections Discrete Colors Basics RColorBrewer Viridis Color Picker Continuous Colors Barplot Heatmap Diverging Scales Colorblind 17.2 Discrete Colors 17.2.1 Basics For discrete colors, we can select the exact colors we want using several different representations: common names, color Hex code, or rgb code. The following code shows this. Notice that the Hex code is case insensitive. For rgb, if you don’t specify maxColorValue=255, the default value range is [0,1]. So in this case we need to divide every value by 255. barplot(rep(1, 5), axes = FALSE, space = 0.1, border = &#39;white&#39;, col = c(&#39;pink&#39;, &#39;#BF87B3&#39;, &#39;#8255a1&#39;, rgb(72/255, 32/255, 143/255), rgb(31, 0, 127, maxColorValue=255))) R differentiates between color and fill. Basically, you color the dots and lines but you fill in an area. In ggplot2, the syntax are usually scale_color_something and scale_fill_something respectively. Let’s take a look at the grey scale for an illustration. # color ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + geom_point(aes(color=Species), size=3) + scale_color_grey(start = 0.6, end = 0.1) + theme_bw() # fill ggplot(iris, aes(x=Species, y=Petal.Width)) + geom_boxplot(aes(fill=Species)) + scale_fill_grey(start = 0.8, end = 0.4) + theme_bw() Back to Top 17.2.2 RColorBrewer The RColorBrewer package presents some nice color palettes for discrete color uses. First we can display all the palettes using this: display.brewer.all() Here is how we can use brewer’s palettes in ggplot2: # color ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length)) + geom_point(aes(color=Species), size=3) + scale_color_brewer(palette = &quot;Set2&quot;) # fill ggplot(iris,aes(x=reorder(Species, Sepal.Length, median),y=Sepal.Length)) + geom_boxplot(aes(fill=Species), color=&#39;#80593D&#39;,varwidth=TRUE) + scale_fill_brewer(palette = &quot;Set3&quot;) + labs(x=&#39;Sepal.Length&#39;) It is worthing noting that we can also extract the Hex code directly from RColorBrewer like this: # say we want 5 colors from the palette &#39;Oranges&#39; brewer.pal(n=5, &#39;Oranges&#39;) ## [1] &quot;#FEEDDE&quot; &quot;#FDBE85&quot; &quot;#FD8D3C&quot; &quot;#E6550D&quot; &quot;#A63603&quot; This adds flexibility to our use of the package. So for instance, we can pass the statements like above into the color arguments in vcd mosaic plots (highlighting_fill for mosaic and gp for doubledecker). Below shows a concrete example. (Note: n must at least be 3) doubledecker(Improved ~ Treatment + Sex, data=Arthritis, gp = gpar(fill = brewer.pal(n=3, &#39;Blues&#39;) )) Back to Top 17.2.3 Viridis The viridis package has continuous scales, but we can use it discretely as well. We can pass in the discrete=TRUE argument to make it work. In addition, we can choose the palette by adding options=palette_name. ggplot(iris, aes(x=Petal.Length, y=Petal.Width)) + geom_point(aes(color=Species), size=3) + scale_color_viridis(discrete = TRUE, option=&#39;plasma&#39;) + theme_bw() Back to Top 17.2.4 Color Picker But what if we see a color combination we would like to use without knowing its code or name? Here is a way to meet our demands. Simply take a screenshot and upload the image to this website https://imagecolorpicker.com/, and we can retrieve the color Hex code and rgb code easily by just scrolling over the desired pixel. image Soruce: https://imagecolorpicker.com/ Thus it is easy to extract the colors we want. I picked two kinds of red from the picture above, #f1beb6 and #e48173, and colored a histogram with them. movies &lt;- filter(movies, length &lt; 250) ggplot(data=movies, aes(length)) + geom_histogram(aes(y=..density..), fill=&quot;#f1beb6&quot;, col=&#39;#e48173&#39;, binwidth = 2) Back to Top 17.3 Continuous Colors For continuous scale, we cannot use the RColorBrewer package directly since it only has discrete palettes. But in general, continuous scales has these low and high arguments where you can pass in discrete colors and generate a continuous scale between them. For the syntax, typically, we can use something like scale_fill_gradient. 17.3.1 Barplot We might want to color a bar chart depending on the frequency or count. Beware the way we choose the colors so that the chart doesn’t look perceptually non-uniform (or simply too ugly). To create a perceptually uniformed barplot, we can do the following: # create a artifical dataframe df &lt;- data.frame(x=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;,&#39;G&#39;), count=c(2,10,14,16,20,15,8)) ggplot(df, aes(x=x, y=count)) + geom_col(aes(fill=count)) + scale_fill_gradient(low=&#39;#141f33&#39;,high=&#39;#5f99f3&#39;) To make a continuous scale out of the RColorBrewer package, we can use a function called colorRampPalette. Then upon coloring, we need to specify scale_fill_manual instead. (Note: need to convert y to factors) discrete_colors &lt;- brewer.pal(3, &#39;Greens&#39;) continuous_palette &lt;- colorRampPalette(discrete_colors) ggplot(df, aes(x=x, y=count)) + geom_col(aes(fill=as.factor(count))) + labs(fill =&quot;count&quot;) + scale_fill_manual(values=continuous_palette(7)) Back to Top 17.3.2 Heatmap Some colored heatmaps could look more enjoyable and clearer than others. We could still use the scale_fill_gradient for these plots, but we can try scale_fill_continuous as well. There are also packages like colorspace that does similar things. Feel free to explore these options. ggplot(faithfuld, aes(waiting, eruptions, fill = density)) + geom_tile() + scale_fill_continuous(type=&#39;viridis&#39;) # using the colorspace package ggplot(faithfuld, aes(waiting, eruptions, fill = density)) + geom_tile() + scale_fill_continuous_sequential(palette = &quot;Blues&quot;) Here is another one. library(reshape2) # normalize the data normalized_mtcars &lt;- as.data.frame(apply(mtcars[, 3:7], 2, function(x) (x - min(x))/(max(x)-min(x)))) # melt the dataframe melt_mtcars &lt;- melt(normalized_mtcars) melt_mtcars$car &lt;- rep(row.names(mtcars), 5) # plot ggplot(melt_mtcars, aes(variable, car)) + geom_tile(aes(fill = value), colour = &quot;#f1beb6&quot;, width = 1, size=0.5, height=1) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) Back to Top 17.3.3 Diverging Scales A diverging color scale creates a gradient between three different colors, allowing you to easily identify low, middle, and high values within your data. Here is an example with ggplot2. # generate some data df1 &lt;- data.frame( x = runif(100)-0.5, # 100 uniformly distributed random values y = runif(100), # 100 uniformly distributed random values z1 = rnorm(100)-0.5 # 100 normally distributed random values ) df2 &lt;- data.frame( x = runif(100)+0.5, # 100 uniformly distributed random values y = runif(100), # 100 uniformly distributed random values z1 = rnorm(100)+0.5 # 100 normally distributed random values ) df &lt;- rbind(df1, df2) # plot ggplot(df, aes(x, y)) + geom_point(aes(colour = z1), size=3) + scale_color_gradient2(low = &#39;blue&#39;, mid = &#39;white&#39;, high = &#39;red&#39;) Back to Top 17.3.4 ColorBlind In class we talked about the ggthemes::scale_color_colorblind method. Here are some other approaches. The RColorBrewer package we used before has a selection of colorblindFriendly choices. display.brewer.all(colorblindFriendly = TRUE) The dichromat package provides color schemes to suit the needs of color blind users. This is how the color palettes look like: Here is how you can extract the colors from the package: colorschemes$BrowntoBlue.12 ## [1] &quot;#331A00&quot; &quot;#663000&quot; &quot;#996136&quot; &quot;#CC9B7A&quot; &quot;#D9AF98&quot; &quot;#F2DACE&quot; &quot;#CCFDFF&quot; ## [8] &quot;#99F8FF&quot; &quot;#66F0FF&quot; &quot;#33E4FF&quot; &quot;#00AACC&quot; &quot;#007A99&quot; Back to Top 17.4 Resources Color Picker Website, “https://imagecolorpicker.com/” viridis package introduction R Documentation of the RColorBrewer package. The colorspace package on CRAN. Color Tutorial from stat545.com “Top R Color Palettes to Know For Great Data Visualization” from datanovia.com. "],["illustrate-commonly-used-graphs-in-r.html", "Chapter 18 Illustrate commonly used graphs in R 18.1 Numerical Variables with Abalone and Mtcars 18.2 Categorical Graphs with Arthritis Data", " Chapter 18 Illustrate commonly used graphs in R Annai Wang and Jinyan Lyu 18.1 Numerical Variables with Abalone and Mtcars In this section, we will explore some plots commonly used in ggplot2, such as scatterplot, heatmap to analyze datasets. Since plot is a very straightforward way for us to see relationships (if any) in the datasets, we would like to know more about visualization tools in R to apply them in our study/research so that we could have a better understanding of the story behind the datasets. First, we will look at the most commonly used plots in R, the scatterplots. (The dataset we used here is abalone from ucidata package, and dataset mtcars) head(abalone) ## sex length diameter height whole_weight shucked_weight viscera_weight ## 1 M 0.455 0.365 0.095 0.5140 0.2245 0.1010 ## 2 M 0.350 0.265 0.090 0.2255 0.0995 0.0485 ## 3 F 0.530 0.420 0.135 0.6770 0.2565 0.1415 ## 4 M 0.440 0.365 0.125 0.5160 0.2155 0.1140 ## 5 I 0.330 0.255 0.080 0.2050 0.0895 0.0395 ## 6 I 0.425 0.300 0.095 0.3515 0.1410 0.0775 ## shell_weight rings ## 1 0.150 15 ## 2 0.070 7 ## 3 0.210 9 ## 4 0.155 10 ## 5 0.055 7 ## 6 0.120 8 head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Scatterplot A scatterplot displays the relationship between 2 numeric variables. Each dot represents an observation. For example, we would like to see if there is any relationship between abalone’s length and height(both are numeric variables), we can then draw a scatterplot. plot1 &lt;- ggplot(abalone,aes(length,height))+ geom_point(color=&quot;lightblue&quot;)+ geom_smooth(color=&quot;yellow&quot;)+ geom_rug()+ ggtitle(&quot;scatterplot for length and weight&quot;) plot1 From the plot, we can observe that there is a strong positive relationship between length and height for abalone. Also, geom_rug() illustrate the distribution of dots. We observe that dots are clustered between 0-0.3 for height, and spread seperatly in length. Then we would like to explore more in this relationship, we will separate those observations by sex here to see if sex plays a role between length and height. plot2 &lt;- ggplot(abalone,aes(length,height))+ geom_point(aes(color=factor(sex)),alpha=0.3)+ ggtitle(&quot;scatterplot for length and height based on sex&quot;) plot2 From the plot, we can see such relationships between length and height still holds, regardless of sex. Scatterplot-Marginal Distribution Then we would like to see the marginal distribition for length and height based on sex. We will use ggMarginal() to help us create marginal distribution. We could create histogram, density curve and boxplot to observe marginal distributions. mplot1 &lt;- ggMarginal(plot2,type=&quot;histogram&quot;,fill=&quot;plum&quot;) mplot1 mt_plot &lt;- ggplot(mtcars,aes(mpg,wt))+ geom_point(aes(color=factor(vs)),alpha=0.5)+ ggtitle(&quot;scatterplot for mpg and wt based on vs&quot;) ggMarginal(mt_plot,type=&quot;density&quot;,fill=&quot;slateblue&quot;) mt_plot1 &lt;- ggplot(mtcars,aes(mpg,disp))+ geom_point(alpha=0.3)+ ggtitle(&quot;scatterplot for mpg and disp&quot;) ggMarginal(mt_plot1,type=&quot;boxplot&quot;) 2D Density Curve Now we can create a 2D density curves for numeric variables 1) shell_weight and diameter and 2) rings and whole_weight. The 2D density curve plot helps to avoid overlapping in dataset by dividing the scatterplot into several fragments, which helps us better understand scatterplots. (Fun fact: two plots look pretty like oyster/abalone) plot3 &lt;- ggplot(abalone,aes(diameter,shell_weight))+ geom_point(aes(color=&quot;gray&quot;))+ geom_density2d_filled(alpha=0.5)+ geom_density_2d(size = 0.5, colour = &quot;black&quot;)+ ggtitle(&quot;2D density curve for diameter and shell_weight&quot;) plot3 plot4 &lt;- ggplot(abalone,aes(rings,whole_weight))+ stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;, colour=&quot;white&quot;)+ ggtitle(&quot;2D density curve for rings and whole_weight&quot;) plot4 3D Scatter Plot We use plotly to illustrate the 3D-plot. plot_ly(abalone,x=~diameter,y=~shucked_weight,z=~length,color=~sex) plot_ly(abalone,x=~diameter,y=~shucked_weight,z=~length,color=~rings) Heatmap Heatmap can be applied in both numerical and categorical variables, and it is a graphical representation of data where individual values contained in a matrix are represented as colors. Here are two examples. new_data &lt;- abalone %&gt;% filter(height&lt;=0.3) plot5 &lt;- ggplot(new_data, aes(height, length, fill= sex)) + geom_tile()+ ggtitle(&quot;heatmap for height and length based on sex&quot;) plot5 plot6 &lt;- ggplot(abalone,aes(diameter,rings,fill=shell_weight))+ geom_tile()+ ggtitle(&quot;heatmap for diameter and rings based on shell_weight&quot;) plot6 Bubble Chart Bubble chart is also used for three numeric variables, and it is an extension of the scatterplot. Thus bubble chart is a visualization tool for us to see relationships among three numeric variables. plot7 &lt;- ggplot(abalone,aes(diameter,viscera_weight,size=length,color=sex))+ geom_point(alpha=0.2)+ scale_size(range=c(0.5,8))+ ggtitle(&quot;bubble chart for diameter,viscera_weight and length based on sex&quot;) plot7 plot8 &lt;- ggplot(mtcars,aes(wt,mpg))+ geom_point(aes(size=qsec,color=as.factor(cyl)),alpha=0.3)+ scale_color_manual(values = c(&quot;sky blue&quot;, &quot;pink&quot;, &quot;coral&quot;)) + scale_size(range=c(0.5,12))+ ggtitle(&quot;bubble chart for wt,mpg and qsec based on cyl&quot;) plot8 18.2 Categorical Graphs with Arthritis Data Bubble Chart We use Arthritis data set in the vcd package where data comes from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. A double-blind trial helps us reduce the potential effects of research bias when collecting data. We want to analyze whether the new treatment works, which affect on the variable improved. To start our visualization analysis, we want to view our data first. We could find there are five variables: three of them are factors; age is a numerical variable; and ID is the number assigned to each patient. By tidyverse our data, we could create a new categorical variable ‘age group’. We cut the age into three groups: young, medium, and aged. 23 to 40 is young people; 41 to 57 is mid-age; and 58 to 74 is elder. Then, we could summarize our new data set. And use the str function to double check the type of variables. data(&quot;Arthritis&quot;) head(Arthritis,5) ## ID Treatment Sex Age Improved ## 1 57 Treated Male 27 Some ## 2 46 Treated Male 29 None ## 3 77 Treated Male 30 None ## 4 17 Treated Male 32 Marked ## 5 36 Treated Male 46 Marked arthritis&lt;-Arthritis%&gt;%mutate(agegroup=cut(Arthritis$Age, 3, labels=c(&#39;Young&#39;, &#39;Medium&#39;, &#39;Aged&#39;))) #cut by (22.9,40] (40,57] (57,74.1] table(arthritis$agegroup) ## ## Young Medium Aged ## 15 29 40 summary(arthritis) ## ID Treatment Sex Age Improved ## Min. : 1.00 Placebo:43 Female:59 Min. :23.00 None :42 ## 1st Qu.:21.75 Treated:41 Male :25 1st Qu.:46.00 Some :14 ## Median :42.50 Median :57.00 Marked:28 ## Mean :42.50 Mean :53.36 ## 3rd Qu.:63.25 3rd Qu.:63.00 ## Max. :84.00 Max. :74.00 ## agegroup ## Young :15 ## Medium:29 ## Aged :40 ## ## ## str(arthritis) ## &#39;data.frame&#39;: 84 obs. of 6 variables: ## $ ID : int 57 46 77 17 36 23 75 39 33 55 ... ## $ Treatment: Factor w/ 2 levels &quot;Placebo&quot;,&quot;Treated&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Age : int 27 29 30 32 46 58 59 59 63 63 ... ## $ Improved : Ord.factor w/ 3 levels &quot;None&quot;&lt;&quot;Some&quot;&lt;..: 2 1 1 3 3 3 1 3 1 1 ... ## $ agegroup : Factor w/ 3 levels &quot;Young&quot;,&quot;Medium&quot;,..: 1 1 1 1 2 3 3 3 3 3 ... head(arthritis,5) ## ID Treatment Sex Age Improved agegroup ## 1 57 Treated Male 27 Some Young ## 2 46 Treated Male 29 None Young ## 3 77 Treated Male 30 None Young ## 4 17 Treated Male 32 Marked Young ## 5 36 Treated Male 46 Marked Medium Barplot Bar graphs are used to show the relationship between numeric variables and categorical variables. This section also includes stacked bar charts and grouped bar charts, which show two levels of grouping. Here, we use bar charts directly to show the improved groups. ggplot(arthritis, mapping = aes(Improved))+ geom_bar(fill=&#39;red&#39;,alpha=0.3)+ xlab(&#39;Improves&#39;)+ labs(title = &quot;Bar plot of improved&quot;) Or, we could use pipe method and geom_col function to help us count the frequency of three different age groups. arthritis%&gt;%group_by(agegroup)%&gt;% mutate(count=n())%&gt;%summarize(count =sum(count))%&gt;% ggplot(aes(agegroup,count))+ geom_col(color =&quot;grey50&quot;, fill =&quot;lightblue&quot;)+ theme_grey(16)+ labs(title = &quot;Bar chart of three different agegroup&quot;) Stacked barplot and Group barplot The grouped bar plot displays a numeric value for a set of entities split into groups and subgroups. ggplot(arthritis, aes(fill=agegroup,y=Age,x=Improved)) + geom_bar(position=&quot;dodge&quot;, stat=&quot;identity&quot;) A stacked bar plot is very similar to the grouped bar plot above. The subgroups are just displayed on top of each other. By splitting the data, we could present the proportion of each age group in the improved bars. a&lt;-arthritis%&gt;%group_by(Improved)%&gt;% mutate(count=n())%&gt;%mutate(prop = count/sum(count)) ggplot(a, aes(fill=agegroup,y=prop,x=Improved)) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) We could also display the stacked bar by adding a third variable. ggplot(arthritis, mapping = aes(x=Improved, fill=Treatment))+ geom_bar(alpha=0.5)+ xlab(&#39;&#39;)+ coord_flip()+ labs(title = &quot;&quot;) Also, we could facet the data by variable improved and then display the difference between treatment and placebo group. ggplot(arthritis, mapping = aes(Treatment, fill=Improved))+ geom_bar(alpha=0.7)+ facet_wrap(~Improved)+ xlab(&#39;&#39;)+ labs(title = &quot;&quot;) The bar plot compare different groups in bars with proportion or frequency. However, the bar plot, especially the stacked bar plot, does not show the difference in total amount of different groups. Hence, we could use the mosaic plot to fix this issue. Mosaic plot The mosaic plot allows us to examine the relationship between two or more categorical variables. We always want to split the dependent variable horizontally. And also, it important to add one variable each time. colors = c(&quot;#E5F9E0&quot;, &quot;#A3F7B5&quot;, &quot;#40C9A2&quot;, &quot;#2F9C95&quot;) mosaic(Improved~Treatment,direction=c(&#39;v&#39;,&#39;h&#39;),highlighting_fill =colors[1:3],arthritis) Improved~agegroup+Sex, we always treat Improved as dependent variable. mosaic(Improved~agegroup+Sex, direction=c(&quot;v&quot;,&#39;v&#39;,&#39;h&#39;),highlighting_fill =colors[1:3],arthritis) Treatment~agegroup+Improve+sex mosaic(Improved~Treatment+agegroup+Sex, direction=c(&quot;v&quot;,&#39;v&#39;,&#39;v&#39;,&#39;h&#39;),highlighting_fill =colors[1:3],arthritis) Heatmap A heat map is a graphical representation of data, where each value contained in the matrix is represented by color. The first heatmap, we have two categorical data with filled in numerical variable age. ggplot(arthritis, aes(Sex, Improved, fill= Age)) + geom_tile(color=&quot;white&quot;)+ coord_fixed() All three are categorical variables. ggplot(arthritis, aes(Treatment, agegroup, fill= Improved)) + geom_raster()+ coord_fixed()+ theme_classic()+ theme(axis.line=element_blank(),axis.ticks=element_blank()) "],["china-choropleth-map.html", "Chapter 19 China choropleth map 19.1 Overview 19.2 Data Collection 19.3 Static Map with ggplot 19.4 Interactive Map with hchinamap 19.5 Interactive Map with leafletCN 19.6 Conclusion", " Chapter 19 China choropleth map Jialu Xia and Danyang Han library(jsonlite) library(tidyverse) library(hash) library(ggplot2) library(maptools) library(tidyr) library(rgdal) library(here) library(hchinamap) library(magrittr) library(data.table) library(mapproj) library(shadowtext) library(leafletCN) 19.1 Overview From a choropleth map of Covid-19 spread, we could clearly see the number of confirmed cases and severity in each area on the map. We would like to introduce some techniques to plot choropleth map with R. There are different types of maps, like the world map, maps of Continents and maps of Countries etc,. Different maps may require different packages to plot. Here we typically explore maps of China. As a running example, we collected China’s Covid-19 data and start from ggplot, a tool we learned in the lectures. In addition to this, we practiced hchinamap and leafletCN to plot interactive maps. 19.2 Data Collection We web-scarped China’s Covid-19 data. The data set includes daily updates of confirmed cases and death for each city in China. url = &#39;https://view.inews.qq.com/g2/getOnsInfo?name=disease_h5&amp;callback=1580373566110&#39; x = readLines(url, encoding=&quot;UTF-8&quot;) x = sub(&quot;^\\\\d+&quot;, &quot;&quot;, x) x = sub(&quot;^\\\\(&quot;, &quot;&quot;, x) x = sub(&quot;\\\\)$&quot;, &quot;&quot;, x) y = fromJSON(x) d = fromJSON(y$data) h = d$areaTree$children[[1]] names_dic = hash(c(&quot;香港&quot;, &quot;上海&quot;, &quot;新疆&quot;, &quot;台湾&quot;, &quot;四川&quot;, &quot;广东&quot;, &quot;陕西&quot;, &quot;福建&quot;, &quot;内蒙古&quot;, &quot;天津&quot;, &quot;河北&quot;, &quot;江苏&quot;, &quot;山东&quot;, &quot;浙江&quot;, &quot;辽宁&quot;, &quot;山西&quot;, &quot;河南&quot;, &quot;重庆&quot;, &quot;云南&quot;, &quot;北京&quot;, &quot;黑龙江&quot;, &quot;湖南&quot;, &quot;广西&quot;, &quot;宁夏&quot;, &quot;吉林&quot;, &quot;西藏&quot;, &quot;海南&quot;, &quot;澳门&quot;, &quot;江西&quot;, &quot;青海&quot;, &quot;湖北&quot;, &quot;甘肃&quot;, &quot;安徽&quot;, &quot;贵州&quot; ), c(&quot;Hongkong&quot;, &quot;Shangahi&quot;, &quot;Xinjiang&quot;, &quot;Taiwan&quot;, &quot;Sichuan&quot;, &quot;Guangdong&quot;,&quot;Shannxi&quot;, &quot;Fujian&quot;, &quot;Inner Mongolia&quot;, &quot;Tianjing&quot;, &quot;Hebei&quot;,&quot;Jiangsu&quot;, &quot;Shandong&quot;, &quot;Zhejiang&quot;, &quot;Liaoning&quot;,&quot;Shanxi&quot;, &quot;Henan&quot;, &quot;Chongqin&quot;, &quot;Yunnan&quot;, &quot;Beijing&quot;,&quot;Heilongjiang&quot;, &quot;Hunan&quot;, &quot;Guangxi&quot;,&quot;Ningxia&quot;,&quot;Jilin&quot;,&quot;Tibet&quot;,&quot;Hainan&quot;, &quot;Macao&quot;, &quot;Jiangxi&quot;,&quot;Qinghai&quot;, &quot;Hubei&quot;, &quot;Gansu&quot;, &quot;Anhui&quot; ,&quot;Guizhou&quot; ) ) name_en = values(names_dic, keys = h$name, USE.NAMES = FALSE) data = data.frame(h$name, name_en, h$total$confirm) #rename column name data&lt;-data %&gt;% rename( province_CH=h.name, province_EN=name_en, total_confirm=h.total.confirm ) data$province_CH=as.factor(data$province_CH) data$province_EN=as.factor(data$province_EN) #write.csv(file=&#39;Heatmap_data.csv&#39;,data, fileEncoding = &#39;UTF-8&#39;) head(data) ## province_CH province_EN total_confirm ## 1 香港 Hongkong 5781 ## 2 上海 Shangahi 1312 ## 3 台湾 Taiwan 618 ## 4 四川 Sichuan 800 ## 5 福建 Fujian 478 ## 6 陕西 Shannxi 495 The dataset we use here is a 34x3 data frame: province_CH: province name in Chinese province_EN: province name in English total_confirm: total confirmed cases We are gonna plot total confirmed cases. 19.3 Static Map with ggplot Since ggplot does not have build-in China map, we download shape files from the “Capital of Statistics” webiste and prepare the shape data for ggplot. 19.3.1 Prepare the shape data of China dsn&lt;-&quot;resources/china_choropleth_map/china-province-border-data/bou2_4p.shp&quot; layer&lt;-&quot;bou2_4p&quot; china_map &lt;- rgdal::readOGR(dsn=dsn, layer=layer) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;/Users/joycerobbins/cc20/resources/china_choropleth_map/china-province-border-data/bou2_4p.shp&quot;, layer: &quot;bou2_4p&quot; ## with 925 features ## It has 7 fields ## Integer64 fields read as strings: BOU2_4M_ BOU2_4M_ID ###Note: we attached external spetial data of china, named &quot;china-province-border-data&quot; and it includes &quot;bou2_4p.shp&quot; ### dsn is the path of &quot;bou2_4p.shp&quot; # extract province information from shap file china_map_data = data.table::setDT(china_map@data) data.table::setnames(china_map_data, &quot;NAME&quot;, &quot;province&quot;) # transform to UTF-8 coding format china_map_data[, province:=iconv(province, from = &quot;GBK&quot;, to = &quot;UTF-8&quot;)] # create id to join province back to lat and long, id = 0 ~ 924 china_map_data[, id:= .I-1] # id = 0, 1, 2, ... , used to match to `dt_china` # there are more shapes for one province due to small islands, extract the provinces that are consistent with our data. china_map_data[, province:= as.factor(province)] china_map_data &lt;- china_map_data[!is.na(province)] china_map_data &lt;- china_map_data[AREA &gt; 0.1] head(china_map_data, 3) ## AREA PERIMETER BOU2_4M_ BOU2_4M_ID ADCODE93 ADCODE99 province id ## 1: 54.447 68.489 2 23 230000 230000 黑龙江省 0 ## 2: 129.113 129.933 3 15 150000 150000 内蒙古自治区 1 ## 3: 175.591 84.905 4 65 650000 650000 新疆维吾尔自治区 2 dt_china = setDT(fortify(china_map)) head(dt_china, 3) ## long lat order hole piece id group ## 1: 121.4884 53.33265 1 FALSE 1 0 0.1 ## 2: 121.4995 53.33601 2 FALSE 1 0 0.1 ## 3: 121.5184 53.33919 3 FALSE 1 0 0.1 dt_china[, id:= as.numeric(id)] setkey(china_map_data, id) setkey(dt_china, id) dt_china &lt;- china_map_data[dt_china] ##adjust province names in the order of levels(china_map_data$province) so that they are compatible with plotting function re_level &lt;- function(levels){ re_level = c() for (i in 1:33) { pro = levels[i] if (grepl(&quot;黑&quot;, pro)) pro = substr(pro, start = 1, stop = 3) else if (grepl(&quot;内&quot;, pro)) pro = substr(pro, start = 1, stop = 3) else pro = substr(pro, start = 1, stop = 2) re_level[i] = pro } re_level &lt;- as.character(re_level) return(re_level) } get.centroids &lt;- function( data = dt1, long = &quot;long&quot;, lat = &quot;lat&quot;, by_var = &quot;state&quot;, # the grouping variable, e.g. state: get centroid by state fill_var = NULL # the variable to plot ){ data &lt;- data[!is.na(data[[by_var]]),] data[[by_var]] &lt;- as.character(data[[by_var]]) # sometimes there is empty factor level dt1_df &lt;- sp::SpatialPointsDataFrame(coords = data[, c(long, lat), with = FALSE], data = data) dt1_geo &lt;- by(dt1_df, dt1_df[[by_var]], function(x) {sp::Polygon(x[c(long, lat)])@labpt}) centroids &lt;- stats::setNames(do.call(&quot;rbind.data.frame&quot;, dt1_geo), c(long, lat)) centroids$name &lt;- names(dt1_geo) if(!is.null(fill_var)){ # if need to join fill value setkeyv(setDT(centroids), &quot;name&quot;) dt_var &lt;- unique(data[,c(by_var, fill_var), with = FALSE]) setkeyv(dt_var, by_var) centroids &lt;- dt_var[centroids] } return(centroids) } # combine our covid data with the shape data input_data&lt;-data %&gt;% filter(province_EN!=&quot;Macao&quot;) input_data&lt;-data.table(input_data) levels(dt_china$province) &lt;- re_level(levels(china_map_data$province)) # relevel dt_china so that it matches the level of shape file. setkey(input_data, province_CH) setkey(dt_china, province) dt_china &lt;- input_data[dt_china, nomatch = 0] centroids_cn &lt;- get.centroids(data = dt_china, by_var = &quot;province_CH&quot;) centroids_en &lt;- get.centroids(data = dt_china, by_var = &quot;province_EN&quot;) 19.3.2 Plot: gg_en &lt;- ggplot(dt_china, aes(x = long, y = lat, group = group, fill = total_confirm)) + labs(fill = &quot;Number of Confirm&quot;)+ geom_polygon()+ scale_fill_gradientn(colours = RColorBrewer::brewer.pal(8, &quot;GnBu&quot;), na.value = &quot;grey90&quot;, guide = guide_colourbar(barwidth = 25, barheight = 0.4, #put legend title on top of legend title.position = &quot;top&quot;)) + labs(fill = &quot;Number of Confirm&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + coord_map() + # map scale theme_void() + theme(legend.position = &quot;bottom&quot;, legend.title=element_text(size=12), # font size of the legend legend.text=element_text(size=10)) # add province name to the map gg_en+geom_text(data = centroids_en, aes(x = long, y = lat, label = name), inherit.aes = FALSE) This method is tedious as we need to deal with the shape data, and we could not see number of confirmed cases directly from the plot. So we introduce other two packages which are simpler and more interactive in the following parts. 19.4 Interactive Map with hchinamap The package hchinamap that allows interactive map plots and contains map with complete Chinese territory. hchinamap(name=data$province_CH, value=data$total_confirm, width=&quot;100%&quot;, height=&quot;400px&quot;, title=&quot;Covid map of China&quot;, region=&quot;China&quot;, minColor = &quot;#f1eef6&quot;, maxColor = &quot;#980043&quot;, itermName = &quot;Total confirmed&quot;, hoverColor = &quot;#f6acf5&quot;, ) Here, the map shows total confirmed cases in each province in China. One of the shortage of this package is that we can only use color to represent numeric values (number of confirmed cases in this plot). So, in this case, it is hard to tell from the color the relative confirmed cases in each province because the vast majority of the cases are concentrated in Hubei province, and other provinces have only relatively minimal confirmed cases. We can see that the map showing dark color in only Hubei province and merely white in all other provinces. An advantage is that we can also plot maps for provinces using this package. Take Hubei province for example, from the web-scraped data set, we created a data frame that contains total confirmed cases in each city in Hubei province. idx = match(&quot;湖北&quot;, h$name) hubei = h$children hubeidata&lt;-hubei[[idx]] #write.csv(file=&#39;Hubei_data.csv&#39;,hubeidata, fileEncoding = &#39;UTF-8&#39;) hchinamap(name = hubeidata$name, value = hubeidata$total$confirm, width = &quot;100%&quot;, height = &quot;400px&quot;, title = &quot;Covid Map of Hubei&quot;, region = &quot;Hubei&quot;, minColor = &quot;#f1eef6&quot;, maxColor = &quot;#980043&quot;, itermName = &quot;Total confirmed&quot;, hoverColor = &quot;#f6acf5&quot;) 19.5 Interactive Map with leafletCN The package leafletCN provides four color methods: numeric, bin, quantile and factor. Coloring by quantile allows us to see how many cases each province have relative to other provinces, which solved the problem we have with hchinamap. And another good thing is that we could customize the content displayed on the area when clicked, so we could display the exact number of confirmed cases! geojsonMap(dat = data, mapName = &quot;china&quot;, namevar = ~ province_CH, valuevar = ~ total_confirm, popup = paste0(data$province_CH,data$province_EN,&quot;:&quot;,data$total_confirm), palette = &quot;Reds&quot;, legendTitle = &quot;Quantile of confirmed number&quot;, colorMethod=&quot;quantile&quot;) 19.6 Conclusion Other data could also be plot on the maps with the packages we introduced above, like population and temperature. Based on the feature of different datasets, you can try all packages and chose the most suitable one! Reference: https://liuyanguu.github.io/post/2020/06/12/ggplot-us-state-and-china-province-heatmap/ "],["benfords-law-analysis-of-2020-election.html", "Chapter 20 Benford’s Law Analysis of 2020 Election", " Chapter 20 Benford’s Law Analysis of 2020 Election Ryan McNally For my community contribution I thought I might consider my civic community in addition to my academic EDAV community :) I have seen some claims of election fraud online in recent days following the election, so thought I might take a look for myself and publish the findings. I have gathered vote count data from a few sources to assess whether fraud is statistically likely to have taken place, through use of Benford’s Law, which claims that sufficiently random counts spanning orders of magnitude will have FIRST DIGITS that occur with decreasing frequency from 1-9. My initial findings are that for the 3 major candidates, Trump , Biden, and Jorgensen - that at the national level there is no evidence of fraud. I also ran the analysis on county (or electoral district) level vote counts by state. This works well for states with a high number of counties, but is noisy and unreliable for those with only a few counties. So the last stage I am working on is to gather precinct level data for specific states that were closely contested, as they’d have had the highest incentive for fraud, and since there is always a large number of reporting precincts. However, this data is much more difficult to find and scrape effectively, so at this point I only have a study of Michigan complete. There are a few counties in Michigan that have a high number of reporting precincts, and don’t conform to the law as closely as one might expect… take a look! In the resources folder is a picture of the national level aggregation, showing very smooth functions and conformance with Benford’s law. Contrast this with Kent County Michigan file (this is not meant to be partisan, there were plenty of counties which Trump appeared to have failed the chi-square test as well). In the Piazza post about this project, a peer posted the following comment/question: “This is neat! Do you have data for other states? Or for past elections? My understanding is that Benford’s Law only applies to data that meet certain criteria. For instance, the pattern emerges more often in data that spans multiple orders of magnitude. That would explain why the national data fits the law almost perfectly. The pattern is also conditional on the mean of the data and the outcome of the election (in each sample). In a county that Biden won, the pattern is more likely to emerge in Trump’s data, and the expected most frequent digit for Biden depends more on the mean than Trump’s (you can see this in how the distribution skews for Biden in Kent County).” My further analysis to address this is as follows: Thanks for taking a look! Yes as I read more about Benford’s Law I am learning about that constraint. I need to delve a little deeper to see that vote counts in each county’s precincts span a few orders of magnitude to be robust. Here are order of magnitude frequencies for all Michigan precincts, and then for Kent County respectively. Michigan total: Kent County total: In Michigan as a whole, Trump appears evenly/randomly distributed across several orders of magnitude, while Biden looks clustered in the hundreds, and Jorgensen only spans 2 orders of magnitude for the whole state. This might disqualify them, I am not sure. In Kent county, we are likely constrained by the precinct sizes, and see clustering at 3 OOM for Trump and Biden and 2 for Jorgensen. Further reading has indicated that using a combination of the first 2 digits together might be more illuminative, as well as using the last 2 digits (which ought to be totally randomly distributed). I don’t currently have any more precinct level data for any more states, but would be interested in checking out other tight contests, perhaps Wisconsin, Pennsylvania, and Georgia. Thanks again for taking a look and following up! ***The raw code and output can be found in the following html file: https://github.com/mcryan6/benfords_law_2020_election/blob/main/benfords_law_2020election.ipynb "],["visualizing-electoral-margins-in-us-presidential-elections-2008-2016.html", "Chapter 21 Visualizing electoral margins in US presidential elections (2008-2016)", " Chapter 21 Visualizing electoral margins in US presidential elections (2008-2016) Jiyeon Chang and Andus Kong #install.packages(&quot;mapproj&quot;) #install.packages(&quot;maps&quot;) library(dplyr) library(tidyverse) library(ggplot2) library(maps) library(mapproj) library(cowplot) library(readxl) library(magrittr) library(GGally) library(parcoords) Against the backdrop of the upcoming election, the media has given a lot of attention to races that are expected to be particularly competitive. Given the large share of international students in the class, we thought it would be interesting to provide some background as to what the electoral map for the presidential election looks like, and the level of margins observed in past elections. The data for the analysis is taken from the MIT Election Lab (https://electionlab.mit.edu/). The data on electoral outcome provide information on the winning candidate/party, as well as the votes received. A separate dataset is available for the turnout. For the purpose of this analysis we look at data from 2000 to 2016, with a focus on the past 3 elections. # create a dataset with electoral margins turnout&lt;-turnout %&gt;% select(c(&quot;year&quot;,&quot;state&quot;,&quot;vep_highest_office&quot;)) %&gt;% filter(state!=&quot;United States&quot;) %&gt;% mutate(state = tolower(state)) %&gt;% rename(turnout = vep_highest_office) elect_data&lt;-elect_pres %&gt;% select(c(&quot;year&quot;,&quot;state&quot;,&quot;state_po&quot;,&quot;candidate&quot;,&quot;party&quot;,&quot;totalvotes&quot;,&quot;candidatevotes&quot;)) pres_top2&lt;- elect_data %&gt;% group_by(state,year) %&gt;% arrange(desc(candidatevotes),.by_group = TRUE) %&gt;% filter(row_number() %in% c(1,2)) %&gt;% mutate(share = candidatevotes/totalvotes) %&gt;% mutate(margin = share-dplyr::lead(share)) pres_winner&lt;- pres_top2 %&gt;% group_by(state,year) %&gt;% filter(row_number() %in% c(1)) %&gt;% mutate(margin_cont = ifelse(party==&quot;republican&quot;,margin,-(margin))) pres_winner[pres_winner$party==&quot;democratic-farmer-labor&quot;,]$party&lt;-c(&quot;democrat&quot;,&quot;democrat&quot;,&quot;democrat&quot;) pres_winner$party&lt;-as.factor(pres_winner$party) pres_winner$state &lt;- tolower(pres_winner$state) margins&lt;-merge(pres_winner, turnout, by=c(&quot;state&quot;,&quot;year&quot;)) To plot state-level data on a map in R, we use the maps package. This package has a dataset with information on longitude and latitude which can be merged by state id to transform the dataframe into a mappable version. The snippet from a data frame shown below shows how the information on longitude and latitude is encoded; for Alabama, for instance, we see several rows with slight variations in the longitude and latitude, covering the area corresponding to the state. # merge the electoral data with data on the latitude and longitude of each US state. us_states &lt;- map_data(&quot;state&quot;) head(us_states) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; ## 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; ## 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; ## 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; names(us_states)[names(us_states) == &quot;region&quot;] &lt;-&quot;state&quot; df.margin &lt;- merge(margins, us_states, sort = FALSE, by = &quot;state&quot;) As a starter, let’s look at how states voted in the 2016 elections. col_party &lt;- c(&quot;blue&quot;, &quot;red&quot;) p_all &lt;- ggplot(data = df.margin[df.margin$year %in% c(2008,2012,2016),], aes(x = long, y = lat, group = state, fill = party)) p_all_map&lt;-p_all + geom_polygon(color = &quot;gray90&quot;, size = 0.1) + coord_map(projection = &quot;albers&quot;, lat0 = 39, lat1 = 45) + scale_fill_manual(values=col_party)+ labs(title=&quot;Figure 1. electoral outcome&quot;) + theme_map() + theme(plot.title = element_text(size = 16, face = &quot;bold&quot;), legend.title = element_text(size = 8), legend.text = element_text(size = 6), # panel.spacing.y=unit(0.1, &quot;cm&quot;), # panel.spacing.x=unit(0.1, &quot;cm&quot;) )+ facet_wrap(~year) p_all_map The above map gives us an idea of which party won the state, but not by what margin. To do this, we need to modify the fill parameter from party to margin. p_margin &lt;- ggplot(data = df.margin[df.margin$year %in% c(2008,2012,2016),], aes(x = long, y = lat, group = state, fill = margin)) # here fill = margin margin_map&lt;-p_margin + geom_polygon(color = &quot;gray90&quot;, size = 0.1) + coord_map(projection = &quot;albers&quot;, lat0 = 39, lat1 = 45) + labs(title=&quot;Figure 2. overall margin by state&quot;,fill=&quot;percent&quot;) + scale_fill_gradient(low=&quot;purple&quot;,high=&quot;white&quot;)+ theme_map()+ theme(plot.title = element_text(size = 16, face = &quot;bold&quot;), legend.title = element_text(size = 8), legend.text = element_text(size = 6))+ facet_wrap(~year) margin_map min(df.margin$margin) ## [1] 9.005368e-05 max(df.margin$margin) ## [1] 0.864135 The map above gives us a better of idea of how competitive the presidential race was in each state. The darker the shade of purple, the smaller was the margin. This made shows the magnitude of the margin but not which party won the state. So next we break down the margin by the winning party. Specifically, the margin is redefined as ranging from -1 to 1, with negative values referring to Democratic lead, and positive values Republican. margin_bin &lt;- ggplot(data = df.margin[df.margin$year %in% c(2008,2012,2016),], aes(x = long, y = lat, group = state, fill = margin_cont)) margin_bin_map&lt;-margin_bin + geom_polygon(color = &quot;gray90&quot;, size = 0.1) + coord_map(projection = &quot;albers&quot;, lat0 = 39, lat1 = 45) + labs(title=&quot;Figure 3. state level margin by winning party&quot;,fill=&quot;percent&quot;) + scale_fill_distiller(palette = &quot;RdBu&quot;,direction = -1, limits = c(-1, 1))+ theme_map()+ theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), legend.title = element_text(size = 8), legend.text = element_text(size = 6))+ facet_wrap(~year) margin_bin_map Now, the diverging color scheme blends the strengths of Figures 1 and 2, making it possible not only to see which parties won the election in each state, but also by what margin. Note, however, that even with this distinction, the range of margins remains quite large, with the smallest ranging from less than 1 percentage point, to the largest where the Democrats won by a 86 percentage point margin. (Note that the 86 percentage pt correspondents to D.C. but we don’t observe a dark blue state because it is too small to be visible on the map) As a result, we can glean from the color scheme the relative competitiveness of each state, but it’s be tough to distinguish between a margin of say, 10% vs 2%. So in an alternative approach, we define the threshold for “a small margin” to be 2%, and highlight only the states that had margins at this level or below. df.margin&lt;-df.margin %&gt;% mutate(low_margin = if_else(margin &lt; 0.02,&quot;Low&quot;, &quot;High&quot;)) p_LH &lt;- ggplot(data = df.margin[df.margin$year %in% c(2008,2012,2016),], aes(x = long, y = lat, group = state, fill = low_margin)) temp &lt;- df.margin %&gt;% filter(year %in% c(2008, 2012, 2016)) %&gt;% filter(low_margin == &quot;Low&quot;) %&gt;% select(state, year, state_po, low_margin) %&gt;% distinct temp2 &lt;- df.margin %&gt;% filter(year %in% c(2008, 2012, 2016)) %&gt;% filter(low_margin == &quot;Low&quot;) %&gt;% select(-long, -lat, -group, -order, -subregion) %&gt;% distinct temp &lt;- temp %&gt;% mutate(long = if_else(state == &quot;florida&quot;, -81.760254, if_else(state == &quot;indiana&quot;,-86.126976, if_else(state == &quot;michigan&quot;, -84.506836, if_else(state == &quot;minnesota&quot;, -94.636230, if_else(state == &quot;missouri&quot;, -92.603760, if_else(state == &quot;new hampshire&quot;,-71.500000, if_else(state == &quot;north carolina&quot;, -80.793457, if_else(state == &quot;pennsylvania&quot;, -77.194527, if_else(state == &quot;wisconsin&quot;, -89.500000, 0))))))))), lat = if_else(state == &quot;florida&quot;, 27.994402, if_else(state == &quot;indiana&quot;,40.273502 , if_else(state == &quot;michigan&quot;, 44.182205 , if_else(state == &quot;minnesota&quot;, 46.392410, if_else(state == &quot;missouri&quot;, 38.573936, if_else(state == &quot;new hampshire&quot;,44.000000, if_else(state == &quot;north carolina&quot;, 35.782169, if_else(state == &quot;pennsylvania&quot;, 41.203323 , if_else(state == &quot;wisconsin&quot;, 44.500000, 0)))))))))) temp3 &lt;- merge(temp, temp2, by = c(&quot;state&quot;, &quot;year&quot;, &quot;state_po&quot;)) %&gt;% rename(low_margin = low_margin.x) LH&lt;-p_LH + geom_polygon(aes(group=group),color = &quot;gray90&quot;, size = 0.1) + coord_map(projection = &quot;albers&quot;, lat0 = 39, lat1 = 45) + labs(title=&quot;Figure 4. low margin states (&lt;2%)&quot;,fill=&quot;margin&quot;) + scale_fill_manual(values=c(&quot;grey&quot;, &quot;yellow&quot;))+ theme_map()+ theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), legend.title = element_text(size = 8), legend.text = element_text(size = 6))+ geom_text(data= temp3,aes(long,lat,label=state_po),size=2)+ facet_wrap(~year) LH Based on this map, we see that different states had competitive races in each election. 2016 stands out for having had more states with a small margin, which are identified as Michigan, Wisconsin, Florida, Pennsylvania, Minnesota and New Hampshire. Many of these states that had close races in 2016 are leaning more Democratic in the 2020 election, but a number of other states, namely, Ohio, Iowa, North Carolina and Arizona, have become competitive in their stead. One aspect of the election that is talked about a lot at the moment is the increase in the cumulative turnout ahead of the election day. Understandably, a lot is at stake in this election; but is there a relationship between how close the race is, and the turnout? What does previous years’ data tell us? df_scatter&lt;-df.margin %&gt;% filter(year %in% c(2008,2012,2016))%&gt;% group_by(state,year) %&gt;% filter(state!=&quot;district of columbia&quot;)%&gt;% filter(row_number() %in% c(1)) p_scatter&lt;-ggplot(df_scatter, aes(turnout, margin)) + geom_point() + facet_wrap(~year) p_scatter Based on a scatterplot of margin against turnout, it seems that there is a pattern of smaller margin being associated with higher turnout. Note that in the charts displayed below, DC was removed as an outlier as the combination of its high margin and high turnout visually crowded out all other observations into a corner. The pattern is quite clear in 2016 and 2012, but slightly less so in 2008. Lastly, we produce a parallel coordinate plot, which makes it possible to play with range of margins in each election cycle and see whether states have had more consistent margins, or have changed over time. Feel free to play around with the parameters! temp &lt;- tibble(df.margin) %&gt;% select(state, year, margin) %&gt;% distinct temp &lt;- temp %&gt;% pivot_wider(names_from = year, values_from = margin) %&gt;% relocate(&quot;state&quot;, &quot;2000&quot;, &quot;2004&quot;, &quot;2008&quot;, &quot;2012&quot;, &quot;2016&quot;) parcoords(temp, brushMode = &quot;1d-axes&quot;, reorderable = TRUE, rownames = FALSE) "],["mosaic-plot-cheatsheet.html", "Chapter 22 Mosaic plot cheatsheet", " Chapter 22 Mosaic plot cheatsheet Tianqi Lou and Liyuan Tang Group number: 15 The community contribution for our team is to create a cheatsheet of mosaic plot with vcd package in r. Our work is divided into two parts. The first part is the basic introduction for mosaic function in vcd package and we show some example outputs which is in the first and second columns. The second part is some further applications including the association plot and the independence model which is in the third column. Our cheatsheet and coding are uploaded on the Github as the follow link: https://github.com/Terrrry98/Community-Contribution The cheatsheet is in the pdf file and the corresponding coding is in the rmd file. "],["time-series-cheatsheet.html", "Chapter 23 Time Series Cheatsheet", " Chapter 23 Time Series Cheatsheet Lingxuan Gu This project includes a pdf version cheatsheet for time series analysis. It includes functions for plotting, modelling, predicting, etc. Click the following link to check out the cheatsheet: https://github.com/gulingxuan98/Stat5702/blob/main/cc-cheatsheet.pdf "],["python-vs-r-cheatsheet.html", "Chapter 24 Python vs. R Cheatsheet", " Chapter 24 Python vs. R Cheatsheet Dingwen Xie and Yu Liu We create a cheatsheet “Python vs R” that lists difference between two languages on common functions. Here is the link of our cheatsheet as a pdf file: https://github.com/yl3738/Python-vs.-R-Cheatsheet/blob/main/community%20contribution_CC%20group14.pdf "],["likert-data-cheat-sheet.html", "Chapter 25 likert data cheat sheet 25.1 link is below", " Chapter 25 likert data cheat sheet Zhibin Li 25.1 link is below https://github.com/lzb1003/statw5702_cc/blob/main/likert_data_cheat_sheet.pdf "],["categorical-data-visualization-cheat-sheet.html", "Chapter 26 Categorical Data Visualization Cheat Sheet", " Chapter 26 Categorical Data Visualization Cheat Sheet Miranda Zhou and Neha Pusarla Cheat Sheet showing different categorical data visualization tools. Has some examples, benefits and detriments, and tips for each one. link to pdf: https://github.com/jtr13/cc20/blob/master/resources/categorical_cheat_sheet/categorical_cheat_sheet.pdf "],["tableau-intro-tutorial-explained-with-proset2.html", "Chapter 27 Tableau intro tutorial explained with proset2", " Chapter 27 Tableau intro tutorial explained with proset2 Yue Wang For the community contribution project, I will briefly introduce Tableau, the major features of this product, and other relevant products that are used in a different setting. I will introduce structure of the interfaces, importation of data, the supported files of data, supported data types, how to build worksheets and an interactive dashboard. I’ll use the Problem1 from Problem Set 2 to do the demo. This problem includes recoding of factor levels, vertical/horizontal bar chart, and horizontal bar chart of proportion. Although it only covers bar chart, I believe this will be a great first step to learn about how Tableau generally works. After doing the demo, I’ll draw a comparison with R and Excel and reemphasize the context of using Tableau. You can check the video through this link: https://www.bilibili.com/video/BV1py4y167YF/ "],["solutioning-a-data-pipeline-for-visualization.html", "Chapter 28 Solutioning a Data pipeline for visualization", " Chapter 28 Solutioning a Data pipeline for visualization Akanxa Padhi As a part of my community contribution, I present a lightning talk on some of the key considerations to keep in mind while designing a data pipeline for an organization. I use a scenario from my workplace to share my experience. Please refer to my slides here - https://github.com/akp21/EDAV_CC20/blob/main/Datapipeline_solution.pdf "],["visualization-research-in-biomedical-informatics.html", "Chapter 29 Visualization Research in Biomedical Informatics 29.1 Community Contribution Zoom Session", " Chapter 29 Visualization Research in Biomedical Informatics 29.1 Community Contribution Zoom Session Adrienne Pichon (ab3886@cumc.columbia.edu) is a 2nd year PhD student in the Department of Biomedical Informatics at Columbia and previously received her MPH in Sociomedical Sciences and Sexuality, Sexual, and Reproductive Health from Columbia’s Mailman School of Public Health. Her work focuses on person-generated data and supporting the collaborative work of patients and providers when caring for chronic disease. This zoom session connected visualization principles learned in class to a real-world application. The hour-long session was held on Friday (11/20 2pm ET) via Zoom 1. Presentation of recently published paper, eliciting design needs of end-users of an interactive visualization tool (5 min video available) 2. Overview of the Phendo app and aggregate data 3. Discussion of initial tool prototype, for viewing and understanding individual-level data to support care of an enigmatic chronic condition 4. Resources for human-centered computing that were discussed in the zoom session are also listed below The slides are available here 29.1.1 Designing an Interactive Visualization Tool for Understanding &amp; Caring for a Complex Chronic Condition Download a copy of the paper, or see current publications: website or google scholar. There is also a five minute video available, that was presented at the CSCW conference. In this qualitative research, focus groups with women with endometriosis (a painful chronic illness) and interviewers medical providers who treat the condition provided useful insights on the work of patients and providers in caring for this enigmatic condition. Findings from thematic analysis enabled identification of 3 design opportunities. The talk highlighted the potential applications of visualization techniques and innovation that we plan to incorporate into the design of this tool. 29.1.2 Phendo App &amp; Prototype of Interactive Visualization A high-level view of the phendo app and data for users was presented. Finally, we reviewed an early prototype of the interactive visualization tool. This early version uses D3 to display a heatmap of self-tracked symptom data in a unified timeline. Discussing and brainstorming this early prototype together with peers made it possible to integrate principles from the EDAV class to this real-world application. 29.1.3 Resources for Human-Centered Visualization &amp; Computing Data Feminism D’Ignazio, C., &amp; Klein, L. F. (2020). Data Feminism. MIT Press. open access, free online; also webinar recordings and sketchnotes from sessions with the authors: [http://datafeminism.io/] Race After Technology Benjamin, R. (2019). Race After Technology: Abolitionist Tools for the New Jim Code. John Wiley &amp; Sons. - [https://www.ruhabenjamin.com/race-after-technology] Design Justice Costanza-Chock, S. (2018). Design Justice: Towards an Intersectional Feminist Framework for Design Theory and Practice (SSRN Scholarly Paper ID 3189696). Social Science Research Network. - [https://designjustice.org/] Data Comics Bach, B., Wang, Z., Farinella, M., Murray-Rust, D., &amp; Henry Riche, N. (2018). Design Patterns for Data Comics. Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 38:1–38:12. [https://doi.org/10.1145/3173574.3173612] Bach, B., Riche, N. H., Carpendale, S., &amp; Pfister, H. (2017). The Emerging Genre of Data Comics. IEEE Computer Graphics and Applications, 37(3), 6–13. [https://doi.org/10.1109/MCG.2017.33] "],["american-history-trivia-night.html", "Chapter 30 American History Trivia Night", " Chapter 30 American History Trivia Night Jingwen Du For my community contribution project, I hosted a trivia night on October 30, 2020. It was both a chance to get to know fellow classmates better in a social setting as well as a learning opportunity for international students to make sense of the election process. I created a presentation with a brief overview of American history and political system. It also included 40+ questions of ranging difficulty levels. Each question was color coded by difficulty level with green being easy worth 1 point and red being difficult worth 3 points. Each person kept their own scores in a honor system. Couple students joined the event and had a great time. I’ve included my presentation with the trivia questions in this link https://github.com/jingwendu/CC2020_Resources/blob/main/Trivia%20Night.pdf "],["consuming-terrorism-statistics.html", "Chapter 31 Consuming terrorism statistics", " Chapter 31 Consuming terrorism statistics Michelle Strayer For the community contribution project, I presented a lecture on how to critically consume statistics on terrorism. This lecture was broken down into several parts, including a discussion of why critical consumption is necessary, how media presentation of terrorism statistics shapes (and distorts) our understanding of current events, barriers in studying terrorism, and suggestions on responsibly consuming terrorism statistics from the lenses of political science, data science, statistics, and mass media. "],["introduction-to-network-analysis.html", "Chapter 32 Introduction to network analysis", " Chapter 32 Introduction to network analysis Guotian Zhu, Panjia Yi For our community contribution project, we briefly introduced basics of network analysis, talked about basic elements that a network graph is consisted of and descriptives of network(global descriptives and local descriptives). You can check out the presentation vide at: https://youtu.be/WfYNMubdM6k The presentation pdf could be find at: https://drive.google.com/file/d/1-zGnmkA46P2O4_CnlNDFLRiW8NToovA6/view?usp=sharing "],["some-techniques-for-label-setting.html", "Chapter 33 Some techniques for label setting 33.1 1. Label placement 33.2 2. Auto wrapping of long labels on axis 33.3 3. Add shadow effect to labels", " Chapter 33 Some techniques for label setting Haonan Wen library(gridExtra) library(ggplot2) library(ggrepel) library(&quot;directlabels&quot;) library(AER) library(dplyr) library(forcats) library(stringr) library(&quot;shadowtext&quot;) 33.1 1. Label placement When adding labels to plots, sometimes we may find it tricky to adjust position of labels to cope with problems like text overlapping and text expanding over the boundary of grid. The hints and tips below worked for me and I believe they should work for most others in similar situations. 33.1.1 Tip 1: Adjusting parameters in geom_text like hjust, vjust, nudge_x and nudge_y Let’s use mpg data set as an example. Sample 10 car models from the data frame and make a scatter plot of highway miles per gallon against engine displacement using model names as labels. Here is the scatter plot under the default setting. set.seed(0) df1 &lt;- mpg[sample(nrow(mpg),10),] ggplot(df1,aes(displ,hwy)) + geom_point() + geom_text(aes(label=model)) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;) We see that the text string on the edge is expanding over the boundary. The problem can be addressed by adding hjust=“inward” to align texts to the left or right of the points. ggplot(df1,aes(displ,hwy)) + geom_point() + geom_text(aes(label=model),hjust=&quot;inward&quot;) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;) All the text strings are inside the grid range now. But another problem is that the texts are overlapping with the points. We can try adjusting nudge_x and nudge_y to make horizontal and vertical adjustment to nudge labels and offset text from points. ggplot(df1,aes(displ,hwy)) + geom_point() + geom_text(aes(label=model), hjust=&quot;inward&quot;, vjust=&quot;center&quot;, nudge_y = -.3) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;) Now it looks better. But there are still some texts overlapped with each other, which makes it difficult to read them. We can solve this problem using ggrepel which is a package developed to make adjustment on label positions. 33.1.2 Tip 2: ggrepel In fact, geom_text_repel can solve all the problems mentioned in Method1 in one step. ggplot(df1,aes(displ,hwy)) + geom_point()+ geom_text_repel(aes(label=model)) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;) 33.1.3 Tip 3: directlabels set.seed(2) df2 &lt;- mpg[sample(nrow(mpg),10),] ggplot(df2,aes(displ,hwy)) + geom_point() + geom_text(aes(label=model)) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;) ggplot(df2,aes(displ,hwy)) + geom_point() + geom_dl(aes(label=model), method = &quot;smart.grid&quot;) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;) directlabels package implements the “smart.grid” method in geom_dl, which is able to sparate overlapped texts and points. But it seems that it works worse than geom_text_repel in that after the adjustment some texts are a little far from their corresponding points. Actually, geom_dl fits better in plots with a lot more points. We can see from the above plot there are two “toyota tacoma 4wd” cars but geom_dl outputs only one label. So geom_dl is more suitable and useful for adding direct labels to replace color legend. g3 &lt;- ggplot(mpg,aes(displ,hwy,color=class)) + geom_point() + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Use Legend and Color&quot;) g4 &lt;- ggplot(mpg,aes(displ,hwy,color=class)) + geom_point(show.legend=F) + geom_dl(aes(label=class),method=&quot;smart.grid&quot;) + labs(x = &quot;Engine Displacement&quot;, y = &quot;Highway Miles per Gallon&quot;, title = &quot;Use Label and Color&quot;) grid.arrange(g3, g4, ncol = 2) Clearly the figure on the right is more artistic and readable. geom_dl adds direct labels to a plot, and hide the color legend. ggplot2 shows automatic legends based on the variable specified for color, but these legends can be confusing if there are too many colors. Direct labels are a useful and clear alternative to a confusing legend in many common plots. 33.2 2. Auto wrapping of long labels on axis We use DoctorVisits data set in AER package as an example. When labels are so long that they overlapped with each other and thus are not readable, we can wrap labels via str_wrap in stringr package. data(&quot;DoctorVisits&quot;) DoctorVisits &lt;- DoctorVisits %&gt;% mutate(numvisits = fct_collapse(factor(visits), &quot;No doctor visit in past 2 weeks&quot; = &quot;0&quot;, &quot;One doctor visit in past 2 weeks&quot; = &quot;1&quot;, group_other = TRUE))%&gt;% mutate(numvisits = fct_collapse(numvisits, &quot;No doctor visit in past 2 weeks&quot; = &quot;No doctor visit in past 2 weeks&quot;, &quot;One doctor visit in past 2 weeks&quot; = &quot;One doctor visit in past 2 weeks&quot;, &quot;Two or more doctor visits in past 2 weeks&quot; = &quot;Other&quot;)) g1 &lt;- DoctorVisits %&gt;% group_by(numvisits) %&gt;% summarise(count = n()) %&gt;% ggplot(aes(x=numvisits, y=count)) + geom_bar(stat = &quot;identity&quot;) g2 &lt;- DoctorVisits %&gt;% group_by(numvisits) %&gt;% summarise(count = n()) %&gt;% ggplot(aes(x=numvisits, y=count)) + geom_bar(stat = &quot;identity&quot;) + scale_x_discrete(labels=function(x) str_wrap(x, width=10)) grid.arrange(g1, g2, ncol = 2) 33.3 3. Add shadow effect to labels We can create labels with background shadow by making use of shadowtext package. Again, we take a look at mpg data set. The use of geom_shadowtext is just like geom_text. Besides there are additional parameters in geom_shadowtext like “bg.color” and “size”, which brings different background colors and shadow text size. set.seed(0) df3 &lt;- mpg[sample(nrow(mpg),10),] set.seed(3) df3$angle = sample(0:-30, 10) ggplot(df3,aes(displ,hwy)) + geom_point() + geom_shadowtext(aes(label=model, color = model, angle = angle), hjust=&quot;inward&quot;, vjust=&quot;inward&quot;, nudge_y = -.1, bg.color=&#39;firebrick3&#39;, size = 4) + theme(legend.position=&quot;none&quot;) Together with parameter settings in Tips 1 in Label Placement, the shadow text effect and angle setting makes the labels more readable and beautiful. Also, this shadow effect can be applied to labels on axis. Just like the use of element_text in theme system, now we have element_shadowtext to add shadow effect to labels on axis. x &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;) y &lt;- c(1, 9,-6,13,-2,-8) df3 &lt;- data.frame(x, y) df3$type &lt;- as.factor(ifelse(as.numeric(df3$y)&lt;0, 0, 1)) g &lt;- ggplot(df3, aes(x = x, y = y)) + geom_bar(stat = &quot;identity&quot;, aes(fill = type)) + theme(axis.text.x = element_shadowtext(color = &quot;white&quot;, size = 12, face = &quot;bold&quot;)) g color &lt;- ggplot_build(g)$data[[1]]$fill g + theme(axis.text.x = element_shadowtext(color = color, angle=30, hjust=1,size = 15,face = &quot;bold&quot;)) "],["chinese-translation-of-candela-package.html", "Chapter 34 Chinese Translation of Candela Package 34.1 Candela 34.2 Candela 包使用 34.3 组件 34.4 API文件 34.5 开发人员文件", " Chapter 34 Chinese Translation of Candela Package Wenjie Zhu and Jin Qian library(candela) Source File：https://readthedocs.org/projects/candela/downloads/pdf/latest/ Github Documentation Link: https://candela.readthedocs.io/en/latest/index.html 34.1 Candela Candela是用于Kitware的Resonant平台的可互操作的Web可视化组件的开源套件, 致力于通过标准化API提供可扩展的丰富可视化效果，以用于现实世界的数据科学应用程序。 集成组件包括： LineUp组件：由哈佛大学视觉计算小组和Caleydo项目进行的LineUp动态排名。 UpSet组件：哈佛大学视觉计算小组和Caleydo项目的UpSet集可视化。 OnSet组件：由乔治亚理工学院信息接口小组提供的OnSet可视化设置。 华盛顿大学互动数据实验室的Vega可视化。示例组件：ScatterPlot。 通过Kitware的Resonant平台进行的GeoJS地理空间可视化。示例组件：GeoDots。 34.1.1 开始 34.1.1.1 快速开始 – JavaScript 在名为index.html的文本文件中输入以下内容： &lt;body&gt; &lt;div id=&quot;vis&quot;&gt;&lt;/div&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var data = [ {x: 1, y: 3}, {x: 2, y: 4}, {x: 2, y: 3}, {x: 0, y: 1} ]; var el = document.getElementById(&#39;vis&#39;); var vis = new candela.components.ScatterPlot(el, { data: data, x: &#39;x&#39;, y: &#39;y&#39; }); vis.render(); &lt;/script&gt; &lt;/body&gt; 在浏览器中打开index.html以显示结果可视化。 34.1.1.2 快速开始 – Python 请确保您已安装Python 2.7和pip（在Linux和OS X系统上，本地软件包管理器应该足够了；对于Windows，请 参见此处）。 打开一个shell程序（例如OS X上的终端; Linux上的Bash或Windows上的命令提示符），然后使用以下命令来安装Candela软件包和Requests库，以从网络上获取示例数据： pip install pycandela requests 在UNIX系统上，您可能需要在root或用sudo执行此操作 发出以下命令以在浏览器中启动Jupyter Notebook服务器： jupyter-notebook Create a notebook from the New menu and enter the following in a cell, followed by Shift-Enter to execute the cell and display the visualization: 从“新建”菜单创建一个笔记本，然后在单元格中输入以下内容，然后按Shift-Enter键执行该单元格并显示可视化效果： import requests data = requests.get( &#39;https://raw.githubusercontent.com/vega/vega-datasets/gh-pages/data/iris.json&#39; ).json() import pycandela pycandela.components.ScatterPlot( data=data, color=&#39;species&#39;, x=&#39;sepalLength&#39;, y=&#39;sepalWidth&#39;) 34.1.1.3 快速开始 – R 下载并安装RStudio。 运行以下指令以安装Candela： install.packages(&#39;devtools&#39;) devtools::install_github(&#39;Kitware/candela&#39;, subdir=&#39;R/candela&#39;) 使用以下指令以显示mtcars数据集的散点图： library(candela) candela(&#39;ScatterPlot&#39;, data=mtcars, x=&#39;mpg&#39;, y=&#39;wt&#39;, color=&#39;disp&#39;) 34.2 Candela 包使用 34.2.1 安装 有两种方式可安装Candela。从标准组件仓库中下载（例如npm）或者从Python包索引（PyPI）中下载。从组件仓库中下载的方式比较简单，但仅限于能够下载公共发布版本；从源头下载稍显复杂，但能让你下载并运行最新开发版本。 34.2.1.1 从组件管理系统中安装 34.2.1.1.1 JavaScript 要安装Candela Javascript代码库至当前文件夹，执行以下代码： npm install candela 要安装Candela JavaScript代码库作为你网页应用的dependency, 并且将其加入你的package.json文件中, 执行以下代码: npm install --save candela 独立的JavaScript Candela包可以在node_modules/candela/dist/candela[.min].js中找到。 使用Webpack安装 如果您的项目使用Webpack构建过程，则可以使用Candela的捆绑式Webpack帮助程序功能轻松地将Candela包括在项目中，而不必使用完整大小的捆绑文件。概念是根据项目的需要直接包含Candela源文件，依靠Webpack帮助程序来安排要使用的正确加载程序和其他配置选项。例如，如果没有Webpack，您的源文件可能包含如下行： var candela = require(&#39;candela/dist/candela.min.js&#39;); var ScatterPlot = candela.components.ScatterPlot; 这将导致您的应用程序在运行时加载整个Candela捆绑包，如果您只想使用ScaterPlot组件，则可能不是最佳选择。相反，使用Webpack，可以按以下方式强制转换此代码： var ScatterPlot = require(&#39;candela/components/ScatterPlot&#39;); 为了确保您的构建过程为此文件使用了正确的加载程序，您应确保在项目的Webpack配置中使用Candela webpack帮助程序功能： var candelaWebpack = require(&#39;candela/webpack&#39;); module.exports = candelaWebpack({ // 这里是你原本的webpack设置 }); 这种方法使您的代码更简洁，更有意义，同时避免了不必要的大型应用程序包。 34.2.1.1.2 Python 可以在Python Package Index中找到适用于Candela的Python绑定的最新发行版。安装Candela的最简单方法是通过Python的软件包管理器Pip: 1. 安装依赖软件 安装以下软件: Python 2.7 Pip 在Linux和OS X计算机上，本地软件包管理器应足以安装它们。在Windows上，请查阅本指南以获取有关Python和Pip的建议。 2. 安装Candela Python软件包 在shell中使用这行指令来安装Candela包以及依赖项： pip install pycandela 您可能需要以管理员账户身份运行此命令，或使用使用sudo或类似方式。 34.2.1.2 从源头进行构建安装 在进行源头安装前，您需要使用以下Git指令克隆Candela的代码仓库: git clone git://github.com/Kitware/candela.git 该指令会创建一个名为candela的目录。里面包含了Candela的源代码。使用cd指令进入目录： cd candela 34.2.1.2.1 JavaScript Candela在GitHub上进行开发。如果您希望贡献代码或获取到最新开发版本，则可以按照以下步骤从GitHub下载，构建和安装： 1.下载依赖软件 要从源代码构建Candela，您将需要安装以下软件： Git Node.js npm cairo (在macOS操作系统上使用brew install cairo) 2.安装节点依赖项 使用以下指令以通过节点软件包管理器（NPM）安装必要的节点依赖项： npm install 需要的软件包将会被安装至名为node_modules的目录中。 3.开始构建过程 使用以下指令开始开始构建过程： npm run build 输出将会在build/candela/candela.js中创建一个构建完成的Candela软件包。 观察输出是否有任何错误。大多数情况下，指令运行错误会终止整个构建过程，并打印出具体错误信息。如果你需要任何解读错误信息的帮助，请在GitHub issues或者Gitter chat里给我们留言。 4.查看示例 Candela包含了许多测试需要的示例。它们也有助于学习Candela提供的多种可视化效果。要构建示例，请执行以下代码： npm run build:examples 查看示例，执行以下代码： npm run examples 5.运行测试套件 Candela配有一系列测试。要运行测试套件，你需要调用测试任务如下： npm run test:all 以上代码会执行单元以及图像测试。每个测试套件可单独用以下代码运行： npm run test:unit 以及： npm run test:image 以上每行指令都会在命令行生成摘要报告。 6. 建立文件 Candela使用ReadTheDocs主机上托管的Sphinx 文档。要在本地构建文档，请首先安装所需的Python依赖项： pip install -r requirements-dev.txt 当安装完成后，运行以下指令： npm run docs 文档将托管在 http://localhost:3000/ 主机上。 34.2.1.2.2 Python 1. 安装软件依赖项 要从Python使用Candela，您将需要Python 2.7和pip。 2.在本地安装库 pip install -e . 3.测试安装 使用以下指令以在浏览器中启动Jupyter Notebook服务器： jupyter-notebook 从“新建”菜单中创建一个笔记本，然后在单元格中输入以下内容，然后按Shift-Enter键执行该单元格并显示可视化效果： import requests data = requests.get( &#39;https://raw.githubusercontent.com/vega/vega-datasets/gh-pages/data/iris.json&#39; ).json() import pycandela pycandela.components.ScatterPlot( data=data, color=&#39;species&#39;, x=&#39;sepalLength&#39;, y=&#39;sepalWidth&#39;) 34.2.1.2.3 R-使用install_github或Git Checkout 此过程将直接从GitHub或从Candela的本地Git Checkout安装Candela。 1.安装R，并选择性安装RStudio 2.安装Candela软件包 要直接从GitHub安装： install.packages(&#39;devtools&#39;) devtools::install_github(&#39;Kitware/candela&#39;, subdir=&#39;R/candela&#39;, dependencies = TRUE) 要从Git checkout安装，请将工作目录设置为Git checkout，然后安装并检查安装。check()将运行测试并执行其他程序包检查。 install.packages(&#39;devtools&#39;) devtools::install(dependencies = TRUE) devtools::check() 3.测试安装 下面将创建mtcars数据集的散点图并将其保存到out.html： w &lt;- candela(&#39;ScatterPlot&#39;, data=mtcars, x=&#39;mpg&#39;, y=&#39;wt&#39;, color=&#39;disp&#39;) htmlwidgets::saveWidget(w, &#39;out.html&#39;) 在RStudio中，当您引用可视化而不将其分配给变量时，可视化将出现在您的应用程序中： w 注意： saveWidget在RStudio外部运行时，需要安装Pandoc。请参阅安装说明进行安装。 34.2.2 版本控制 Candela 使用语义法版本命名法来命名版本号。这意味着每个发行版的版本号都可以确定该发行版中存在的功能级别和向后兼容性。 Candela的版本号有两种形式: x.y 和 x.y.z。x为主版本号, y为次版本号, z为补丁版本号。 遵循语义版本控制方法，主要版本代表了整个软件的稳定API。如果主版本号增加，则意味着可以向后兼容。也就是说，适用于1.3版的设置将适用于1.4、1.5和1.10版，但不应期望与2.0版一起使用。 次版本号代表了在前版中加入了新功能。所以1.1版将包含1.0版中未提供的某些功能，且可以确保向后兼容。 当对软件进行错误修复或其他更正时，补丁版本号将递增。 主版本号0比较特殊：本质上，在0.y系列中不保证兼容性。API和行为的稳定性从1.0版开始。 除了标准的语义版本控制做法外，Candela还在Git存储库中将当前版本号标记为“ dev”，从而为从源代码构建的Candela软件包产生了诸如“ 1.1dev”的版本号。发行协议会在将包上传到Python包索引之前，从版本号中删除该标签。 34.3 组件 34.3.1 条形图 (BarChart) 条形图。 x变量应为每个条形包含不同的值，而y变量将对应于每个条形的高度。颜色变量可用于为每个条着色。在单个x值有多个记录的情况下，可以使用合计将值组合成单个条。 该组件可以在candela/plugins/vega插件中找到。 34.3.1.1 示例 JavaScript &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 10; d += 1) { data.push({ a: d, b: d }); } var vis = new candela.components.BarChart(el, { data: data, x: &#39;a&#39;, y: &#39;b&#39; }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [{&#39;a&#39;: d, &#39;b&#39;: d} for d in range(10)] pycandela.components.BarChart(data=data, x=&#39;a&#39;, y=&#39;b&#39;) R library(candela) candela(&#39;BarChart&#39;, data=mtcars, x=&#39;mpg&#39;, y=&#39;wt&#39;, color=&#39;disp&#39;) 34.3.1.2 可选参数 data (Table) 数据表。 x (String) x轴（条形位置）变量。 必须包含数字类数据。 请参见坐标轴刻度。 xType (String) x变量的数据类型。 默认值为’nominal’。 y (String) y轴（条形高度）变量。 必须包含数字类数据。 请参见坐标轴刻度。 yType (String) y变量的数据类型。 默认值为’quantitative’。 color (String) 用于为条形着色的变量。 colorType (String) 颜色变量的数据类型。 默认值为’nominal’。 aggregate (String) 当多个记录中的x值相同时，y值的合计模式。 默认值为’sum’。 width (Number) 图表的宽度（以像素为单位）。 请参阅调整大小。 height (Number) 图表的高度（以像素为单位）。 请参阅调整大小。 renderer (String) 是否以’svg’或’canvas’模式渲染（默认为’画布’）。 34.3.2 箱线图 (BoxPlot) 箱线图。可视化采用一组测量数据（变量），并为每个测量生成箱线图。可选的group变量会将数据划分为具有匹配数值的小组，并为每个组创建一个或一组箱形图。 该组件可以在candela/plugins/vega插件中找到。 34.3.2.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 10; d += 1) { data.push({ a: d, b: d/2 + 7 }); } var vis = new candela.components.BoxPlot(el, { data: data, fields: [&#39;a&#39;, &#39;b&#39;] }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python data = [{&#39;a&#39;: d, &#39;b&#39;: d/2 + 7} for d in range(10)] pycandela.components.BoxPlot(data=data, fields=[&#39;a&#39;, &#39;b&#39;]) R library(candela) candela(&#39;BoxPlot&#39;, data=mtcars, fields=c(&#39;mpg&#39;, &#39;wt&#39;, &#39;disp&#39;)) 34.3.2.2 可选参数 data (Table) 数据表。 fields (Array of String) 用作于测量的变量。 可视化将为每个变量生成一个箱线图。必须包含数字或时间数据。请参见坐标轴刻度。 坐标轴类型将由数组中第一个变量的推断值选择。 x (String) group by 所使用的可选参数。 默认情况下，所有记录都放在一个组中。 请参见轴刻度。 x轴（条形位置）变量。 必须包含数字类数据。 请参见坐标轴刻度。 xType (String) x变量的数据类型。 默认值为’nominal’。 color (String) 用于为箱形图着色的变量。 colorType (String) 颜色变量的数据类型。 默认值为’nominal’。 width (Number) 图表的宽度（以像素为单位）。 请参阅调整大小。 height (Number) 图表的高度（以像素为单位）。 请参阅调整大小。 renderer (String) 是否以’svg’或’canvas’模式渲染（默认为’画布’）。 34.3.3 甘特图 (GanttChart) 甘特图。 数据表必须包含两个数字变量，start和end，用于指定水平条的开始和结束。 label变量可以指定每个项目的名称。 该组件可以在candela/plugins/vega插件中找到。 34.3.3.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = [ {name: &#39;Do this&#39;, level: 1, start: 0, end: 5}, {name: &#39;This part 1&#39;, level: 2, start: 0, end: 3}, {name: &#39;This part 2&#39;, level: 2, start: 3, end: 5}, {name: &#39;Then that&#39;, level: 1, start: 5, end: 15}, {name: &#39;That part 1&#39;, level: 2, start: 5, end: 10}, {name: &#39;That part 2&#39;, level: 2, start: 10, end: 15} ]; var vis = new candela.components.GanttChart(el, { data: data, label: &#39;name&#39;, start: &#39;start&#39;, end: &#39;end&#39;, level: &#39;level&#39;, width: 700, height: 200 }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [ dict(name=&#39;Do this&#39;, level=1, start=0, end=5), dict(name=&#39;This part 1&#39;, level=2, start=0, end=3), dict(name=&#39;This part 2&#39;, level=2, start=3, end=5), dict(name=&#39;Then that&#39;, level=1, start=5, end=15), dict(name=&#39;That part 1&#39;, level=2, start=5, end=10), dict(name=&#39;That part 2&#39;, level=2, start=10, end=15) ]; pycandela.components.GanttChart( data=data, label=&#39;name&#39;, start=&#39;start&#39;, end=&#39;end&#39;, level=&#39;level&#39;, width=700, height=200 ) R library(candela) data &lt;- list( list(name=&#39;Do this&#39;, level=1, start=0, end=5), list(name=&#39;This part 1&#39;, level=2, start=0, end=3), list(name=&#39;This part 2&#39;, level=2, start=3, end=5), list(name=&#39;Then that&#39;, level=1, start=5, end=15), list(name=&#39;That part 1&#39;, level=2, start=5, end=10), list(name=&#39;That part 2&#39;, level=2, start=10, end=15)) candela(&#39;GanttChart&#39;, data=data, label=&#39;name&#39;, start=&#39;start&#39;, end=&#39;end&#39;, level=&#39;level&#39;, width=700, height=200) 34.3.3.2 可选参数 data (Table) 数据表。 label (String) 用于标记每个任务的字段。 start (String) 代表每个任务结束开始的变量。 必须为数字。 end (String) 代表每个任务结束刻度的变量。 必须为数字。 level (String) 用作分层项目级别的字符串。 当前支持两个不同的值，遇到的第一个值将是级别1，该级别将展现的更为突出，而第二个值将是级别2。 type (String) 开始和结束变量的数据类型。 默认值为’quantitative’。 tickCount (String) 建议沿x轴放置的刻度线数量。 axisTitle (String) x轴的标题。 width (Number) 图表的宽度（以像素为单位）。 请参阅调整大小。 height (Number) 图表的高度（以像素为单位）。 请参阅调整大小。 renderer (String) 是否以’svg’或’canvas’模式渲染（默认为’画布’）。 34.3.4 地理空间图 (Geo) 使用GeoJS的地理空间图。 该组件可以在candela/plugins/geojs插件中找到。 34.3.4.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) el.style.width = &#39;500px&#39;; el.style.height = &#39;500px&#39;; document.body.appendChild(el); var data = [ {lat: 41.702, lng: -87.644}, {lat: 41.617, lng: -87.693}, {lat: 41.715, lng: -87.712} ]; var vis = new candela.components.Geo(el, { map: { zoom: 10, center: { x: -87.6194, y: 41.867516 } }, layers: [ { type: &#39;osm&#39; }, { type: &#39;feature&#39;, features: [ { type: &#39;point&#39;, data: data, x: &#39;lng&#39;, y: &#39;lat&#39; } ] } ] }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [ dict(lat=41.702, lng=-87.644), dict(lat=41.617, lng=-87.693), dict(lat=41.715, lng=-87.712) ] pycandela.components.Geo( map=dict( zoom=10, center=dict(x=-87.6194, y=41.867516) ), layers=[ dict(type=&#39;osm&#39;), dict( type=&#39;feature&#39;, features=[ dict(type=&#39;point&#39;, data=data, x=&#39;lng&#39;, y=&#39;lat&#39;) ] ) ] ) R library(candela) data = list( list(lat=41.702, lng=-87.644), list(lat=41.617, lng=-87.693), list(lat=41.715, lng=-87.712)) candela(&#39;Geo&#39;, map=list( zoom=10, center=list(x=-87.6194, y=41.867516) ), layers=list( list(type=&#39;osm&#39;), list( type=&#39;feature&#39;, features=list( list(type=&#39;point&#39;, data=data, x=&#39;lng&#39;, y=&#39;lat&#39;) ) ) ) ) 34.3.4.2 可选参数 map (Object) 描述GeoJS地图选项的key-value对。 layers (Array of Layer) 地图的图层。 34.3.4.3 Layers详述 图层包含描述GeoJS图层选项的key-value对。 这些选项会传递给GeoJS，但type设置为’feature’的图层的’feature’选项除外。 在这种情况下，’feature’选项是一组Feature的详述。 34.3.4.4 Feature详述 每个feature都是具有以下属性的对象： name (String) feature的名字。 type (String) feature的类 （现支持：‘point’）。 data (Table) 数据表。 x (String) 用于feature的x轴参数。 y (String) 用于feature的y轴参数。 34.3.5 地理点图(GeoDots) 使用GeoJS的地理空间视图，其位置用点标记。 纬度（latitude）和经度 （longitude）变量包含数据中每个位置的经/纬度值。 该组件可以在candela/plugins/geojs插件中找到。 34.3.5.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) el.style.width = &#39;500px&#39;; el.style.height = &#39;500px&#39;; document.body.appendChild(el); var data = [ {lat: 41.702, lng: -87.644, a: 5}, {lat: 41.617, lng: -87.693, a: 15}, {lat: 41.715, lng: -87.712, a: 25} ]; var vis = new candela.components.GeoDots(el, { zoom: 10, center: { longitude: -87.6194, latitude: 41.867516 }, data: data, latitude: &#39;lat&#39;, longitude: &#39;lng&#39;, size: &#39;a&#39;, color: &#39;a&#39; }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [ dict(lat=41.702, lng=-87.644, a=5), dict(lat=41.617, lng=-87.693, a=15), dict(lat=41.715, lng=-87.712, a=25) ] pycandela.components.GeoDots( zoom=10, center=dict(longitude=-87.6194, latitude=41.867516), data=data, latitude=&#39;lat&#39;, longitude=&#39;lng&#39;, size=&#39;a&#39;, color=&#39;a&#39; ) R library(candela) data = list( list(lat=41.702, lng=-87.644, a=5), list(lat=41.617, lng=-87.693, a=15), list(lat=41.715, lng=-87.712, a=25)) candela(&#39;GeoDots&#39;, zoom=10, center=list(longitude=-87.6194, latitude=41.867516), data=data, latitude=&#39;lat&#39;, longitude=&#39;lng&#39;, size=&#39;a&#39;, color=&#39;a&#39;) 34.3.5.2 可选参数 data (Table) 数据表。 longitude (String) 经度变量。 latitude (String) 纬度变量。 color (String) 用来为点着色的变量。 size (String) 用来确定点的大小的变量。 该字段必须包含数字值。 zoom (Integer) 初始缩放程度。 center (Object) 具有经度和纬度属性的对象，用于指定地图的初始中心。 tileUrl (String) 切片URL模板（请参见GeoJS OSM图层选项）。 设置为null可完全禁用OSM层。 34.3.6 图级操作 (GLO - Graph-Level Operations) 可视化框架。将数据看作图形的节点，使用定位和视觉命令将它们排列为不同的格式，以实现不同的可视化。 节点表包含一个对象列表，每个对象都有一个包含唯一标识符的id变量以及所需的其他数据属性。边缘表（edges table）包含源头和目标变量（各自指向引用节点表中的id值）、无向或有向的可选类变量、标识每个边缘的id参数、以及权重值（可选）。 width和height控制用于呈现可视化效果的画布的大小。 该组件可以在candela/plugins/glo插件中找到。 34.3.6.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) el.setAttribute(&#39;width&#39;, 700); el.setAttribute(&#39;width&#39;, 700); document.body.appendChild(el); var alphabet = &#39;abcdefghijklmnopqrstuvwxyz&#39;; var vowels = &#39;aeiou&#39;.split(&#39;&#39;); var nodes = []; for (var i = 0; i &lt; 26; i++) { var letter = { id: i, label: alphabet[i], vowel: vowels.indexOf(alphabet[i]) &gt; 0 ? &#39;vowel&#39; : &#39;consonant&#39; }; for (var j = 0; j &lt; 26; j++) { letter[alphabet[j]] = Math.abs(j - i); } nodes.push(letter); } var edges = []; var counter = 0; for (var i = 0; i &lt; 26; i++) { for (var j = i + 1; j &lt; 26; j++) { if (nodes[i][alphabet[j]] &gt; 20) { edges.push({ source: i, target: j, type: &#39;Undirected&#39;, id: counter++, weight: 1 }); } } } var vis = new candela.components.Glo(el, { nodes: nodes, edges: edges, width: 700, height: 200 }); vis.render(); vis.distributeNodes(&#39;x&#39;); vis.colorNodesDiscrete(&#39;vowel&#39;); vis.curvedEdges(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [ {&#39;id&#39;: 0, &#39;label&#39;: &#39;A&#39;, &#39;class&#39;: 0}, {&#39;id&#39;: 1, &#39;label&#39;: &#39;B&#39;, &#39;class&#39;: 1}, {&#39;id&#39;: 2, &#39;label&#39;: &#39;C&#39;, &#39;class&#39;: 1} ] edges = [ {&#39;id&#39;: 0, &#39;source&#39;: 0, &#39;target&#39;: 1}, {&#39;id&#39;: 1, &#39;source&#39;: 0, &#39;target&#39;: 2}, {&#39;id&#39;: 2, &#39;source&#39;: 2, &#39;target&#39;: 1} ] glo = pycandela.components.Glo(nodes=nodes, edges=edges) glo.render() glo.distributeNodes(&#39;x&#39;); glo.colorNodesDiscrete(&#39;class&#39;); glo.curvedEdges(); R library(candela) id = c(0, 1, 2) label = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) class = c(0, 1, 1) nodes = data.frame(id, label, class) source = c(0, 0, 2) target = c(1, 2, 1) edges = data.frame(id, source, target) glo = candela(&#39;SimilarityGraph&#39;, nodes=nodes, edges=edges) glo.render() glo.distributeNodes(&#39;x&#39;) glo.colorNodesDiscrete(&#39;class&#39;) glo.curvedEdges() 34.3.6.2 可选参数 nodes (Table) 节点表。 edges (Table) 边缘表。 width (number) 绘图区域宽度。 height (number) 绘图区域高度。 34.3.6.3 函数方程 colorNodesDiscrete(field) 参数： - field (string) – 要着色的变量 使用分类颜色图（color map）通过变量中的值为节点着色。 colorNodesContinuous(field) 参数： - field (string) – 要着色的变量 使用连续的颜色图（color map）通过变量中的值为节点着色。 colorNodesDefault() 将节点颜色恢复为默认状态（无颜色图）。 sizeNodes(field) 参数： - field (string) – 用于调整大小的变量 根据变量中的值调整节点的大小。 sizeNodesDefault() 将节点大小恢复为默认状态（固定大小）。 distributeNodes(axis[, attr]) 参数： - string (attr) – 分布节点的轴 -string -– 用于对节点进行分组的参数 沿轴均匀放置节点，该轴必须是“ x”，“ y”，“ rho”（径向轴）或“ theta”（角度轴）之一。如果指定了attr，则将根据此attr对节点进行分区和分组。 positionNodes(axis, value) 参数: - axis (string) – 分布节点的轴 - value (string|number) – 用于绘制位置数据的变量或常量 根据值中的数据沿轴定位节点（请参见distributionNodes()）。如果value是字符串，则表示节点表中的列；如果是数字，则所有节点都将被放置于该位置。 forceDirected() 将力导引算法（force-directed positioning）应用于节点。 showEdges() 展示所有节点之间的边。 hideEdges() 隐藏所有节点之间的边。 fadeEdges() 使用透明的灰色渲染边缘。 solidEdges() 使用黑色渲染边缘。 incidentEdges() 仅在鼠标指针悬停在节点上时，渲染入射在该节点上的边缘。 curvedEdges() 使用曲线渲染边缘。 straightEdges() 使用直线渲染边缘。 34.3.7 直方图 (Histogram) 直方图。参数bin指定要汇总的字段。默认情况下，数据表data中的每条记录占bin的总数的1。 通过指定一个汇总字段aggregate，将在每个bin里加总该字段。 该组件可以在candela/plugins/vega插件中找到。 34.3.7.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 1000; d += 1) { data.push({ a: Math.sqrt(-2*Math.log(Math.random()))*Math.cos(2*Math.PI*Math.random()) }); } var vis = new candela.components.Histogram(el, { data: data, x: &#39;a&#39;, width: 700, height: 400 }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela from random import normalvariate as nv data = [{&#39;a&#39;: nv(0, 1)} for d in range(1000)] pycandela.components.Histogram(data=data, x=&#39;a&#39;, width=700, height=400) R library(candela) candela(&#39;Histogram&#39;, data=mtcars, x=&#39;mpg&#39;) 34.3.7.2 可选参数 data (Table) 数据表。 x (String) x轴字段，组合为直方图。 xType (String) x字段的数据类型。默认值是“名义变量”。 aggregate (String) 每个直方图bin中y值的聚合模式。默认值为“计数”，它不使用y值，但将统计bin中出现的记录数。 y (String) y轴字段，当没设置聚合为“计数”时，用于确定直方图条的高度。 yType (String) y字段的数据类型。默认值为“定量”。 color (String) 用于为条形着色的字段。 colorType (String) 颜色字段的数据类型。默认值为“名义变量”。 width (Number) 图表的宽度（以像素为单位）。请参阅API文件的尺寸调整。 height (Number) 图表的高度（以像素为单位）。请参阅API文件的尺寸调整。 renderer (String) 以“矢量图形(svg)”或“画布(canvas)”绘制图像。（默认为“画布”）。 34.3.8 折线图 (LineChart) 折线图。该图表针对单个x字段绘制了y字段的线，可以选择使用系列字段series将其分成多条线。 该组件可以在candela/plugins/vega插件中找到。 34.3.8.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 10; d += 1) { data.push({ a: d, b: d }); } var vis = new candela.components.LineChart(el, { data: data, x: &#39;a&#39;, y: &#39;b&#39;, width: 700, height: 400 }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [{&#39;a&#39;: d, &#39;b&#39;: d} for d in range(10)] pycandela.components.LineChart( data=data, x=&#39;a&#39;, y=&#39;b&#39;, width=700, height=400) R library(candela) candela(&#39;LineChart&#39;, data=mtcars, x=&#39;mpg&#39;, y=&#39;wt&#39;, color=&#39;disp&#39;) 34.3.8.2 可选参数 data (Table) 数据表。 x (String) x轴字段，组合为直方图。 xType (String) x字段的数据类型。默认值是“名义变量”。 y (String) y轴字段，当没设置聚合为“计数”时，用于确定直方图条的高度。 yType (String) y字段的数据类型。默认值为“定量”。 series (String) 用于将数据分隔成多行的可选字段。 seriesType (String) 序列字段的数据类型。默认值为“名义变量”。 colorSeries (Boolean) 是否为不同序列上色并显示图例。默认为“是”。 showPoints (Boolean) 是否在直线上添加点。默认值为“否”。 width (Number) 图表的宽度（以像素为单位）。请参阅API文件的尺寸调整。 height (Number) 图表的高度（以像素为单位）。请参阅API文件的尺寸调整。 renderer (String) 以“矢量图形(svg)”或“画布(canvas)”绘制图像。（默认为“画布”）。 34.3.9 排序图(Lineup) LineUp用于排名可视化。 可以在candela/plugins/lineup插件中找到该组件。 34.3.9.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 10; d += 1) { data.push({ a: d, b: 10 - d, name: d }); } var vis = new candela.components.LineUp(el, { data: data, fields: [&#39;a&#39;, &#39;b&#39;] }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [{&#39;a&#39;: d, &#39;b&#39;: 10 - d, &#39;name&#39;: d} for d in range(10)] pycandela.components.LineUp(data=data, fields=[&#39;a&#39;, &#39;b&#39;]) R library(candela) candela(&#39;LineUp&#39;, data=mtcars, fields=c(&#39;_row&#39;, &#39;mpg&#39;, &#39;wt&#39;, &#39;disp&#39;)) 34.3.9.2 可选参数 data (Table) 数据表。 fields (Array of String) 将在LineUp视图上显示的字段列表。该列表确定字段的顺序。如果未提供，则显示数据中的所有字段。 stacked (Boolean) 是否将分组的度量显示为堆叠的条形（默认为“否”）。 histograms (Boolean) 是否在每个度量的标题中显示直方图（默认为“是”）。 animation (Boolean) 是否在评分标准更改时设置过渡动画（默认为“是”）。 34.3.10 OnSet组件 (OnSet) 34.3.10.1 示例 OnSet集合可视化. OnSet将二进制列（即每行中文本为“ 0”或“ 1”，“true”或“false”，“yes”或“no”的列）解释为集合。 sets选项中的任何字段都将以这种方式解释。 由于大多数数据不是按二进制列排列的，因此可视化还通过field选项支持任意类别的字段。 此列表中指定的每个字段首先将被预处理为一个集合集，每个集合值对应的名称为&lt;fieldName&gt;&lt;value&gt; 例如，假设数据表为： [ {&quot;id&quot;: &quot;n1&quot;, &quot;f1&quot;: 1, &quot;f2&quot;: &quot;x&quot;}, {&quot;id&quot;: &quot;n2&quot;, &quot;f1&quot;: 0, &quot;f2&quot;: &quot;x&quot;}, {&quot;id&quot;: &quot;n3&quot;, &quot;f1&quot;: 0, &quot;f2&quot;: &quot;y&quot;} ] 可以使用以下选项创建OnSet可视化： new OnSet({ data: data, id: &#39;id&#39;, sets: [&#39;f1&#39;], fields: [&#39;f2&#39;] }); 如下所示，这会将f2字段预处理为f2 x和f2 y，并使它们可用于OnSet： f1: n1 f2 x: n1, n2 f2 y: n3 如果rowSets选项设置为true，则集合将转变为： n1: f1, f2 x n2: f2 x n3: f2 y 该组件可以在candela/plugins/onset插件中找到。 34.3.10.2 可选参数 data (Table) 数据表。 id (String) 包含每个记录的唯一ID的字段。 fields (Array of String) 分类字段的列表，这些字段将每个字段中每个不同值的转换为0/1组集合，并填充在OnSet视图中。 rowSets (Boolean) 如果为false，则将列视为集合；如果为true，则将行视为集合。默认为“否”。 34.3.11 散点图 (ScatterPlot) 散点图。 该可视化将在指定的x和y位置绘制值。其他字段可以确定绘制点的颜色color，大小size和形状shape。 该组件可以在candela/plugins/vega插件中找到。 #### 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 10; d += 1) { data.push({ a: d, b: d }); } var vis = new candela.components.ScatterPlot(el, { data: data, x: &#39;a&#39;, y: &#39;b&#39;, width: 700, height: 400 }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [{&#39;a&#39;: d, &#39;b&#39;: d} for d in range(10)] pycandela.components.ScatterPlot( data=data, x=&#39;a&#39;, y=&#39;b&#39;, width=700, height=400) R library(candela) candela(&#39;ScatterPlot&#39;, data=mtcars, x=&#39;mpg&#39;, y=&#39;wt&#39;, color=&#39;disp&#39;) 34.3.11.1 可选参数 data (Table) 数据表。 x (String) x轴字段。 xType (String) x字段的数据类型。默认值是“名义变量”。 y (String) y轴字段。 yType (String) y字段的数据类型。默认值为“定量”。 color (String) 该字段用于给点上色。 colorType (String) color颜色字段的数据类型。默认为“名义变量”。 size (String) 该字段用于改变点的大小。 sizeType (String) size大小字段的数据类型。默认值为“定量”。 shape (String) 该字段用于改变点的形状。 shapeType (String) shape形状字段的数据类型。默认值为“名义变量”。 filled (String) 是填充点还是仅绘制轮廓。默认值为“是”。 width (Number) 图表的宽度（以像素为单位）。请参阅API文件的尺寸调整。 height (Number) 图表的高度（以像素为单位）。请参阅API文件的尺寸调整。 renderer (String) 以“矢量图形(svg)”或“画布(canvas)”绘制图像。（默认为“画布”）。 34.3.12 散点图矩阵 (ScatterPlotMatrix) 散点图矩阵。该可视化将为每对指定字段fields绘制散点图。其他字段可以确定点的大小size，颜色color和形状shape。 该组件可以在candela/plugins/vega插件中找到。 34.3.12.1 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) document.body.appendChild(el); var data = []; for (var d = 0; d &lt; 10; d += 1) { data.push({ a: d, b: 10 - d, name: d }); } var vis = new candela.components.ScatterPlotMatrix(el, { data: data, fields: [&#39;a&#39;, &#39;b&#39;] }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [{&#39;a&#39;: d, &#39;b&#39;: 10 - d, &#39;name&#39;: d} for d in range(10)] pycandela.components.ScatterPlotMatrix(data=data, fields=[&#39;a&#39;, &#39;b&#39;]) R library(candela) candela(&#39;ScatterPlotMatrix&#39;, data=mtcars, fields=c(&#39;mpg&#39;, &#39;wt&#39;, &#39;disp&#39;)) 34.3.12.2 可选参数 data (Table) 数据表。 fields (Array of String) 该字段在散点图矩阵中用作轴。指定N个字段将生成N x N的散点图矩阵。 color (String) 该字段用于给点上色。 colorType (String) 颜色字段的数据类型。默认为“名义变量”。 size (String) 该字段用于改变点的大小。 sizeType (String) size大小字段的数据类型。默认值为“定量”。 shape (String) 该字段用于改变点的形状。 shapeType (String) shape形状字段的数据类型。默认值为“名义变量”。 filled (String) 是填充点还是仅绘制轮廓。默认值为“是”。 width (Number) 图表的宽度（以像素为单位）。请参阅API文件的尺寸调整。 height (Number) 图表的高度（以像素为单位）。请参阅API文件的尺寸调整。 renderer (String) 以“矢量图形(svg)”或“画布(canvas)”绘制图像。（默认为“画布”）。 34.3.13 词树 (SentenTree) 词树：句子可视化。 给定一个有关某个主题的文本样本表，SentenTree尝试抽象出它们之间的共同表达，将它们可视化为“句子树”。 数据表data包含一系列对象，每个对象都有一个id字段，该字段包含每行的唯一标识符，一个文本text字段，包含文本样本，一个计数count字段，表示该样本的强度或在语料库中发生的次数等。 可以在candela/plugins/sententree插件中找到此组件。 #### 示例 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) el.setAttribute(&#39;width&#39;, 1200); el.setAttribute(&#39;width&#39;, 700); document.body.appendChild(el); var data = [ {id: 0, count: 3787, text: &#39;brazil\\&#39;s marcelo scores the first goal of the world cup ... against brazil.&#39;}, {id: 1, count: 2878, text: &#39;at least brazil have scored the first goal of the world cup&#39;}, {id: 2, count: 1702, text: &#39;first game of the world cup tonight! can\\&#39;t wait!&#39;}, {id: 3, count: 1689, text: &#39;the first goal of the world cup is an own goal! marcelo accidentally knocks it into his own net past julio cesar! croatia leads 1-0.&#39;}, {id: 4, count: 1582, text: &#39;goal: brazil 0-1 croatia marcelo scores an own goal in the 11th minute&#39;}, {id: 5, count: 1525, text: &#39;just like we predicted, a brazilian scored the first goal in the world cup&#39;}, {id: 6, count: 1405, text: &#39;whoever bet that the first goal of the world cup was going to be an own goal just made a lot of money.&#39;}, {id: 7, count: 1016, text: &#39;736 players 64 matches 32 teams 12 stadiums 4 years of waiting 1 winning country the 2014 world cup has started .&#39;}, {id: 9, count: 996, text: &#39;watching the world cup tonight! with the tour fam&#39;}, {id: 10, count: 960, text: &#39;the first goal of the world cup was almost as bad as the opening ceremony.&#39;}, {id: 11, count: 935, text: &#39;live from the 2014 fifa world cup in brazil, the unveiling of the happiness flag.&#39;}, {id: 13, count: 915, text: &#39;world cup starts today!!!!!! amazing!!!&#39;}, {id: 14, count: 818, text: &#39;the first goal scored of the world cup 2014... was an own goal!&#39;}, {id: 15, count: 805, text: &#39;after 4 years, the wait is finally over.&#39;}, {id: 16, count: 803, text: &#39;that\\&#39;s not in the script! own goal from marcelo puts croatia up 0-1.&#39;}, {id: 17, count: 746, text: &#39;that moment when you score an own goal in the opening game of the world cup.&#39;}, {id: 18, count: 745, text: &#39;scoring on themselves in the world cup&#39;}, {id: 19, count: 719, text: &#39;world cup 2014 first goal is own-goal by marcelo&#39;} ]; var vis = new candela.components.SentenTree(el, { data: data, graphs: 3 }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [ {&#39;id&#39;: 0, &#39;count&#39;: 3787, &#39;text&#39;: &#39;brazil\\&#39;s marcelo scores the first goal of the world cup ... against brazil.&#39;}, {&#39;id&#39;: 1, &#39;count&#39;: 2878, &#39;text&#39;: &#39;at least brazil have scored the first goal of the world cup&#39;}, {&#39;id&#39;: 2, &#39;count&#39;: 1702, &#39;text&#39;: &#39;first game of the world cup tonight! can\\&#39;t wait!&#39;}, {&#39;id&#39;: 3, &#39;count&#39;: 1689, &#39;text&#39;: &#39;the first goal of the world cup is an own goal! marcelo accidentally knocks it into his own net past julio cesar! croatia leads 1-0.&#39;}, {&#39;id&#39;: 4, &#39;count&#39;: 1582, &#39;text&#39;: &#39;goal: brazil 0-1 croatia marcelo scores an own goal in the 11th minute&#39;}, {&#39;id&#39;: 5, &#39;count&#39;: 1525, &#39;text&#39;: &#39;just like we predicted, a brazilian scored the first goal in the world cup&#39;}, {&#39;id&#39;: 6, &#39;count&#39;: 1405, &#39;text&#39;: &#39;whoever bet that the first goal of the world cup was going to be an own goal just made a lot of money.&#39;}, {&#39;id&#39;: 7, &#39;count&#39;: 1016, &#39;text&#39;: &#39;736 players 64 matches 32 teams 12 stadiums 4 years of waiting 1 winning country the 2014 world cup has started .&#39;}, {&#39;id&#39;: 9, &#39;count&#39;: 996, &#39;text&#39;: &#39;watching the world cup tonight! with the tour fam&#39;}, {&#39;id&#39;: 10, &#39;count&#39;: 960, &#39;text&#39;: &#39;the first goal of the world cup was almost as bad as the opening ceremony.&#39;}, {&#39;id&#39;: 11, &#39;count&#39;: 935, &#39;text&#39;: &#39;live from the 2014 fifa world cup in brazil, the unveiling of the happiness flag.&#39;}, {&#39;id&#39;: 13, &#39;count&#39;: 915, &#39;text&#39;: &#39;world cup starts today!!!!!! amazing!!!&#39;}, {&#39;id&#39;: 14, &#39;count&#39;: 818, &#39;text&#39;: &#39;the first goal scored of the world cup 2014... was an own goal!&#39;}, {&#39;id&#39;: 15, &#39;count&#39;: 805, &#39;text&#39;: &#39;after 4 years, the wait is finally over.&#39;}, {&#39;id&#39;: 16, &#39;count&#39;: 803, &#39;text&#39;: &#39;that\\&#39;s not in the script! own goal from marcelo puts croatia up 0-1.&#39;}, {&#39;id&#39;: 17, &#39;count&#39;: 746, &#39;text&#39;: &#39;that moment when you score an own goal in the opening game of the world cup.&#39;}, {&#39;id&#39;: 18, &#39;count&#39;: 745, &#39;text&#39;: &#39;scoring on themselves in the world cup&#39;}, {&#39;id&#39;: 19, &#39;count&#39;: 719, &#39;text&#39;: &#39;world cup 2014 first goal is own-goal by marcelo&#39;} ] pycandela.components.SentenTree(data=data, id=&#39;id&#39;, count=&#39;count&#39;, text=&#39;text&#39;) R library(candela) id = c(0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19) count = c(3787, 2878, 1702, 1689, 1582, 1525, 1405, 1016, 996, 960, 935, 915, 818, 805, 803, 746, 745, 719) text = c(&#39;brazil\\&#39;s marcelo scores the first goal of the world cup ... against brazil.&#39;, &#39;at least brazil have scored the first goal of the world cup&#39;, &#39;first game of the world cup tonight! can\\&#39;t wait!&#39;, &#39;the first goal of the world cup is an own goal! marcelo accidentally knocks it into his own net past julio cesar! croatia leads 1-0.&#39;, &#39;goal: brazil 0-1 croatia marcelo scores an own goal in the 11th minute&#39;, &#39;just like we predicted, a brazilian scored the first goal in the world cup&#39;, &#39;whoever bet that the first goal of the world cup was going to be an own goal just made a lot of money.&#39;, &#39;736 players 64 matches 32 teams 12 stadiums 4 years of waiting 1 winning country the 2014 world cup has started .&#39;, &#39;watching the world cup tonight! with the tour fam&#39;, &#39;the first goal of the world cup was almost as bad as the opening ceremony.&#39;, &#39;live from the 2014 fifa world cup in brazil, the unveiling of the happiness flag.&#39;, &#39;world cup starts today!!!!!! amazing!!!&#39;, &#39;the first goal scored of the world cup 2014... was an own goal!&#39;, &#39;after 4 years, the wait is finally over.&#39;, &#39;that\\&#39;s not in the script! own goal from marcelo puts croatia up 0-1.&#39;, &#39;that moment when you score an own goal in the opening game of the world cup.&#39;, &#39;scoring on themselves in the world cup&#39;, &#39;world cup 2014 first goal is own-goal by marcelo&#39;) data = data.frame(id, count, text) candela(&#39;SentenTree&#39;, data=data, id=&#39;id&#39;, color=&#39;class&#39;, threshold=0.4) 34.3.13.1 可选参数 data (Table) 数据表。 id（String） ID字段。可以包含任何数据类型，但是该值对于每个数据记录而言都是唯一的。 text（String） 文本样本字段。 count (Integer) 表示每个文本样本的数量或强度的字段。 34.3.14 相似图 (SimilarityGraph) 交互式相似图。给定一个对其他实体的连接强度进行编码的实体列表，此组件将创建一个以实体为节点的图形，并在连接强度超过某个阈值的节点之间出现链接。 数据表data包含一个对象列表，每个对象都有一个id字段，其中包含每个实体的唯一标识符。每个对象还应该有一个由其他实体的id命名的数字字段，其中包含到每个实体的链接强度。如果缺少任何实体的链接强度，则假定其为0。每个对象可以选择包含一个颜色字段color和一个大小size字段，该颜色字段包含标识其颜色的值，大小字段可以是每个半径的数字（以像素为单位）节点，也可以是标识数据中字段的字符串，该字段包含一个将映射到每个节点半径的数字。threshold是一个数值，指定要在图中显示的链接强度的最小值。linkDistance设置所需的链接长度（以像素为单位）。 这个组件可以在candela/plugins/similaritygraph插件中找到。 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var el = document.createElement(&#39;div&#39;) el.setAttribute(&#39;width&#39;, 700); el.setAttribute(&#39;width&#39;, 700); document.body.appendChild(el); var alphabet = &#39;abcdefghijklmnopqrstuvwxyz&#39;; var vowels = &#39;aeiou&#39;.split(&#39;&#39;); var data = []; for (var i = 0; i &lt; 26; i++) { var letter = { id: alphabet[i], size: 10 + i, color: vowels.indexOf(alphabet[i]) &gt; 0 ? &#39;vowel&#39; : &#39;consonant&#39; }; for (var j = 0; j &lt; 26; j++) { letter[alphabet[j]] = Math.abs(j - i); } data.push(letter); } var vis = new candela.components.SimilarityGraph(el, { data: data, size: &#39;size&#39;, threshold: 20 }); vis.render(); &lt;/script&gt; &lt;/body&gt; Python import pycandela data = [ {&#39;id&#39;: &#39;A&#39;, &#39;class&#39;: 0, &#39;A&#39;: 1.0, &#39;B&#39;: 0.5, &#39;C&#39;: 0.3}, {&#39;id&#39;: &#39;B&#39;, &#39;class&#39;: 1, &#39;A&#39;: 0.5, &#39;B&#39;: 1.0, &#39;C&#39;: 0.2}, {&#39;id&#39;: &#39;C&#39;, &#39;class&#39;: 1, &#39;A&#39;: 0.3, &#39;B&#39;: 0.2, &#39;C&#39;: 1.0} ] pycandela.components.SimilarityGraph(data=data, id=&#39;id&#39;, color=&#39;class&#39;, threshold=0.4) R library(candela) id = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) class = c(0, 1, 1) A = c(1.0, 0.5, 0.3) B = c(0.5, 1.0, 0.2) C = c(0.3, 0.2, 1.0) data = data.frame(id, class, A, B, C) candela(&#39;SimilarityGraph&#39;, data=data, id=&#39;id&#39;, color=&#39;class&#39;, threshold=0.4) 34.3.14.1 可选参数 data (Table) 数据表。 id（String） ID字段。可以包含任何数据类型，但是该值对于每个数据记录而言都是唯一的。 color (String) 用于为节点着色的字段。请参阅API文件的色阶。 size (String or Number) 如果是字符串，则该字段用于提供每个节点的半径；如果为数字，则为所有节点使用的半径。 threshold (Number) 高于该强度临界值的链接将出现在图形中。 linkDistance (Number) 每个链接的期望长度（以像素为单位）。 34.3.15 树状图(Tree Heatmap) 带有附加到行和列的可选层次结构的热图。 该组件可以在candela / plugins / treeheatmap插件中找到。 #### 示例 以下示例假定您已下载 示例数据。 JavaScript &lt;body&gt; &lt;script src=&quot;//unpkg.com/candela/dist/candela.min.js&quot;&gt;&lt;/script&gt; &lt;div id=&quot;vis&quot; style=&quot;width:600px;height:600px&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot; &gt; var el = document.getElementById(&#39;vis&#39;); d3.json(&#39;heatmap.json&#39;, function (error, data) { var vis = new candela.components.TreeHeatmap(el, { data: data, scale: &#39;column&#39; }); vis.render(); }); &lt;/script&gt; &lt;/body&gt; Python import pycandela import json data = json.load(open(&#39;heatmap.json&#39;)) pycandela.components.TreeHeatmap(data=data, scale=&#39;column&#39;) R library(candela) library(jsonlite) data &lt;- fromJSON( &#39;https://candela.readthedocs.io/en/latest/_static/heatmap.json&#39;) candela(&#39;TreeHeatmap&#39;, data=data, scale=&#39;column&#39;) 34.3.15.1 可选参数 data (Table) 数据表。 idColumn (String) 具有唯一标识符的列。如果未设置，则可视化将使用名称为空的列，或者(如果存在)使用名为“ _”或“ _id”的列。 scale (String) 指定是使用全局比例尺（“ global”）为数据值着色，单独缩放每一行或每一列（“ row”或“ column”）还是使用适合于相关矩阵（“correlation”）的-1到1色标。如果未指定此参数，则视图使用全局比例。 clusterRows (Boolean) 如果设置为true，则按层次集群链接对行进行排序。此选项需要专门定义的列，名称为“cluster”，“ child1”，“ _ child2”，“ _distance”和“_size”来定义行的聚类。有关如何创建适当的层次结构列的示例，请参见 pysciencedock中的 热图分析。 clusterColumns (Boolean) 如果设置为true，则按层次集群链接对列进行排序。此选项需要专门定义的名为“_cluster”，“child1”，“ child2”，“_distance”和“ _size”的行来定义列的聚类。有关如何创建适当的层次结构行的示例，请参见 pysciencedock中的 热图分析。 threshold (Number) 根据阈值模式设定阈值。 thresholdMode (String) 如果设置，则使用阈值仅显示表格中的某些单元格。有效值为“none”（无阈值），“greater than”（显示大于阈值的值），“less than”（显示小于阈值的值）或“absolute value greater than”（仅显示绝对值大于阈值的值）。如果设置为“ none”以外的任何值，则还必须设置阈值参数。 removeEmpty (Boolean) 如果为true，则全部删除由阈值过滤掉的行和列。如果设置此标志，将不使用按行和列进行聚类。 34.3.16 集合可视化 (UpSet) 集合可视化。 UpSet将二进制列（即每行为文字“0”或“1”，“true”或“false”，“yes”或“no”的列）解释为集合。 sets选项中的任何字段都将以这种方式解释。 由于大多数数据未按照二进制列形式排列，因此可视化还通过“fields”选项支持任意类别的字段。 此列表中指定的每个字段将首先被预处理为0/1列的集合，然后将其传递给UpSet。 例如，假设数据表为： [ {&quot;id&quot;: &quot;n1&quot;, &quot;f1&quot;: 1, &quot;f2&quot;: &quot;x&quot;}, {&quot;id&quot;: &quot;n2&quot;, &quot;f1&quot;: 0, &quot;f2&quot;: &quot;x&quot;}, {&quot;id&quot;: &quot;n3&quot;, &quot;f1&quot;: 0, &quot;f2&quot;: &quot;y&quot;} ] 您可以使用以下选项创建UpSet可视化文件： new UpSet({ data: data, id: &#39;id&#39;, sets: [&#39;f1&#39;], fields: [&#39;f2&#39;] }); 如下所示，这会将f2字段预处理为f2 x和f2 y集合，并使它们可用于UpSet： [ {&quot;id&quot;: &quot;n1&quot;, &quot;f1&quot;: 1, &quot;f2 x&quot;: 1, &quot;f2 y&quot;: 0}, {&quot;id&quot;: &quot;n2&quot;, &quot;f1&quot;: 0, &quot;f2 x&quot;: 1, &quot;f2 y&quot;: 0}, {&quot;id&quot;: &quot;n3&quot;, &quot;f1&quot;: 0, &quot;f2 x&quot;: 0, &quot;f2 y&quot;: 1} ] 该组件可以在插件candela/plugins/upset包中找到。 34.3.16.1 可选参数 data (Table) 数据表。 id（String） 包含每个记录的唯一标识符的字段。 fields (Array of String) 将在UpSet视图中显示的字段列表。 每个字段中的值在传递给UpSet之前都将转换为设置的0/1字段。 sets (Array of String) 将在UpSet视图中显示的字段列表。 假定每个字段已经是0/1集字段。 metadata (Array of String) 向下钻取单个记录时将显示为元数据的字段列表。数字数据还将显示在每组右侧的摘要框图中。 34.4 API文件 34.4.1 Candela JavaScript API candela.components – Candela内置组件。 Sizing Field matchings Data types Visualization components –Candela组件基类。 candela.mixins – Candela内置组件Mixin。 Utilities – Candela实用功能。 34.4.1.1 组件 Candela随附了几个可立即使用的可视化组件。为了使这些组件更容易地包含在您的项目中，它们被分为几个内置插件。每个插件通过其index.js文件导出其内容： import * as candelaVega from &#39;candela/plugins/vega&#39;; let vis = new candelaVega.BarChart(...); 并可以通过其load.js文件将其内容加载到candela.components对象，如下所示： import candela from &#39;candela&#39;; import &#39;candela/plugins/vega/load.js&#39;; let vis = new candela.components.BarChart(...); 您也可以直接导入组件： import BarChart from &#39;candela/plugins/vega/BarChart&#39;; let vis = new BarChart(...); 最后，您还可以导入candela捆绑包，该捆绑包的构建是为了包含每个预装到candela.components中的组件： import candela from &#39;candela/dist/candela&#39;; let vis = new candela.components.BarChart(...); 但是candela捆绑包非常大。使用其他方法之一构建应用程序将产生一个更小，更易于管理的大小的捆绑包。 当前的插件列表是： candela/plugins/vega – 基于Vega的图表，包括基本图表类型，例如条形图，散点图和直方图。 candela/plugins/geojs – 基于GeoJS的组件用于地理空间数据可视化。 candela/plugins/glo – 基于GLO的组件-“图形级操作”。 candela/plugins/lineup – 基于LineUp的组件，用于可视化排序。 candela/plugins/onset – 基于OnSet的组件，用于可视化子集关系。 candela/plugins/sententree - 基于SentenTree的组件，用于可视化文本语料库的语法结构。 candela/plugins/similaritygraph – 一种专用的交互式图形可视化组件，用于调查数据表中节点之间的相似度。 candela/plugins/trackerdash – 基于TrackerDash算法指标跟踪仪表板的组件。 candela/plugins/treeheatmap – 一个结合了层次聚类的热图。 candela/plugins/upset – 基于UpSet的组件，也用于可视化子集关系。 有关每个组件的更多详细信息（包括如何将这些捆绑包导入到项目中），请参阅组件文档的完整列表。 34.4.1.2 尺寸调整 组件通常具有’width’和’height’选项，该选项以像素为单位指定组件的宽度和高度。 34.4.1.3 变量 34.4.1.3.1 坐标轴刻度 几个组件具有与可视化轴相关的选项。这些通常称为x和y，但也可能具有更描述性的名称。该组件通常会自动检测要映射到轴的字段中的值的类型，并会创建适当的轴类型，例如字符串字段的均等值和数字字段和日期字段的连续范围轴。显示连续范围的轴的可视化通常允许通过在可视化区域中拖动和滚动来平移和缩放轴。 The component will often automatically detect the type of values in the field being mapped to an axis and will create an appropriate axis type, such as evenly-spaced values for string fields and a continuous-ranged axis for numeric and date fields. 34.4.1.3.2 色标 许多Candela组件包含一个color选项，该选项将通过指定的变量为可视元素着色。在可能的情况下，颜色将检测列的数据类型并使用适当的色标。对于包含字符串/文本值的字段，可视化将为每个唯一数值使用具有不同颜色的色标。对于包含数字或日期值的变量，可视化将使用从低到高的平滑颜色渐变。 34.4.1.4 资料类型 34.4.1.4.1 表格 Candela表是以下形式的记录的数组： [ { &quot;a&quot;: 1, &quot;b&quot;: &quot;Mark&quot;, &quot;c&quot;: &quot;Jun 1, 2010&quot; }, { &quot;a&quot;: 2, &quot;b&quot;: &quot;Andy&quot;, &quot;c&quot;: &quot;Feb 6, 2010&quot; }, { &quot;a&quot;: 3, &quot;b&quot;: &quot;Joe&quot;, &quot;c&quot;: &quot;Nov 27, 2010&quot; } ] 34.4.1.4.2 可视化组件 VisComponent是Candela可视化组件的基类。此类是故意最小的，因为所有Candela组件只有几个共同的特征： Candela组件可在网络上运行，因此构造函数为new VisComponent(el)，其中el（通常情况下）是DOM元素。 VisComponent构造函数将el附加到对象上，因此您始终可以使用this.el对其进行引用。 Candela组件都会执行某种类型的可视化，因此它们具有各自的生成（render）函数。基类生成函数只会抛出异常。 有时您需要在运行时更改可视化的某个方面，例如颜色图、哪些数据列正在背可视化、甚至是数据本身。为了支持此类更改，Candela组件提供了一种更新方法。基类更新返回一个promise对象，该对象将交付组件本身。 当可视化组件的生命周期即将结束时，可能需要对其进行清理，这可以通过组件的destroy函数来完成。基类destroy只是从this.el中删除其所有内容。您可以通过扩展VisComponent来创建一个具体的可视化组件。 以下最佳做法可最大程度地提高组件的清晰度，可重用性和互操作性（在本文档的其余部分中，假设Component被声明为VisComponent的扩展，例如BarChart）： 构造函数应该使用额外的一个参数选项来封装该组件所有的运行时选项。 组件应在Component.options中报告其预期的输入参数。 var component = new Component(el, options) 构造Candela组件的新实例。 el是用于可视化的有效容器。容器通常是DOM元素，例如 ，但对于某些可视化可能需要另一种容器类型。 options是一个对象，其中包含可视化的初始选项。这包括任何与可视化相关的数据，视觉匹配或其他设置。选项以{name：value}的形式指定。 注意：抽象超类的构造函数为空。您应该将构造函数用于VisComponent的特定子类。 component.render() 使用当前选项集将组件渲染到其容器中。 注意： VisComponent的render()方法只会抛出一个异常。 如果您确实希望组件在渲染时不执行任何操作，只需将方法重新定义为无操作即可。 component.update(options) 更改组件状态以反映选项。此方法允许对组件状态进行增量更改。选项的格式应与构造函数采用的格式相同。区别在于，只有被传输入此函数的参数会被更改，而任何未指定的参数应保持不变。 注意：VisComponent的update()函数返回一个promise对象。 因为每个组件的更新语义都不同，该对象将交付组件本身而不会更改组件。 component.destroy() 当不再需要该组件时，执行任何所需的清理。这可能只是简单的清空组件一直在使用的容器元素，也可能涉及取消注册事件侦听器等。 注意：VisComponent的destroy()函数只是清空顶层容器，因为这是最常见的清空操作。 component.empty() 清空组件容器元素的便捷方法。这可以在构造函数中用于准备容器元素，也可以在destroy函数中用于在组件之后进行清理。 component.serializationFormats serializationFormats参数是受支持格式的字符串列表。格式包括： ‘png’：PNG图片的base64编码的字符串。可以将字符串放在元素的src属性中以显示图像。 ‘svg’：SVG场景的base64编码的字符串。可以将字符串放在元素的src属性中以显示图像。 component.serialize(format) 将组件序列化为指定的格式。 Component.options 此静态属性是Option规范的数组(array)，其中包含此可视化接受的参数的描述。可以用于自检组件以实现诸如自动UI构建之类的功能。 Component.container 包含此可视化可以添加到的容器类型的静态字段。最常见的是DOMElement。 34.4.1.4.3 Mixins VisComponent在创建新组件时，Candela使用mixins向其添加功能。要使用mixin，模式如下： class MyCoolVisualization extends Mixin(candela.VisComponent) { . . . } Mixin使用以下模式定义: const Mixin = Base =&gt; class extends Base { mixinMethod() { . . . } }; 这是一个将基本类映射到新的未命名类的函数表达式。 换句话说，mixin是可以应用于VisComponent（或任何现有组件类）的函数，以产生具有附加功能的新类。 Candela自带有几个mixin，可在插件candela/plugins/mixin中找到。 Events() 向组件添加基本事件处理。该组件获得一个.on()函数和一个事件类型，该函数使用一个命名事件类型的字符串、在事件发生时调用的回调函数、以及一个使用事件类型和可选参数的.trigger()函数来触发该类事件。 InitSize() 根据实例化组件时this.el的大小，将width和height属性写入组件。 Resize() 每当包含元素的大小更改时，使用Events mixin来触发调整大小(resize)事件。 该事件使用元素的新宽度、高度、和组件的reference触发。 AutoResize() 合并了InitSize和Resize mixin的功能，并通过更新this.width和this.height属性自动响应resize事件。 VegaView() 实现Vega或Vega-Lite可视化组件。子类需要实现generateSpec()函数。generateSpec()函数根据视图选项返回适当的Vega / Vega-Lite Specification。 34.4.1.5 选项详述 选项详述将可视化的输入描述为Component.options数组(array)的一部分 。它是一个包含以下属性的对象： name (String) 选项的名称。 type (String) 选项的类型。类型和格式遵循 Girder Worker类型/格式。 format (String) 选项的格式（类型中的特定编码）。类型和格式遵循 Girder Worker类型/格式。 domain (Domain) 可选；限制选项的允许值集。 34.4.1.6 域详述 选项的域限制了选项的允许值集。它是具有以下属性的对象： mode (String) 域模式，‘choice’或’field’之一。 ’choice’模式将允许在“from”参数设置一组固定的选项集。 ’field’模式将允许来自另一个输入的单个变量或变量列表。如果选项类型为’string’，则选项为单个变量；如果选项类型为’ string_list’，则选项接受变量列表。 from (Array 或 String) 如果模式是’choice’，则它是用作下拉列表的字符串数组。如果模式为’field’，则为要从中提取变量的输入的名称。 fieldTypes (Array) 如果模式是’field’, 则指定支持的参数类型。此数组可以包含datalib支持的变量类型的任意组合，包括’string’，‘date’，‘number’，‘integer’，和 ‘boolean’。 34.4.1.7 实用工具 Candela 实用函数。 &gt; util.getElementSize(el) 返回一个对象，该对象的width和height包含以像素为单位的DOM元素el的当前宽度和高度。 util.vega 34.4.1.7.1 用于生成Vega规范的实用工具 util.vega.chart(template, el, options, done) 根据带有选项的实例化模板生成Vega图表。 template是代表图表的Vega模版。 el是放置Vega可视化文件的DOM元素。 options是{key：value} pair的对象，包含在编译模板时使用的选项。选项可以包含任意嵌套的对象和数组。 done是在生成Vega图表时调用的回调函数。该函数采用一个参数–即生成的Vega图表。 util.vega.transform(template, options) 返回具有已给选项的模板实例。这是js：funcʻutil.vega.chart`使用的基础函数，用于在使用Vega库渲染之前实例化其模板。 template是Vega模板。 options是{key：value}pair的对象，包含在编译模板时使用的选项。选项可以包含任意嵌套的对象和数组。 34.4.2 Candela Python API Candela Python代码库允许在Jupyter Notebook中使用交互式Candela可视化效果。 candela.components.ComponentName(**options) 根据已给可选变量创建一个代表Candela可视化效果的对象。ComponentName是Candela组件的名称，例如ScatterPlot。有关组件及其选项的完整列表，请参见组件。 如果将pandas DataFrame作为可选参数传入函数，在将其发送到Candela可视化组件之前，它将被自动转换为以下格式的记录列表： [{“a”: 1, “b”: “foo”}, {“a”: 2, “b”: “baz”}]。 要显示组件，只需简单的将可视化（不需要将其分配给其他变量）作为Notebook单元格中的最后一条语句即可。您还可以使用vis.display()来从单元格中的任意位置显示可视化效果。 34.4.3 Candela R API Candela R代码库通过将Candela转为htmlwidgets来启用R Studio中的交互式Candela可视化效果。 candela(name, …) 创建一个代表定选项指定的Candela可视化效果的小部件。name是Candela组件的名称，例如“ScatterPlot”。有关组件及其选项的完整列表，请参见组件。 Creates a widget representing the Candela visualization specified by the given options. 如果将data frame作为可选变量传入函数，在将其发送到Candela可视化组件之前，它将被自动转换为以下格式的记录列表： [{“a”: 1, “b”: “foo”}, {“a”: 2, “b”: “baz”}]。 34.5 开发人员文件 34.5.1 编码规范指南 我们所有JavaScript代码都遵循JavaScript Semistandard编码规范。 34.5.2 创建Candela代码发布 要执行Candela的新版本发布，请按照以下步骤操作。假设master上的代码已准备好转换为新版本（即，它已通过所有测试并包含新版本所需的所有新功能）。在此示例中，我们将模拟新发行版的版本号为1.2.0。 创建一个新的发行分支，名为release-1.2.0： git checkout -b release-1.2.0 master 通过编辑package.json将版本号提升到1.2.0。使用提交消息“ 提升版本号以发布”进行提交，然后推送分支： vim package.json git commit -am &#39;提升版本号以发布&#39; git push -u origin release-1.2.0 新建一个本地分支以将您的位置保存在此处的提交树中。请确保您的checkout仍为1.2.0版。您可以执行以下操作： git branch save-point 使用“production” NPM脚本构建分发文件： npm run build:production 这将会创建一个dist目录，其中包含两个JavaScript文件（普通版和精简版）。 提交production文件，然后再次推送。 git add dist git commit -m &#39;添加production文件以发布&#39; git push 从release-1.2.0分支创建代码合并请求。请确保将请求的基础分支定为release分支，而不是master分支。 等待“LGTM”（Looks Good To Me）确认信息，然后合并拉取请求并删除release-1.2.0分支。 检出release分支，拉取，标记发布，推送，然后删除release-1.2.0分支。 git checkout release git pull git tag v1.2.0 git push --tags git branch -d release-1.2.0 将新程序包发布到NPM, 您将需要首先使用NPM凭据登录。 npm login npm publish 将保存点分支合并到master中（请勿使用快进合并（fast-forward merge）。因为这是一种特殊的提交类型，它会准备master分支使用新的版本号的下一次开发，而不是添加新功能）。 其次推送，然后删除savepoint。请确保您没有合并release-1.2或release到master中；我们不希望distribution files合并进主线开发分支。 git checkout master git merge save-point git branch -d save-point git push 发行过程到此结束。您发布了一个带有标签的新发行版，并在发行分支上进行了相应的提交， 而master会更新软件包的版本号，以备进一步开发。 34.5.3 测试 34.5.3.1 图像测试 Candela的测试阶段之一就是图像测试：可视化组件的图像会以编程的方式被生成，和基准图像进行比较。 图像会自动上传到Kitware’s Girder 实例，通过Travis构建编号（build number）分类，可以被所有人查看。 "],["greek-translation-of-edav-infohisto.html", "Chapter 35 Greek translation of edav.info/histo 35.1 Διάγραμμα: Ιστόγραμμα 35.2 Επισκόπηση 35.3 Σύνοψη 35.4 Simple examples 35.5 Θεωρία 35.6 Τύποι ιστογραμμάτων 35.7 Παράμετροι 35.8 Διαδραστικά ιστογράμματα με το ggvis 35.9 Εξωτερικές πηγές", " Chapter 35 Greek translation of edav.info/histo Kassiani Papasotiriou 35.1 Διάγραμμα: Ιστόγραμμα 35.2 Επισκόπηση Αυτή η ενότητα καλύπτει τον τρόπο δημιουργίας ιστογραμμάτων. 35.3 Σύνοψη Δώσε μου ένα πλήρες παράδειγμα! Ορίστε μια εφαρμογή ιστογραμμάτων που εξετάζει το πώς άλλαξαν τα ράμφη των σπίνων των Νησιών Γκαλαπάγκος λόγω εξωτερικών παραγόντων: Και εδώ είναι ο κώδικας: library(Sleuth3) # data library(ggplot2) # plotting # load data finches &lt;- Sleuth3::case0201 # finch histograms by year with overlayed density curves ggplot(finches, aes(x = Depth, y = ..density..)) + # plotting geom_histogram(bins = 20, colour = &quot;#80593D&quot;, fill = &quot;#9FC29F&quot;, boundary = 0) + geom_density(color = &quot;#3D6480&quot;) + facet_wrap(~Year) + # formatting ggtitle(&quot;Μεγάλη Ξηρασία Οδήγησε σε Σπίνους με Μεγαλύτερα Ράμφη&quot;, subtitle = &quot;Πυκνότητα Βάθους Ραμφών των Σπίνων των Γκαλαπάγκος ανά Έτος&quot;) + labs(x = &quot;Βάθος Ράμφους (mm)&quot;, caption = &quot;Source: Sleuth3::case0201&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) Για περισσότερες πληροφορίες σχετικά με αυτό το σύνολο δεδομένων, γράψτε ?Sleuth3::case0201 στην κονσόλα. 35.4 Simple examples Έη, όπα, στάσου! Πολύ απλούστερο παρακαλώ! Ας χρησιμοποιήσουμε ένα πολύ απλό σύνολο δεδομένων: # store data x &lt;- c(50, 51, 53, 55, 56, 60, 65, 65, 68) 35.4.1 Ιστόγραμμα με χρήση βασικής R # plot data hist(x, col = &quot;lightblue&quot;, main = &quot;Ιστόγραμμα βασικής R για το x&quot;) Το πλεονέκτημα του ιστογράμματος βασικής R είναι πως μπορεί να ρυθμιστεί εύκολα. Στην πραγματικότητα, το μόνο που χρειάζεσαι για να απεικονίσεις γραφικά τα συγκεκριμένα δεδομένα x είναι η hist(x), αλλά συμπεριλάβαμε λίγο χρώμα και έναν τίτλο ώστε να τα κάνουμε πιο ευπαρουσίαστα. Πλήρης τεκμηρίωση σχετικά με τη hist() μπορεί να βρεθεί εδώ 35.4.2 Ιστόγραμμα με χρήση ggplot2 # import ggplot library(ggplot2) # must store data as dataframe df &lt;- data.frame(x) # plot data ggplot(df, aes(x)) + geom_histogram(color = &quot;grey&quot;, fill = &quot;lightBlue&quot;, binwidth = 5, center = 52.5) + ggtitle(&quot;Ιστόγραμμα ggplot2 για το x&quot;) Η εκδοχή με ggplot είναι λίγο πιο περίπλοκη φαινομενικά, αλλά ως αποτέλεσμα παίρνεις μεγαλύτερη ισχύ και έλεγχο. Σημείωση: Όπως φαίνεται παραπάνω, η ggplot αναμένει ένα πλαίσιο δεδομένων, οπότε εάν λαμβάνεις ένα σφάλμα όπου “η R δεν ξέρει τι να κάνει” όπως αυτό: ggplot dataframe error βεβαιώσου πως χρησιμοποιείς ένα πλαίσιο δεδομένων. 35.5 Θεωρία Σε γενικές γραμμές, το ιστόγραμμα είναι μία από πολλές επιλογές για την προβολή συνεχών δεδομένων. Το ιστόγραμμα μπορεί να δημιουργηθεί εύκολα και γρήγορα. Τα ιστογράμματα είναι λίγο πολύ αυτονόητα: δείχνουν την εμπειρική κατανομή των δεδομένων σου σε ένα σύνολο διαστημάτων. Τα ιστογράμματα μπορούν να χρησιμοποιηθούν σε ανεπεξέργαστα δεδομένα για να δείξουν γρήγορα την κατανομή χωρίς πολλούς χειρισμούς. Χρησιμοποίησε ένα ιστόγραμμα για να πάρεις μια βασική αίσθηση της κατανομής έχοντας ελάχιστες απαιτήσεις για επεξεργασία. • Για περισσότερες πληροφορίες σχετικά με τα ιστογράμματα και τις συνεχείς μεταβλητές, δες το Κεφάλαιο 3 του βιβλίου. 35.6 Τύποι ιστογραμμάτων Χρησιμοποίησε ένα ιστόγραμμα για να δείξεις την κατανομή μιας συνεχούς μεταβλητής. Η κλίμακα του άξονα y μπορεί να αναπαρασταθεί με διάφορους τρόπους για να εκφράσει διαφορετικά αποτελέσματα: 35.6.1 Συχνότητα ή μέτρηση y = αριθμός τιμών που υπάγονται στην κάθε ζώνη 35.6.2 Ιστόγραμμα σχετικής συχνότητας y = αριθμός τιμών που υπάγονται στην κάθε ζώνη / συνολικός αριθμός τιμών 35.6.3 Ιστόγραμμα συνολικής συχνότητας y = συνολικός αριθμός τιμών &lt;= (ή &lt;) του άνω ορίου της ζώνης 35.6.4 Πυκνότητα y = σχετική συχνότητα / εύρος ζώνης 35.7 Παράμετροι 35.7.1 Όρια ζωνών Σκέψου τα όρια των ζωνών και εάν ένα σημείο θα πέσει στην αριστερή ή τη δεξιά ζώνη όταν βρίσκεται πάνω στο όριο. # format layout op &lt;- par(mfrow = c(1, 2), las = 1) # right closed hist(x, col = &quot;lightblue&quot;, ylim = c(0, 4), xlab = &quot;δεξί κλειστό (55, 60]&quot;, font.lab = 2) # right open hist(x, col = &quot;lightblue&quot;, right = FALSE, ylim = c(0, 4), xlab = &quot;δεξί ανοιχτό [55, 60)&quot;, font.lab = 2) 35.7.2 Αριθμός ζωνών Ο προεπιλεγμένος αριθμός των 30 ζωνών στη ggplot2 δεν είναι πάντα ιδανικός, οπότε σκέψου να τον αλλάξεις εάν τα πράγματα φαίνονται περίεργα. Μπορείς να καθορίσεις το εύρος ρητά με το binwidth ή να δώσεις τον επιθυμητό αριθμό ζωνών με το bins. # default...note the pop-up about default bin number ggplot(finches, aes(x = Depth)) + geom_histogram() + ggtitle(&quot;Προεπιλογή με αναδυόμενο παράθυρο για τον αριθμό ζωνών&quot;) Ακολουθούν παραδείγματα αλλαγής των ζωνών με χρήση των δύο τρόπων που περιγράφηκαν παραπάνω: # using binwidth p1 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(binwidth = 0.5, boundary = 6) + ggtitle(&quot;Αλλάχθηκε η τιμή του binwidth&quot;) # using bins p2 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(bins = 48, boundary = 6) + ggtitle(&quot;Αλλάχθηκε η τιμή του bins&quot;) # format plot layout library(gridExtra) grid.arrange(p1, p2, ncol = 2) 35.7.3 Ευθυγράμμιση ζωνών Βεβαιώσου ότι οι άξονες αντικατοπτρίζουν τα πραγματικά όρια του ιστογράμματος. Μπορείς να χρησιμοποιήσεις το boundary για να προσδιορίσεις το τέλος οποιασδήποτε ζώνης ή το center για να προσδιορίσεις το κέντρο οποιασδήποτε ζώνης. Η ggplot2 θα μπορέσει να υπολογίσει πού να τοποθετήσει τις υπόλοιπες ζώνες. (Επίσης, παρατήρησε πως όταν το όριο άλλαξε, ο αριθμός των ζωνών μειώθηκε κατά μία. Αυτό συμβαίνει επειδή ως προεπιλογή οι ζώνες είναι κεντραρισμένες και υπερκαλύπτουν (πιο κάτω/ παραπάνω) το εύρος των δεδομένων.) df &lt;- data.frame(x) # default alignment ggplot(df, aes(x)) + geom_histogram(binwidth = 5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Προεπιλεγμένη Ευθυγράμμιση Ζωνών&quot;) # specify alignment with boundary p3 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, boundary = 60, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Ευθ. Ζωνών με χρήση ορίων&quot;) # specify alignment with center p4 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, center = 67.5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Ευθ. Ζωνών με χρήση κέντρου&quot;) # format layout library(gridExtra) grid.arrange(p3, p4, ncol = 2) Σημείωση: Μη χρησιμοποιείς και το boundary και το center για ευθυγράμμιση των ζωνών. Διάλεξε μόνο το ένα. 35.8 Διαδραστικά ιστογράμματα με το ggvis To πακέτο ggvis δεν βρίσκεται σε εξέλιξη επί του παρόντος, αλλά κάνει ορισμένα πράγματα πολύ καλά, όπως η ενεργή προσαρμογή των παραμέτρων ενός ιστογράμματος κατά την συγγραφή του κώδικα. Από τη στιγμή που οι εικόνες δε μπορούν να μοιραστούν με knitting (όπως συμβαίνει με άλλα πακέτα, όπως το plotly), παρουσιάζουμε εδώ τον κώδικα, αλλά όχι την έξοδο. Για να τα δοκιμάσεις, αντίγραψε και επικόλλησε σε μια συνεδρία R. 35.8.1 Διαδραστική αλλαγή του εύρου ζώνης library(tidyverse) library(ggvis) faithful %&gt;% ggvis(~eruptions) %&gt;% layer_histograms(fill := &quot;lightblue&quot;, width = input_slider(0.1, 2, value = .1, step = .1, label = &quot;width&quot;)) 35.8.2 Παράδειγμα ΑΕΠ df &lt;-read.csv(&quot;countries2012.csv&quot;) df %&gt;% ggvis(~GDP) %&gt;% layer_histograms(fill := &quot;green&quot;, width = input_slider(500, 10000, value = 5000, step = 500, label = &quot;width&quot;)) 35.8.3 Διαδραστική αλλαγή κέντρου df &lt;- data.frame(x = c(50, 51, 53, 55, 56, 60, 65, 65, 68)) df %&gt;% ggvis(~x) %&gt;% layer_histograms(fill := &quot;red&quot;, width = input_slider(1, 10, value = 5, step = 1, label = &quot;width&quot;), center = input_slider(50, 55, value = 52.5, step = .5, label = &quot;center&quot;)) 35.8.4 Αλλαγή κέντρου (με τις τιμές δεδομένων που εμφανίζονται) df &lt;- data.frame(x = c(50, 51, 53, 55, 56, 60, 65, 65, 68), y = c(.5, .5, .5, .5, .5, .5, .5, 1.5, .5)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_histograms(fill := &quot;lightcyan&quot;, width = 5, center = input_slider(45, 55, value = 45, step = 1, label = &quot;center&quot;)) %&gt;% layer_points(fill := &quot;blue&quot;, size := 200) %&gt;% add_axis(&quot;x&quot;, properties = axis_props(labels = list(fontSize = 20))) %&gt;% scale_numeric(&quot;x&quot;, domain = c(46, 72)) %&gt;% add_axis(&quot;y&quot;, values = 0:3, properties = axis_props(labels = list(fontSize = 20))) 35.8.5 Διαδραστική αλλαγή ορίου df %&gt;% ggvis(~x) %&gt;% layer_histograms(fill := &quot;red&quot;, width = input_slider(1, 10, value = 5, step = 1, label = &quot;width&quot;), boundary = input_slider(47.5, 50, value = 50, step = .5, label = &quot;boundary&quot;)) 35.9 Εξωτερικές πηγές Τεκμηρίωση του hist: Σελίδα τεκμηρίωσης ιστογράμματος της βασικής R. Σκονάκι της ggplot2: Πάντα καλό να το έχεις παραδίπλα. "],["korean-translation-of-scatterplot.html", "Chapter 36 Korean translation of scatterplot 36.1 개요 36.2 요약 36.3 간단한 예시 36.4 이론 36.5 사용 시기 36.6 고려 사항 36.7 변경 36.8 추가 자료", " Chapter 36 Korean translation of scatterplot 산점도 by Matthew Lim *Source: https://edav.info/scatter.html 36.1 개요 R을 이용해 산점도를 그리는 방법 36.2 요약 예시 지상에 사는 포유류 62종의 뇌무게와 몸무게 사이의 관계 사용된 코드: library(ggplot2) # 그래프 mammals &lt;- MASS::mammals # 색상 비율 ratio &lt;- mammals$brain / (mammals$body*1000) ggplot(mammals, aes(x = body, y = brain)) + # 색상별로 그룹지정 후 포인트 그리기 geom_point(aes(fill = ifelse(ratio &gt;= 0.02, &quot;#0000ff&quot;, ifelse(ratio &gt;= 0.01 &amp; ratio &lt; 0.02, &quot;#00ff00&quot;, ifelse(ratio &gt;= 0.005 &amp; ratio &lt; 0.01, &quot;#00ffff&quot;, ifelse(ratio &gt;= 0.001 &amp; ratio &lt; 0.005, &quot;#ffff00&quot;, &quot;#ffffff&quot;))))), col = &quot;#656565&quot;, alpha = 0.5, size = 4, shape = 21) + # 데이터 포인트에 대한 텍스트 삽입 geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Mouse&quot;, &quot;Human&quot;, &quot;Asian elephant&quot;, &quot;Chimpanzee&quot;, &quot;Owl monkey&quot;, &quot;Ground squirrel&quot;), paste(as.character(row.names(mammals)), &quot;→&quot;, sep = &quot; &quot;),&#39;&#39;)), hjust = 1.12, vjust = 0.3, col = &quot;grey35&quot;) + geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Golden hamster&quot;, &quot;Kangaroo&quot;, &quot;Water opossum&quot;, &quot;Cow&quot;), paste(&quot;←&quot;, as.character(row.names(mammals)), sep = &quot; &quot;),&#39;&#39;)), hjust = -0.12, vjust = 0.35, col = &quot;grey35&quot;) + # 범례/색상 커스터마이제이션 scale_fill_manual(name = &quot;Brain Weight, as the\\n% of Body Weight&quot;, # values = c(&#39;#e66101&#39;,&#39;#fdb863&#39;,&#39;#b2abd2&#39;,&#39;#5e3c99&#39;), values = c(&#39;#d7191c&#39;,&#39;#fdae61&#39;,&#39;#ffffbf&#39;,&#39;#abd9e9&#39;,&#39;#2c7bb6&#39;), breaks = c(&quot;#0000ff&quot;, &quot;#00ff00&quot;, &quot;#00ffff&quot;, &quot;#ffff00&quot;, &quot;#ffffff&quot;), labels = c(&quot;Greater than 2%&quot;, &quot;Between 1%-2%&quot;, &quot;Between 0.5%-1%&quot;, &quot;Between 0.1%-0.5%&quot;, &quot;Less than 0.1%&quot;)) + # 포맷팅 scale_x_log10(name = &quot;Body Weight&quot;, breaks = c(0.01, 1, 100, 10000), labels = c(&quot;10 g&quot;, &quot;1 kg&quot;, &quot;100 kg&quot;, &quot;10K kg&quot;)) + scale_y_log10(name = &quot;Brain Weight&quot;, breaks = c(1, 10, 100, 1000), labels = c(&quot;1 g&quot;, &quot;10 g&quot;, &quot;100 g&quot;, &quot;1 kg&quot;)) + ggtitle(&quot;An Elephant Never Forgets...How Big A Brain It Has&quot;, subtitle = &quot;Brain and Body Weights of Sixty-Two Species of Land Mammals&quot;) + labs(caption = &quot;Source: MASS::mammals&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) + theme(legend.position = c(0.832, 0.21)) 이 데이터셋에 대한 더 많은 정보가 궁금하시다면 콘솔에 ?MASS::mammals을 입력하세요. 맨 오른쪽 상단 끝에 있는 점이 궁금하셨다면 그 점은 다른 종의 코끼리입니다. 좀 더 구체적으론 아프리카 코끼리입니다. 아프리카 코끼리도 자기 뇌가 얼마나 큰지 잊지않죠. 36.3 간단한 예시 이전 예시는 너무 복잡했어요! 좀 더 간단하게 부탁해요! 이번엔 GDAdata에서 SpeedSki 데이터셋을 통해 참가자가 태어난 해와 그들의 속도의 관계에 대해 알아봅시다: library(GDAdata) head(SpeedSki, n = 7) ## Rank Bib FIS.Code Name Year Nation Speed Sex Event ## 1 1 61 7039 ORIGONE Simone 1979 ITA 211.67 Male Speed One ## 2 2 59 7078 ORIGONE Ivan 1987 ITA 209.70 Male Speed One ## 3 3 66 190130 MONTES Bastien 1985 FRA 209.69 Male Speed One ## 4 4 57 7178 SCHROTTSHAMMER Klaus 1979 AUT 209.67 Male Speed One ## 5 5 69 510089 MAY Philippe 1970 SUI 209.19 Male Speed One ## 6 6 75 7204 BILLY Louis 1993 FRA 208.33 Male Speed One ## 7 7 67 7053 PERSSON Daniel 1975 SWE 208.03 Male Speed One ## no.of.runs ## 1 4 ## 2 4 ## 3 4 ## 4 4 ## 5 4 ## 6 4 ## 7 4 36.3.1 base R을 이용한 산점도 x &lt;- SpeedSki$Year y &lt;- SpeedSki$Speed # 그래프 삽입 plot(x, y, main = &quot;Scatterplot of Speed vs. Birth Year&quot;) 두개의 변수만 있으면 아주 간단하게 Base R을 통해 산점도를 그릴 수 있습니다. 산점도를 통해 범주 변수를 그릴 수도 있지만 대게 연속 변수를 그릴 때 사용됩니다. 36.3.2 ggplot2를 이용한 산점도 library(GDAdata) # 데이터 library(ggplot2) # 그래프 # 메인 그래프 scatter &lt;- ggplot(SpeedSki, aes(Year, Speed)) + geom_point() # 타이틀, 축 이름 추가 scatter + labs(x = &quot;Birth Year&quot;, y = &quot;Speed Achieved (km/hr)&quot;) + ggtitle(&quot;Ninety-One Skiers by Birth Year and Speed Achieved&quot;) ggplot2을 이용해서 산점도를 아주 간단하게 그릴 수 있습니다. geom_point() 코드를 통해 한 그래프에 2개의 aesthetic을 그릴 수 있습니다. 또한, 추가적인 포맷을 통해 그래프를 좀 더 깔끔하게 만드는데 더 욱 더 용이합니다. (실질적으로 필요한건 데이터와 aesthetics과 geom입니다). 36.4 이론 산점도를 통해 변수간의 상관관계를 아주 쉽게 이해할 수 있습니다. 예를 들어 section 13.2의 산점도를 보면 포유류의 뇌무게와 몸무게가 정적 상관관계를 가지고 있는걸 볼 수 있습니다. 산점도를 통해 변수들이 상관관계가 있는지, 있다면 정적 상관인지 부적 상관인지 알 수 있습니다. 하지만 상관관계를 원인과 혼동하시면 안됩니다! 이제 어떻게하면 산점도에 대한 이해도를 높힐 수 있을지 알아봅시다. For more info about adding lines/contours, comparing groups, and plotting continuous variables check out Chapter 5 of the textbook. 36.5 사용 시기 산점도는 변수들간의 관계를 알아볼때 유용합니다. 당신이 변수들간의 관계가 궁금하다면 산점도부터 시작해보세요. 36.6 고려 사항 36.6.1 겹치는 데이터 비슷한 값의 데이터는 산점도에서 겹쳐질 것이며 이는 문제를 일으킬 수 있습니다. 해결방안으로 alpha blending이나 jittering을 고려해보세요 (Overlapping Data의 Iris Walkthrough섹션의 링크). 36.6.2 스케일링 스케일링이 어떻게 산점도의 해석을 바꿀 수 있는지 고려하세요: library(ggplot2) num_points &lt;- 100 wide_x &lt;- c(rnorm(n = 50, mean = 100, sd = 2), rnorm(n = 50, mean = 10, sd = 2)) wide_y &lt;- rnorm(n = num_points, mean = 5, sd = 2) df &lt;- data.frame(wide_x, wide_y) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Linear X-Axis&quot;) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Log-10 X-Axis&quot;) + scale_x_log10() 36.7 변경 36.7.1 등고선 등고선은 데이터의 밀도에 대해 알려줍니다. SpeedSki 데이터셋을 이용해 등고선에 대해 알아봅시다. geom_density_2d()을 사용해 등고선을 추가할 수 있습니다: ggplot(SpeedSki, aes(Year, Speed)) + geom_density_2d() 등고선은 다른 그래프와 사용시 가장 효율적입니다: ggplot(SpeedSki, aes(Year, Speed)) + geom_point() + geom_density_2d(bins = 5) 36.7.2 산점도 행렬 여러개의 매개변수를 비교하고 싶다면 사점도 행렬을 사용하는 것을 고려해보세요. 산점도 행렬을 통해 좀 더 효율적으로 변수들을 비교할 수 있습니다. ggplot2movies패키지의 movies 데이터셋을 통해 산점도 행렬에 대해 더 자세히 알아봅시다. base R의 plot() 펑션을 통해 산점도 행렬을 그릴 수 있습니다: library(ggplot2movies) # 데이터 library(dplyr) # 데이터 편집 index &lt;- sample(nrow(movies), 500) #샘플 데이터 moviedf &lt;- movies[index,] # 데이터 프레임 splomvar &lt;- moviedf %&gt;% dplyr::select(length, budget, votes, rating, year) plot(splomvar) base R의 plot() 펑션을 개인적인 연구를 위해 사용하는건 괜찮지만 프레젠테이션을 위해 해당 펑션을 사용하는 것은 추천드리지 않습니다. 해당 펑션을 사용해 산점도 행렬을 그릴 시 Hermann grid illusion 때문에 해당 그래프를 해석하기게 매우 어렵습니다. lattice의 splom() 펑션을 통해 이 문제를 해결할 수 있습니다: library(lattice) #sploms splom(splomvar) 36.8 추가 자료 Quick-R article Base R을 통한 산점도. 산점도 행렬, 고밀도, 3D 버전의 간단한 예시부터 어려운 예시까지 있습니다. STHDA Base R: Base R을 통한 산점도에 대한 자료. 더 많은 예시가 있습니다. STHDA ggplot2: ggplot2를 통한 산점도에 대한 자료. 포맷과 face wraps에 대해 더 자세히 알려줍니다. Stack Overflow geom_point()의 포인트에 라벨을 더해주는 방법 ggplot2 cheatsheet: 언제나 가지고 있으면 좋은 자료. "],["rmarkdown-tutorial-chinese-translation-rmarkdown-.html", "Chapter 37 Rmarkdown tutorial Chinese translation: Rmarkdown 中文版指南 37.1 1. 概述 37.2 2. 入门 37.3 3. Markdown语法 37.4 4. 嵌入代码 37.5 5. 渲染", " Chapter 37 Rmarkdown tutorial Chinese translation: Rmarkdown 中文版指南 Author: Yuzheng Jia 37.0.0.1 源文件链接 https://jtr13.github.io/cc19/rmarkdown-tutorial.html 37.1 1. 概述 R Markdown为数据科学提供了一个整洁的框架。Markdown文件通常可以帮助我们： - 保存并执行您编写的代码 - 生成可以与受众共享的高质量报告 R Markdown文件是可完全重新实现的，并支持数十种静态和动态输出格式。 该链接提供了R Markdown功能的快速介绍。-链接 37.1.1 1.1 什么是R Markdown？ Rmd文件 · R Markdown（.Rmd）文件是您的项目的记录。它包含他人重现您的作品所需的代码以及帮助他人可以理解您所做工作的所有脚本。 可重实现的研究 · 您可以使用 Knit来重新运行R Markdown文件中的代码以重实现您的作品并将结果导出为格式正确的报告。 动态文档 · 有很多方法可以导出报告。格式包括html、pdf、MS Word或RTF文档、基于html或pdf的幻灯片、Notebooks等。 37.1.2 1.2 工作流程 打开一个新的.Rmd文件 ：文件 ▶ 新文件 ▶ R Markdown。 通过编辑模板编写文档 编织文件以创建报告；使用编织按钮或 render()进行编织 在IDE窗口中预览输出 发布（可选）到Web服务器 在R Markdown控制台中检查构建日志 使用输出文件：与.Rmd一起保存的 37.2 2. 入门 37.2.1 2.1. 安装套件 您可以使用以下命令来安装所需的库。 install.packages(&#39;rmarkdown&#39;) 37.2.2 2.2.打开文件 您可以创建一个新文件或从您选择的目录中打开现有文件。 37.2.3 2.3. 输出格式 R markdown可以将任何Rmd文件呈现为R markdown支持的格式。例如，下面的代码将OutputExample.Rmd呈现为Microsoft Word文档。 library(rmarkdown) # render(&quot;resources/rmarkdown_tutorial/OutputExample.Rmd&quot;, output_format = &quot;word_document&quot;) 以下是可以选择的所有格式的表： 输出值表 输出值 输出 html_document html pdf_document pdf (需要Tex ) word_document Microsof Word (.docx) odt_document OpenDocument Text rtf_document Rich Text Format md_document Markdown github_document Github兼容的markdown ioslides_presentation ioslides HTML slides slidy_presentation slidy HTML slides beamer_presentation Beamer pdf slides (需要Tex) 您可以通过单击编织按钮旁边的下拉菜单来选择呈现所需的格式： 37.3 3. Markdown语法 Rmarkdown有许多精美的语法，因此您可以生成有序且美观的文档。 我们将在此处提供我们经常使用的语法。 纯文本 纯文本 斜体和粗体 *斜体* and **粗体** 列表 * 无序列表 + 分项1 + 分项2 - 次分项1 无序列表 分项1 分项2 次分项1 1. 有序列表 2. 项2 i) 分项1 A. 次分项1 有序列表 项2 分项1 A. 次分项1 标头 # 标头1 {#anchor} ## 标头2 {#css_id} ### 标头3 {.css_class} #### 标头4 ##### 标头5 ###### 标头6 超连接 &lt;http://www.rstudio.com&gt; [link](www.rstudio.com) Jump to [Header 1](#anchor) http://www.rstudio.com link Jump to Header 1 表 | 右 | 左 | 默认 | 中央 | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | 右 左 默认 中央 12 12 12 12 123 123 123 123 1 1 1 1 方程 $$E = mc^{2}$$ \\[E = mc^{2}\\] 37.4 4. 嵌入代码 37.4.1 4.1. 内联代码 您可以在代码前后加上勾号和r。R将用其结果替换内联代码。 例如： 一加一等于 `r 1+1` 输出为: 一加一等于2 37.4.2 4.2. 代码块 您需要以 ``` {r}开始一个块并以```结尾。 例如： print(&quot;Hello, world!&quot;) ## [1] &quot;Hello, world!&quot; 37.4.3 4.3. 显示选项 还有很多选项可以显示您的代码和结果。 例如，您可以使用eval在输出中选择TRUE或FALSE来决定是否评估代码并包括其结果。 这是两个选项之间的区别： eval = TRUE: print(&quot;Hi there!&quot;) ## [1] &quot;Hi there!&quot; eval = FALSE: print(&quot;Hi there!&quot;) 下表包含我们通常使用的选项 选项 默认 效果 eval TRUE Whether to evaluate the code and include its results echo TRUE Whether to display code along with its results warning TRUE Whether to display warnings error FALSE Whether to display errors message TRUE Whether to display messages tidy FALSE Whether to reformat code in a tidy way when displaying it cache FALSE Whether to cache results for future renders comment “##” Comment character to preface results with 37.5 5. 渲染 第一种方式 您可以在控制台中运行rmarkdown::render(\"&lt;file path&gt;\")。 第二种方式 您可以单击Knit顶部窗格中的按钮，然后选择所需的输出格式。 "],["webscraping-dynamic-content-rselenium-tutorial.html", "Chapter 38 Webscraping Dynamic Content: Rselenium Tutorial 38.1 Introduction and Setup 38.2 Simple Illustrations with Websites 38.3 Other Features", " Chapter 38 Webscraping Dynamic Content: Rselenium Tutorial Chenchao You 38.1 Introduction and Setup This is a tutorial on webscrapping dynamic content with the help of RSelenium. In previous courses we have learned how to use rvest to webscrap html pages. Specifically, recall that in problem set 2 we are asked to webscrape “https://www.metacritic.com/publication/digital-trends” to gather information about metascores, using rvest package and SelectorGadget. metacritic &lt;- read_html(&quot;https://www.metacritic.com/publication/digital-trends&quot;) title_html &lt;- html_nodes(metacritic,&#39;.review_product a&#39;) title_data &lt;- html_text(title_html) meta_html &lt;- html_nodes(metacritic,&#39;.brief_metascore .game&#39;) meta_score &lt;- html_text(meta_html) crit_html &lt;- html_nodes(metacritic,&#39;.brief_critscore .indiv&#39;) crit_score &lt;- html_text(crit_html) This webpage is not dynamic so we could scrape the content by identifying CSS component using SelectorGadget and extract the data from html with rvest. However, most of the modern website use dynamic content. An example of this is Linkedin or Twitter. When you scroll down the webpage, new content is loaded without changing the URL. What if say, we want to webscrape Donald Trump’s Twitter information? Preparations First you need to make sure you have a working environment. That includes the corresponding R packages, a working JAVA environment installed on your OS, and a installed browser. In this tutorial I’ll use chrome version “86.0.4240.22” as the browser. install.packages(&quot;RSelenium&quot;) install.packages(&quot;rvest&quot;) install.packages(&quot;tidyverse&quot;) After the environment is properly set, we can proceed to initialize our browser. rD &lt;- rsDriver(browser=&quot;chrome&quot;, chromever=&quot;86.0.4240.22&quot;, verbose=T) remDr &lt;- rD[[&quot;client&quot;]] Some common error messages include port occupied or failed to receive handshake. Either try resetting connection with system(&quot;taskkill /im java.exe /f&quot;, intern=FALSE, ignore.stdout=FALSE) or use VPN in the internet connection. If everything is setup correctly, you should see a browser popping up. Now you are ready to navigate dynamic pages using RSelenium. 38.2 Simple Illustrations with Websites We will use “https://www.google.com” as a simple example to use some Rselenium functions. To navigate to google webpage: remDr$navigate(&quot;https://www.google.com&quot;) Rselenium use HTML, CSS or XPath to find which object to operate on. More information on the locator strategies for webdrivers can be found on https://stackoverflow.com/questions/48369043/official-locator-strategies-for-the-webdriver/48376890#48376890. However for the time being, we only need to understand some simple locator techniques. Some common locators include: id, name, css selector, xpath To find information about the target object (in our example the google search text input box), right click and select inspect: We spot name=‘q’ to locate the target object. In this case use $findElement function to select the textbox and use sendKeysToElement to input the text you want input &lt;- &quot;edav&quot; selected &lt;- remDr$findElement(using = &quot;name&quot;, value = &quot;q&quot;) selected$sendKeysToElement(list(input)) Next we want to click the search button to search EDAV on google. Again use Inspect to locate the button object, and we found name=‘btnK’ to locate our button remDr$findElements(&quot;name&quot;, &quot;btnK&quot;)[[1]]$clickElement() Now we have the search result of EDAV. To scroll up or down the webpage, select the entire webpage using: webpage &lt;- remDr$findElement(&quot;css&quot;, &quot;body&quot;) 38.3 Other Features Rselenium can simulate shortcut keys of the browser. selKeys function gives the list of shortcut keys. selKeys We can use: home, end, up_arrow, or down_arrow to navigate the webpage webpage$sendKeysToElement(list(key = &quot;home&quot;)) webpage$sendKeysToElement(list(key = &quot;end&quot;)) webpage$sendKeysToElement(list(key = &quot;up_arrow&quot;)) webpage$sendKeysToElement(list(key = &quot;down_arrow&quot;)) Read HTML After using RSelenium to get the webpage in the form we want, we can extract html using command $getPageSource(). Remember to give page time to fully load if your internet connection is poor. Sys.sleep(3) html &lt;- remDr$getPageSource()[[1]] This html object can then be processed by all the rvest tools we have learned earlier. More information on rvest can be found on https://github.com/tidyverse/rvest if you are not familiar with this tool already. Summary RSelenium provides R bindings for the Selenium Webdriver, which is a powerful tool for automated web browsers. We have only covered a few simple functions in this tutorial. To realize the full potential of this package, we encourage the reader to explore the documentation of RSelenium on https://cran.r-project.org/web/packages/RSelenium/vignettes/basics.html. Background knowledge of HTML and CSS are also super helpful in the webscraping process. "],["brief-introduction-and-tutorial-of-ggpubr-package.html", "Chapter 39 Brief Introduction and Tutorial of ggpubr Package 39.1 Installation and loading 39.2 Introduction 39.3 Histogram 39.4 Density plot 39.5 Box plot 39.6 Violin plot 39.7 Bar plot 39.8 Lollipop Chart 39.9 Cleveland dot plot", " Chapter 39 Brief Introduction and Tutorial of ggpubr Package Heng Kan and Haichao Yi 39.1 Installation and loading install.packages(&quot;ggpubr&quot;) library(ggplot2) library(ggpubr) 39.2 Introduction ggplot2, by Hadley Wickham, is an excellent and flexible package for elegant data visualization in R. However the default generated plots requires some formatting before we can send them for publication. Furthermore, to customize a ggplot, the syntax is opaque and this raises the level of difficulty for researchers with no advanced R programming skills. The ggpubr package provides some easy-to-use functions for creating and customizing ‘ggplot2’- based publication ready plots. We will give some examples about how to use easy functions in ggpubr to give various kinds of plots. 39.3 Histogram We construct a dataset with attributes sex and height where there are 300 people for each sex and values of height are in inches. set.seed(2000) heightdata &lt;- data.frame( sex = factor(rep(c(&quot;Female&quot;, &quot;Male&quot;), each=300)), height = c(rnorm(300, 65), rnorm(300, 69))) Now we use heightdata to plot the corresponding histogram. Here we use the function gghistogram where we add mean lines showing mean values for each sex and marginal rug showing one-dimensional density plot on the axis. gghistogram(heightdata, x = &quot;height&quot;, add = &quot;mean&quot;, rug = TRUE, color = &quot;sex&quot;, fill = &quot;sex&quot;, bins= 15) The histogram is good-looking and straightforward to plot. 39.4 Density plot Now we still use heightdata to plot the corresponding two-dimensional density plot. Here we still add the mean lines and marginal rug. ggdensity(heightdata, x = &quot;height&quot;, add = &quot;mean&quot;, rug = TRUE, color = &quot;sex&quot;, fill = &quot;sex&quot;) The density plot is similar as the histogram, it is also easy and straightforward to plot. 39.5 Box plot We use the first 50 rows of ais dataset in ‘alr4’ package. data(ais, package = &quot;alr4&quot;) ais &lt;- ais[1:50,] head(ais,10) ## Sex Ht Wt LBM RCC WCC Hc Hg Ferr BMI SSF Bfat Label ## 1 1 195.9 78.9 63.32 3.96 7.5 37.5 12.3 60 20.56 109.1 19.75 f-b_ball ## 2 1 189.7 74.4 58.55 4.41 8.3 38.2 12.7 68 20.67 102.8 21.30 f-b_ball ## 3 1 177.8 69.1 55.36 4.14 5.0 36.4 11.6 21 21.86 104.6 19.88 f-b_ball ## 4 1 185.0 74.9 57.18 4.11 5.3 37.3 12.6 69 21.88 126.4 23.66 f-b_ball ## 5 1 184.6 64.6 53.20 4.45 6.8 41.5 14.0 29 18.96 80.3 17.64 f-b_ball ## 6 1 174.0 63.7 53.77 4.10 4.4 37.4 12.5 42 21.04 75.2 15.58 f-b_ball ## 7 1 186.2 75.2 60.17 4.31 5.3 39.6 12.8 73 21.69 87.2 19.99 f-b_ball ## 8 1 173.8 62.3 48.33 4.42 5.7 39.9 13.2 44 20.62 97.9 22.43 f-b_ball ## 9 1 171.4 66.5 54.57 4.30 8.9 41.1 13.5 41 22.64 75.1 17.95 f-b_ball ## 10 1 179.9 62.9 53.42 4.51 4.4 41.6 12.7 44 19.44 65.1 15.07 f-b_ball ## Sport ## 1 b_ball ## 2 b_ball ## 3 b_ball ## 4 b_ball ## 5 b_ball ## 6 b_ball ## 7 b_ball ## 8 b_ball ## 9 b_ball ## 10 b_ball We now draw a box plot with jittered points of Wt variable within different groups of Sports. We can also change the colors and shapes based on the kind of sports. pbox &lt;- ggboxplot(ais,x = &quot;Sport&quot;, y = &quot;Wt&quot;, color = &quot;Sport&quot;, add = &quot;jitter&quot;, shape = &quot;Sport&quot;) pbox This boxplot is different from what we have seen in geom_boxplot(). In this boxplot, we can see the distribution of all points within different groups of Sports. Also, we can change the color and shape very easily. We can set up customized comparison of mean between groups. And the result p-value of ANOVA can be added to the graph as well. The p-value of Kruskal-Wallis test can also be shown on the graph. mycomparisons &lt;- list(c(&quot;b_ball&quot;, &quot;netball&quot;), c(&quot;netball&quot;, &quot;row&quot;), c(&quot;b_ball&quot;, &quot;row&quot;)) pbox + stat_compare_means(comparisons = mycomparisons)+ stat_compare_means(label.y = 50) 39.6 Violin plot We still use the first 50 rows of ais dataset. This time, we draw a violin plot regarding Wt variable with boxplots inside. And we can also compare means with ths plot. Just like the boxplot we have above, we can have the p-value of ANOVA on the violin plot as well. ggviolin(ais, x = &quot;Sport&quot;, y = &quot;Wt&quot;, fill = &quot;Sport&quot;, add = &quot;boxplot&quot;, add.params = list(fill = &quot;white&quot;))+ stat_compare_means(comparisons = mycomparisons)+ stat_compare_means(label.y = 50) 39.7 Bar plot We use the mtcars dataset. We convert the gear variable to a factor. And we add the name columns. data(mtcars) cars &lt;- mtcars cars$gear &lt;- factor(cars$gear) cars$name &lt;- rownames(cars) head(cars[,c(&quot;name&quot;, &quot;wt&quot;, &quot;hp&quot;,&quot;gear&quot;)]) ## name wt hp gear ## Mazda RX4 Mazda RX4 2.620 110 4 ## Mazda RX4 Wag Mazda RX4 Wag 2.875 110 4 ## Datsun 710 Datsun 710 2.320 93 4 ## Hornet 4 Drive Hornet 4 Drive 3.215 110 3 ## Hornet Sportabout Hornet Sportabout 3.440 175 3 ## Valiant Valiant 3.460 105 3 Now we draw a bar plot of hp variable and we change the fill color by grouping variable gear. Sorting will be done globally. ggbarplot(cars, x = &quot;name&quot;, y = &quot;hp&quot;, fill = &quot;gear&quot;, color = &quot;white&quot;, palette = &quot;jco&quot;, sort.val = &quot;desc&quot;, sort.by.groups = FALSE, x.text.angle = 60, ylab = &quot;Horse Power&quot;, xlab = FALSE, legend.title=&quot;Gear&quot; ) Now we sort bars inside each group. ggbarplot(cars, x = &quot;name&quot;, y = &quot;hp&quot;, fill = &quot;gear&quot;, color = &quot;white&quot;, palette = &quot;jco&quot;, sort.val = &quot;asc&quot;, sort.by.groups = TRUE, x.text.angle = 60, ylab = &quot;Horsepower&quot;, xlab = FALSE, legend.title=&quot;Gear&quot; ) We can standarize the hp and compare each model with the mean. cars$hp_z &lt;- (cars$hp-mean(cars$hp))/sd(cars$hp) cars$hp_grp &lt;- factor(ifelse(cars$hp_z&lt;0, &quot;low&quot;,&quot;high&quot;), levels=c(&quot;low&quot;, &quot;high&quot;)) head(cars[,c(&quot;name&quot;, &quot;wt&quot;, &quot;hp&quot;, &quot;hp_grp&quot;, &quot;gear&quot;)]) ## name wt hp hp_grp gear ## Mazda RX4 Mazda RX4 2.620 110 low 4 ## Mazda RX4 Wag Mazda RX4 Wag 2.875 110 low 4 ## Datsun 710 Datsun 710 2.320 93 low 4 ## Hornet 4 Drive Hornet 4 Drive 3.215 110 low 3 ## Hornet Sportabout Hornet Sportabout 3.440 175 high 3 ## Valiant Valiant 3.460 105 low 3 ggbarplot(cars, x=&quot;name&quot;, y=&quot;hp_z&quot;, fill = &quot;hp_grp&quot;, color = &quot;white&quot;, palette = &quot;jco&quot;, sort.val = &quot;asc&quot;, sort.by.groups = FALSE, x.text.angle=60, ylab = &quot;Horsepower z-scores&quot;, xlab = FALSE, legend.title=&quot;Horsepower Group&quot;) We can rotate it with adding just one line of code: rotate=TRUE ggbarplot(cars, x=&quot;name&quot;, y=&quot;hp_z&quot;, fill = &quot;hp_grp&quot;, color = &quot;white&quot;, palette = &quot;jco&quot;, sort.val = &quot;asc&quot;, sort.by.groups = FALSE, x.text.angle=90, ylab = &quot;Horsepower z-scores&quot;, xlab = FALSE, legend.title=&quot;Horsepower Group&quot;, rotate=TRUE, ggtheme = theme_minimal()) 39.8 Lollipop Chart Lollipop chart is an alternative to bar plots, when you have a large set of values to visualize. ggdotchart(cars, x=&quot;name&quot;, y=&quot;hp&quot;, color=&quot;gear&quot;, palette = &quot;jco&quot;, sorting = &quot;ascending&quot;, add = &quot;segments&quot;, ggtheme = theme_pubr(), legend.title = &quot;Gear&quot;, ylab=&quot;Horsepower&quot;, xlab=FALSE) We can custom dots with labels: ggdotchart(cars, x=&quot;name&quot;, y=&quot;hp&quot;, color=&quot;gear&quot;, palette = &quot;jco&quot;, sorting = &quot;descending&quot;, add = &quot;segments&quot;, rotate=TRUE, group=&quot;gear&quot;, dot.size=6, label=round(cars$mpg), font.label = list(color=&quot;white&quot;, size=9, vjust=0.5), ggtheme = theme_pubr(), legend.title = &quot;Gear&quot;, ylab=FALSE, xlab = &quot;Horsepower&quot;) 39.9 Cleveland dot plot We still use mtcars dataset to draw a Cleveland dot plot. ggdotchart(cars, x = &quot;name&quot;, y = &quot;hp&quot;, color = &quot;gear&quot;, palette = &quot;jco&quot;, sorting = &quot;descending&quot;, rotate = TRUE, dot.size = 2, ggtheme = theme_pubr(), legend.title = &quot;Gear&quot;)+ theme_cleveland() It is more interesting to color y text by groups. All we need to add is: y.text.col=TRUE ggdotchart(cars, x = &quot;name&quot;, y = &quot;hp&quot;, color = &quot;gear&quot;, palette = &quot;jco&quot;, sorting = &quot;descending&quot;, rotate = TRUE, dot.size = 2, ggtheme = theme_pubr(), y.text.col=TRUE, legend.title = &quot;Gear&quot;)+ theme_cleveland() As we can see, it’s a lot easier to plot required graphs using ggpubr without learning much about layers of ggplot2, which makes graphing less difficult for people who are not familiar with R programming. However, for people who want to customize more with their plots, ggplot2 is still a better option. Sources: https://cran.r-project.org/web/packages/ggpubr/index.html https://github.com/kassambara/ggpubr "],["visualization-of-geographical-maps.html", "Chapter 40 Visualization of geographical maps", " Chapter 40 Visualization of geographical maps Yuxin Qian library(maps) library(tmap) 40.0.1 1. What is map visualization When dealing with geographically-related data, you might want to show the result on a map instead of just a bin graph for a more concrete and vivid effect. Let’s first look at an example. The following graph from TopData highlights the most popular fast food chain in each state during quarantine.(Source) I accidentally saw it on Twitter one day (during my quarantine) and thought it would be really helpful and interesting if I know how to draw such a graph. an map image Therefore, in this article, I will briefly introduce how to conduct map visualization in R. 40.0.2 2. How to draw a map in R There are several useful package related to various geographical maps. First, we are going to use the package maps. The package includes the world map, as well as maps of several countries. map(database = &quot;state&quot;) title(&quot;The United States&quot;) And it also includes state maps for the U.S. map(&quot;county&quot;, regions = &quot;ca&quot;) For each map function, there is a logical flag add. By setting the flag to be true, we can add more details, such as colors and texts, to the existed plot. The following is an example. map(&quot;state&quot;, fill = FALSE) map(&#39;state&#39;, regions = c(&#39;texas&#39;, &#39;ca&#39;, &#39;utah&#39;), fill = TRUE, col = &#39;green&#39;, add = TRUE) map(&#39;state&#39;, regions = c(&#39;penn&#39;, &#39;new york&#39;), fill = TRUE, col = &#39;yellow&#39;, add = TRUE) map.text(&#39;state&#39;, regions = &#39;ca&#39;, labels =&quot;CA&quot;, add = TRUE) 40.0.3 3. tmap If you cannot find the map you are looking for in the maps package, or you are not satisfied with the limited functions it provided, you may consider using the tmap package instead. The tmap package saves data of the maps as ‘SpatialPolygonsDataFrame’, or say, an sp objects. Let’s take the world map as an example. As we can see, besides the geographical info, there are also some other existed variable included in the map data such as gdp and population. data(World) summary(World) ## iso_a3 name sovereignt ## AFG : 1 Afghanistan: 1 France : 3 ## AGO : 1 Albania : 1 Denmark : 2 ## ALB : 1 Algeria : 1 Israel : 2 ## ARE : 1 Angola : 1 United Kingdom : 2 ## ARG : 1 Antarctica : 1 United States of America: 2 ## ARM : 1 Argentina : 1 Afghanistan : 1 ## (Other):171 (Other) :171 (Other) :165 ## continent area pop_est pop_est_dens ## Africa :51 Min. : 2590 Min. :1.400e+02 Min. : 0.0003 ## Asia :47 1st Qu.: 42390 1st Qu.:3.442e+06 1st Qu.: 20.8203 ## Europe :39 Median : 183630 Median :9.036e+06 Median : 65.3438 ## North America:18 Mean : 814568 Mean :3.827e+07 Mean : 105.5498 ## South America:13 3rd Qu.: 622980 3rd Qu.:2.595e+07 3rd Qu.: 116.8924 ## Oceania : 7 Max. :16376870 Max. :1.339e+09 Max. :1198.8237 ## (Other) : 2 ## economy income_grp ## 1. Developed region: G7 : 7 1. High income: OECD :32 ## 2. Developed region: nonG7:32 2. High income: nonOECD:17 ## 3. Emerging region: BRIC : 4 3. Upper middle income :44 ## 4. Emerging region: MIKT : 4 4. Lower middle income :47 ## 5. Emerging region: G20 :19 5. Low income :37 ## 6. Developing region :66 ## 7. Least developed region :45 ## gdp_cap_est life_exp well_being footprint ## Min. : 300.5 Min. :48.91 Min. :2.867 Min. : 0.610 ## 1st Qu.: 2215.4 1st Qu.:65.04 1st Qu.:4.575 1st Qu.: 1.425 ## Median : 7394.8 Median :73.24 Median :5.200 Median : 2.604 ## Mean : 14977.1 Mean :70.80 Mean :5.412 Mean : 3.223 ## 3rd Qu.: 19193.8 3rd Qu.:76.93 3rd Qu.:6.300 3rd Qu.: 4.482 ## Max. :200000.0 Max. :83.24 Max. :7.800 Max. :15.820 ## NA&#39;s :1 NA&#39;s :41 NA&#39;s :41 NA&#39;s :41 ## inequality HPI geometry ## Min. :0.04322 Min. :12.78 MULTIPOLYGON :177 ## 1st Qu.:0.13931 1st Qu.:21.21 epsg:NA : 0 ## Median :0.21293 Median :26.29 +proj=eck4...: 0 ## Mean :0.23427 Mean :26.48 ## 3rd Qu.:0.32932 3rd Qu.:31.73 ## Max. :0.50734 Max. :44.71 ## NA&#39;s :41 NA&#39;s :41 Let’s try to draw a world map based on life expectation. tm_shape(World) + tm_polygons(&quot;life_exp&quot;) The tmap package plot the map in a way similar ggplot2, which means many functions are still workable in the map drawing, such as facet. You can also change the layout as you like. Here is an example. tm_shape(World) + tm_polygons(c(&quot;life_exp&quot;,&quot;economy&quot;)) + tm_facets(sync = TRUE, nrow = 2) + tm_layout(bg.color = &quot;#66CCFF&quot;) 40.0.4 4.Optional watching: create your own sf object The tmap package itself does not include enough sf object. So in application, you may have to create a new sf object yourself. This can be done by installing the sf package, using the sf_read function to introduce your shapefile to a new sf object, and combining it with your dataset. I find a 6-minute video on this topic which I think is very helpful, and I would post the link here. click "],["health-datasets-for-the-final-project.html", "Chapter 41 Health datasets for the final project 41.1 Big Cities Health Inventory Data 41.2 MHealth Dataset 41.3 Human Mortality Database (HMD) 41.4 SEER Cancer Incidence 41.5 UNICEF Data Warehouse", " Chapter 41 Health datasets for the final project Natasha McLeod With the onset of COVID earlier this spring, there may be interest in the use public health datasets for the final project. If so, below is a summary of five datasets that can be used and background on each dataset. 41.1 Big Cities Health Inventory Data An open data platform that provides a “snapshot” of health of the state of health in 30 of largest cities in the United States accross the country. Data is collected at the state, county and city levels as well as through several federal surveys. Drawbacks: most of data is only disaggregated at the the state or country level, not city level data. The indicators encompass 10 categories: 1) Behavioral Health and Substance Abuse; 2) Cancer; 3) Chronic Disease; 4) Environmental Health; 5) Food Safety; HIV/AIDs; 6) Infectious Disease; 7) Injury and Violence; 8) Maternal and Child Health; 9) demographics; and 10) life expectancy/overall death rate. bigcities_df &lt;- read.csv(&quot;https://bchi.bigcitieshealth.org/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBPUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--d420076690399fd28de697fa5a876e8b1d3188ac/BCHI-dataset_2019-03-04.csv?disposition=attachment&quot;) 41.2 MHealth Dataset Dataset comprising of bodymotion and vital signs for 10 volunteers of divese demographic backgrounds performing several physical activites. Sensors were placed on the subject’s chest, right wrist, and left ankle. Activities measued: L1: Standing still (1 min); L2: Sitting and relaxing (1 min); L3: Lying down (1 min); L4: Walking (1 min); L5: Climbing stairs (1 min);L6: Waist bends forward (20x);L7: Frontal elevation of arms (20x); L8: Knees bending (crouching) (20x); L9: Cycling (1 min); L10: Jogging (1 min); L11: Running (1 min); L12: Jump front &amp; back (20x) Be sure to reference the following sources which were used to build the dataset: 1) Banos, O., Garcia, R., Holgado, J. A., Damas, M., Pomares, H., Rojas, I., Saez, A., Villalonga, C. mHealthDroid: a novel framework for agile development of mobile health applications. Proceedings of the 6th International Work-conference on Ambient Assisted Living an Active Ageing (IWAAL 2014), Belfast, Northern Ireland, December 2-5, (2014). 2) Banos, O., Villalonga, C., Garcia, R., Saez, A., Damas, M., Holgado, J. A., Lee, S., Pomares, H., Rojas, I. Design, implementation and validation of a novel open framework for agile development of mobile health applications. BioMedical Engineering OnLine, vol. 14, no. S2:S6, pp. 1-20 (2015). The data can be found as zipfile here: (https://archive.ics.uci.edu/ml/machine-learning-databases/00319/) 41.3 Human Mortality Database (HMD) The team behind this database has created a COVID database of Short-term Mortality Fluctuations (STMF) data series. This data base includes weekly death counts for 36 countries: Austria, Belgium, Bulgaria, Chile, Canada, Croatia, Czech Republic, Denmark, England and Wales, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Israel, Italy, Latvia, Lithuania, Luxembourg, Netherlands, New Zealand, Northern Ireland, Norway, Poland, Portugal, Republic of Korea, Russia, Scotland, Slovenia, Slovakia, Spain, Sweden, Switzerland and the USA. Beware: As you may know, classifying a death as ‘COVID related’ varies by country. Something to considering during exploratory analysis. You can go to the follwoing link to find out more information about the data format and methods: (https://www.mortality.org/Public/STMF_DOC/STMFNote.pdf) Download the xlsx dataset here:(https://www.mortality.org/Public/STMF/Outputs/stmf.xlsx) 41.4 SEER Cancer Incidence U.S. government data about cancer incidence segmented by age, race, gender, year and other factors. Data source: National Cancer Institute’s Surveillance, Epidemiology, and End Results Program.\" Data goes back to 1975 with 18 databases. Link to SEER Explorer where you can download data: (https://seer.cancer.gov/explorer/application.html?site=1&amp;data_type=1&amp;graph_type=2&amp;compareBy=sex&amp;chk_sex_3=3&amp;chk_sex_2=2&amp;race=1&amp;age_range=1&amp;hdn_stage=101&amp;rate_type=2&amp;advopt_precision=1&amp;advopt_display=2) NB: For background information on the cancer data checkout the state cancer profiles at the National Cancer Institute: (https://statecancerprofiles.cancer.gov) 41.5 UNICEF Data Warehouse UNICEF Data Warehouse is the place to go for internationl health data. There are too many datasets to name here, but I have summarized a few below. For more information on the available datasets go to this link: (https://data.unicef.org/dv_index/) Datasets to look at: Child mortality: (https://data.unicef.org/resources/data_explorer/unicef_f/?ag=UNICEF&amp;df=GLOBAL_DATAFLOW&amp;ver=1.0&amp;dq=.PV_LEVEL..&amp;startPeriod=2008&amp;endPeriod=2018) Youth Literacy: (https://data.unicef.org/resources/data_explorer/unicef_f/?ag=UNICEF&amp;df=GLOBAL_DATAFLOW&amp;ver=1.0&amp;dq=.ED_15-24_LR..&amp;startPeriod=2008&amp;endPeriod=2018) Asylum seekers, by country: (https://data.unicef.org/resources/data_explorer/unicef_f/?ag=UNICEF&amp;df=GLOBAL_DATAFLOW&amp;ver=1.0&amp;dq=.MG_ASYLM_CNTRY_ASYLM..&amp;startPeriod=2007&amp;endPeriod=2017) Asylum seekers, by country of destination :(https://data.unicef.org/resources/data_explorer/unicef_f/?ag=UNICEF&amp;df=GLOBAL_DATAFLOW&amp;ver=1.0&amp;dq=.MG_ASYLM_CNTRY_DEST..&amp;startPeriod=2007&amp;endPeriod=2017) "],["laying-out-multiple-plots-for-baseplot-and-ggplot.html", "Chapter 42 Laying out multiple plots for Baseplot and ggplot 42.1 Overview 42.2 Most easy and normal form par() 42.3 Complex plot layouts with layout() 42.4 Layout for ggplot 42.5 Reference", " Chapter 42 Laying out multiple plots for Baseplot and ggplot Zhiyi Chen (zc2489) 42.1 Overview In the process of data analysis, single plot showing is likely to be a weak visual shock. Instead, sometimes several plots should be output which are used for comparison or more strong evidence to verify our ideas. How to lay out several plots seems to be a easy stuff, however, type of plotting, the scale of different plots and other details will make laying out multiple plots in one page difficult. In this tutorial, I will introduce some method from easy use to comprehensive use and pinpoint matters need attention to make the visualizaton logically and readable. 42.2 Most easy and normal form par() par() from package{graphics} Parameter value … In the form: Tag = value no.readonly TRUE/FALSE More Tag = value can be seen from this link: https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/par par() is a function to create a whole setting for plots. ... shows we can set parameters in form Tag = value, we take no.readonly is equal to TRUE when there is no input in ..., which means it will shows the previous par() settings. Of course, layout of multiple plots can be constructed within par(). The mfrow and mfcol parameters allow us to create a matrix of plots in one plotting space. Both parameters take a vector of length two as an argument, corresponding to the number of rows and columns in the resulting plotting matrix. For example, the following code sets up a 2 x 2 plotting matrix. The difference between mfrow and mfcol is that the former one is by row and the latter one is by column. # store the previous plot settings opar &lt;- par(no.readonly = TRUE) # set 2 x 2 layout par(mfrow = c(2,2)) # First plot: line chart plot(c(7,12,28,3,41), type = &#39;o&#39;) # Second plot: scatter plot plot(iris$Sepal.Length, iris$Sepal.Width, main = &#39;scatter plot&#39;, xlab = &#39;Sepal length&#39;, ylab = &#39;Sepal width&#39;) # Third plot: histgram chart hist(iris$Sepal.Length, main = &#39;Histgram chart&#39;, xlab = &#39;Sepal length&#39;) # Fourth plot: boxplot boxplot(Petal.Length ~ Species, data = iris) There is one thing need attention that from then on, all the layout will be 2x2 since par() is set for all plots. Otherwise we reset par() or recall the setting before par() par(opar) hist(rnorm(100)) One disadvantage for par() is that it cannot work for ggplot, we can see below that the plot should appear on the upper left of the page, but it just happen as if par() isn’t written here. However, don’t worry about it since ggplot has another function to lay out which is very similar to par() and we will discuss about it later. par(mfrow = c(2,2)) ggplot(data = iris, aes(Sepal.Length))+ geom_histogram(bins = 20) 42.3 Complex plot layouts with layout() layout() from package{graphics} Parameter value mat a matrix object specifying the location of the next figures on the output device. widths a vector of values for the widths of columns on the device. heights a vector of values for the heights of rows on the device. respect controls whether a unit column-width is the same physical measurement on the device as a unit row-height. layout is not a plotting parameter, rather it is a function all on its own. Different from par(), layout() can align position number to certain plot and define the width and height of plot area respectively. We can first have a look to understand how it works. layout.matrix &lt;- matrix(c(2, 1, 0, 3), nrow = 2, ncol = 2) layout(mat = layout.matrix, heights = c(1, 2), # Heights of the two rows widths = c(2, 2)) # Widths of the two columns layout.show(3) From above chart, 0 means there will be an empty area, 1-3 means the order of plot query, heights here means the length of second row will be 2 times the first row, widths means the length of first column and second column is the same. Moreover, heights and widths will use the same scale. Let use an example to show how layout() performs. The example is from : https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/layout # Get the data manually x &lt;- pmin(3, pmax(-3, stats::rnorm(50))) y &lt;- pmin(3, pmax(-3, stats::rnorm(50))) xhist &lt;- hist(x, breaks = seq(-3,3,0.5), plot = FALSE) yhist &lt;- hist(y, breaks = seq(-3,3,0.5), plot = FALSE) top &lt;- max(c(xhist$counts, yhist$counts)) xrange &lt;- c(-3, 3) yrange &lt;- c(-3, 3) # Create a multiple plot layout interface nf &lt;- layout(matrix(c(2,0,1,3),2,2,byrow = TRUE), c(3,1), c(1,3), TRUE) # plot 1 par(mar = c(3,3,1,1)) # mar = c(down, left, up, right) plot(x, y, xlim = xrange, ylim = yrange, xlab = &quot;&quot;, ylab = &quot;&quot;) # plot 2 par(mar = c(0,3,1,1)) barplot(xhist$counts, axes = FALSE, ylim = c(0, top), space = 0) # plot 3 par(mar = c(3,0,1,1)) barplot(yhist$counts, axes = FALSE, xlim = c(0, top), space = 0, horiz = TRUE) In this case, it is obvious to see the density distribution from two variables and also scatter plot can be seen which gives our a direct and reliable visual shock. It is no doubt that layout() is able to let us better understand data. 42.4 Layout for ggplot 42.4.1 grid.arrange() in gridExtra grid.arrange() from package{griddExtra} Parameter value … grobs, gtables, ggplot or trellis objects grobs list of grobs(graphical object) layout_matrix structure of layout widths c() heights c() ncol the number of columns nrow the number of rows More parameter description can be found: https://www.rdocumentation.org/packages/gridExtra/versions/2.3/topics/arrangeGrob grid.arrange is able to work for grobs, gtables, ggplots, the function and strategy is very similar to par(). ... means the plots we need to combine on a single page. widths,heights,ncol and nrow are the parameters we are very familiar with. Here we still use an example to show how it works. # prepare all ggplots p1 &lt;- qplot(Sepal.Length,Sepal.Width,data = iris,col=Species) p2 &lt;- qplot(Petal.Length, data = iris, binwidth = 0.1) + ggtitle(&quot;Histogram&quot;) p3 &lt;- ggplot(data = iris, aes(x = Species, y = Petal.Length)) + geom_boxplot() p4 &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(col = Species)) + facet_wrap( ~ Species, nrow = 1) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;facetted plot&quot;) # use grid.arrange() grid.arrange(p1,p2,p3,p4,nrow = 2, widths = c(1,1.5)) 42.4.2 method in grid The functions in grid also can be used for arranging plots and it is easy for us to align postion for certain graph. One thing worth attention is that we have to write grid.newpage() first to record all the graphs below. I use the example which is one of first questions in homework 3 to display the layout work. # plots pr &lt;- data.frame(penguins_raw) g1 &lt;- ggparcoord(pr, columns = c(10,11,12,13,15,16),alphaLines = 0.3, groupColumn = &quot;Species&quot;) + ggtitle(&quot;Clusters in Species&quot;) g2 &lt;- ggparcoord(pr, columns = c(10,11,12,13,15,16),alphaLines = 0.3, groupColumn = &quot;Island&quot;) + ggtitle(&quot;Clusters in Island&quot;) g3 &lt;- ggparcoord(pr, columns = c(10,11,12,13,15,16),alphaLines = 0.3, groupColumn = &quot;Clutch.Completion&quot;) + ggtitle(&quot;clusters in Clutch.Completion&quot;) g4 &lt;- ggparcoord(pr, columns = c(10,11,12,13,15,16),alphaLines = 0.3, groupColumn = &quot;Date.Egg&quot;) + ggtitle(&quot;Clusters in Date.Egg&quot;) g5 &lt;- ggparcoord(pr, columns = c(10,11,12,13,15,16),alphaLines = 0.3, groupColumn = &quot;Sex&quot;) # the function in grid to arrange the plots grid.newpage() pushViewport(viewport(layout = grid.layout(3, 2))) vplayout &lt;- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y) print(g1, vp = vplayout(1, 1:2)) print(g2, vp = vplayout(2, 1)) print(g3, vp = vplayout(2, 2)) print(g4, vp = vplayout(3, 1)) print(g5, vp = vplayout(3, 2)) 42.5 Reference More about par() and layout(): https://bookdown.org/ndphillips/YaRrr/arranging-plots-with-parmfrow-and-layout.html More about layout for ggplot: https://cran.r-project.org/web/packages/egg/vignettes/Ecosystem.html More about custom layout: https://blog.csdn.net/kMD8d5R/article/details/85182184 (Chinese version) "],["a-basic-introduction-to-markov-chain-monte-carlo-method-in-r.html", "Chapter 43 A basic Introduction to Markov Chain Monte Carlo Method in R 43.1 1. Introduction 43.2 2. Markov Chain Simulation Method 43.3 3. Acceptance-Rejection Sampling 43.4 4. Sampling from Markov Chain 43.5 5. MCMC Sampling Method 43.6 6. Metropolis-Hastings Sampling 43.7 7. Implementation in R 43.8 8 Refenence", " Chapter 43 A basic Introduction to Markov Chain Monte Carlo Method in R Qiran Li library(ggplot2) 43.1 1. Introduction It is often not possible to understand (or learn) complicated probability distribution by theoretical analysis. In that scenario, one convenient way to learn about the probability distribution is to simulate from that particular distribution. Markov Chain Monte Carlo (MCMC) is probably the most popular way for the simulation purpose. It has wide application in statistics, data science, and machine learning. In this tutorial, I would first explain the theory of MCMC, and then provide my own implementation of this method in R as well as useful graphs for explanation. The purpose of this community contribution tutorial is to help people understand this tough but useful method. As we can see from the name of MCMC method, it contains two parts. One is related to Monte Carlo Simulation (MC) Method; the other is associated with Markov chain. Therefore, It would be nature to discuss MCMC by first introducing these two related methods. 43.2 2. Markov Chain Simulation Method The name of Monte Carlo was derived from a casino. The earliest applications of Monte Carlo Method were designed to solve some complex summation or integration problems. Suppose we want to solve the integration problem: \\(\\int_{a}^{b} f(x) dx\\), but it’s hard to find the explicit form of \\(f(x)\\). One easy way of solving this problem is: find a \\(x_0\\) between a and b and use \\(f(x_0)\\) to represent all the values of \\(f(x)\\) inside a and b. Then the answer would be \\((b - a)f(x_0)\\). However, using one value to represent all the values between a and b is a broad assumption. We could also use n values \\((x_0,x_1,x_2,x_3,...,x_{n-1})\\) between a and b instead, thus the solution would be \\(\\frac{b-a}{n} \\sum_{i=0}^{n - 1} f(x_i)\\). Here we also made an assumption that x is uniformly distributed on [a,b]. If we know \\(x\\)’s distribution \\(p(x)\\) on [a,b], the answer would become \\(\\frac{1}{n} \\sum_{i=0}^{n - 1} \\frac {f(x_i)}{p(x_i)}\\). Therefore, as long as we know the distribution of x on the interval, we could get an accurate estimate of the integral. 43.3 3. Acceptance-Rejection Sampling If the probability distribution of {x} is found, we need to get n samples based on this probability distribution and bring them into the Monte Carlo Simulation Method formula to solve it. How do we get the n samples based on the probability distribution? For common distributions like Uniform, F, Beta, Gamma distributions, we could use random number generator to get samples. However, for complicated distribution \\(p(x)\\) that we could not directly sample, we need to use Acceptance-Rejection Sampling method. The process contains: Set a common probability distribution function \\(q(x)\\) that is convenient for sampling, set a constant \\(k\\) so that \\(p(x)\\) is always below \\(kq(x).\\) Get a sample \\(z_0\\) of \\(q(x)\\). Sample a value u from a uniform distribution \\((0, kq(z_0))\\). If u falls below \\(kq(z)\\) and above \\(p(z)\\), then reject this sampling, otherwise accept this sample. Repeat the above process to get n accepted samples \\(z_0\\), \\(z_1\\), … \\(z_{n-1}\\). Get into the formula \\(\\frac{1}{n} \\sum_{i=0}^{n - 1} \\frac {f(x_i)}{p(x_i)}\\). 43.4 4. Sampling from Markov Chain For a finite irreducible aperiodic Markov Chain, we have the following properties: \\(\\lim_{n\\to\\infty} P_{ij}^n = \\pi(j)\\), \\(\\pi(j) = \\sum_{i=0}^{\\infty} \\pi(i) P_{ij}\\), and \\(\\sum_{i=0}^{\\infty} \\pi(i) = 1\\). From these properties, we can see if we get the transition matrix of a Markov chain, we can easily get the samples from the stationary distribution. The procedure is as follow: Input the transition matrix P. Let \\(n_1\\) be the number of the transition times Let \\(n_2\\) be how many samples do we want Sampling from any simple probability distribution to get the initial state value for \\(t = 0\\) to \\(t = n_1 + n_2 - 1\\): get sample \\(x_{t+1}\\) from the conditional probability distribution \\(P(x|x_t)\\) output the sample \\(x_{n_1}, x_{n_1 + 1}, ..., x_{n_1 + n_2 - 1}\\) 43.5 5. MCMC Sampling Method From Sampling from Markov Chain, we know if we get the transition matrix of the Markov Chain, we could easily get the samples from distribution and put it into use of the Monte Carlo Method. However, given a detailed balance probability distribution \\(\\pi\\), it’s difficult to directly find associated Markov Chain probability transition matrix \\(P\\). So, here comes the MCMC Sampling Method. The definition of detailed balance condition: \\[ \\pi(i)P(i,j) = \\pi(j)P(j,i), \\forall{i,j}\\] In most of the cases, the detailed balance conditions would not hold. \\[ \\pi(i)Q(i,j) \\neq \\pi(j)Q(j,i)\\] So we need to construct a \\(\\alpha(i,j)\\) to fulfill the detailed balance condition, such that: \\[ \\pi(i)Q(i,j)\\alpha(i,j) = \\pi(j)Q(j,i)\\alpha(j,i)\\] Such \\(\\alpha(i,j)\\) and \\(\\alpha(j,i)\\) are easy to make by setting, \\[ \\alpha(i,j) = \\pi(j)Q(j,i)\\] and \\[ \\alpha(j,i) = \\pi(i)Q(i,j)\\] Here is the procedure of MCMC Sampling: 1. Input the transition matrix \\(Q\\) (randomly selected), stationary distribution \\(\\pi(x)\\) Let \\(n_1\\) be the number of the transition times Let \\(n_2\\) be sample size Sampling from any simple probability distribution to get the initial state value \\(x_0\\) for \\(t = 0\\) to \\(t = n_1 + n_2 - 1\\): get sample \\(x_{\\ast}\\) from the conditional probability distribution \\(Q(x|x_t)\\) Sampling \\(u\\) from \\(Uniform[0,1]\\) if \\(u &lt; \\alpha(x_t,x_{\\ast}) = \\pi(x_{\\ast})Q(x_{\\ast}, x_t)\\), let \\(x_{t+1} = x_{\\ast}\\); else \\(x_{t+1} = x_{t}\\) Output the sample \\(x_{n_1}, x_{n_1 + 1}, ..., x_{n_1 + n_2 - 1}\\) 43.6 6. Metropolis-Hastings Sampling MCMC has a hidden problem when the \\(\\alpha(x_t,x_{\\ast})\\) is too small. It could result in the fact that majority of our samplings are being rejected. This significantly reduces the efficiency of sampling. Metropolis-Hastings Sampling Method is designed to solve this problem. The major change is with related to the \\(\\alpha\\) value: \\[ \\alpha(i,j) = \\min\\{\\frac{\\pi(j)Q(j,i)}{\\pi(i)Q(i,j)} ,1\\} \\] The process is as follow: Input the transition matrix \\(Q\\) (randomly selected), stationary distribution \\(\\pi(x)\\) Let \\(n_1\\) be the number of the transition times Let \\(n_2\\) be sample size Sampling from any simple probability distribution to get the initial state value \\(x_0\\) for \\(t = 0\\) to \\(t = n_1 + n_2 - 1\\): get sample \\(x_{\\ast}\\) from the conditional probability distribution \\(Q(x|x_t)\\) Sampling \\(u\\) from \\(Uniform[0,1]\\) if \\(u &lt; \\alpha(x_t,x_{\\ast}) = \\min\\{\\frac{\\pi(j)Q(j,i)}{\\pi(i)Q(i,j)} ,1\\}\\), then set \\(x_{t+1} = x_{\\ast}\\); else \\(x_{t+1} = x_{t}\\) Output the sample \\(x_{n_1}, x_{n_1 + 1}, ..., x_{n_1 + n_2 - 1}\\) 43.7 7. Implementation in R 43.7.1 1. Sampling from an exponential distribution 43.7.1.1 a) Define the pdf of exponential distribution exp_dist = function(x){ if (x &lt; 0) { return (0) } else { return (exp(-x)) } } 43.7.1.2 b) Define the MCMC function MCMC = function(T, startval, sd, target){ x = rep(0,T) x[1] = startval for(t in 2:T) { pi_star = rnorm(1,x[t-1],sd) alpha = min(target(pi_star)/target(x[t-1]), 1) u = runif(1) if (u &lt; alpha) { x[t] = pi_star } else { x[t] = x[t -1] } } return(x) } 43.7.1.3 c) ggplot the curve z = MCMC(5000,3, 1, exp_dist) z = data.frame(z) ggplot(data=z,aes(x=z)) + geom_density(color=&quot;darkblue&quot;, fill=&quot;lightblue&quot;) + stat_function(fun = dexp, colour = &quot;red&quot;) + ylab(&quot;&quot;) We find the density function is similar to the true distribution. 43.7.2 2. Sampling from a normal distribution 43.7.2.1 a) Define the pdf of normal distribution norm_dist = function(x){ return (dnorm(x)) } 43.7.2.2 b) use previous function and validate the result z = MCMC(5000,3, 1, norm_dist) z = data.frame(z) ggplot(data=z,aes(x=z)) + geom_density(color=&quot;darkblue&quot;, fill=&quot;lightblue&quot;) + stat_function(fun = dnorm, colour = &quot;red&quot;) + ylab(&quot;&quot;) Again, we find the density function is similar to the true distribution. 43.8 8 Refenence https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo http://www.yaroslavvb.com/papers/peres-markov.pdf https://www.cnblogs.com/pinard/p/6638955.html https://blog.csdn.net/weixin_30745553/article/details/98648799 https://github.com/ljpzzz/machinelearning/blob/master/mathematics https://stephens999.github.io/fiveMinuteStats/MH-examples1.html "],["automate-eda-with-dataexplorer.html", "Chapter 44 automate eda with dataexplorer 44.1 Overview 44.2 DataExplorer 44.3 Installation 44.4 Exploratory data analysis (EDA) 44.5 Feature Engineering 44.6 Data Report 44.7 External Resources", " Chapter 44 automate eda with dataexplorer Jing You # library(&quot;devtools&quot;) # devtools::install_github(&quot;boxuancui/DataExplorer&quot;) library(alr4) library(DataExplorer) 44.1 Overview This section shows how to use package DataExplorer to automate Exploratory Data Analysis (EDA) and create Data Report. 44.2 DataExplorer DataExplorer attempts to automate exploratory data analysis (EDA) process and offer one-click report generation to show and visualize basics about a data set. It is a user-friendly and efficient tool for first-step analysis with visualization that can avoid time consuming manual coding. 44.3 Installation #devtools::install_github(&quot;boxuancui/DataExplorer&quot;) #library(DataExplorer) The dataset “sleep1” will be used for the following demonstration, which has both discrete and continuous variables. data(sleep1) 44.4 Exploratory data analysis (EDA) DataExplorer is a great tool for exploratory data analysis that it can handle most visualization plots without the need to manipulate data formats and data types. Every visualization plots can be done in one line function call instead of using different packages and functions. 44.4.1 Overall Information There are many plots available in DataExplorer for preliminary data analysis to help us better understand the data set. For example, plot_str function can be used to visualizing the basic structure about the data set with names and types specified. We can see the sleep dataset has 62 observtations, 10 variables (7 discrete and 3 continuous) ## View basic description for the data plot_str(sleep1,type=&quot;diagonal&quot;) plot_intro function can be used to describe the basic info about the data set including number of rows/cols/data type/missing values. The plot show the percentage of the data types and the comple rows. It also specify if there are column where all values are missing. ## View basic description for the data plot_intro(sleep1,title=&quot;Introduction of the sleep data&quot;) plot_missing function can further let us take a peek into the profile of the missing values. Specifically which variables has missing values and what is the corresponding proportion. ## View data profie for the missing data plot_missing(sleep1, title=&quot;Missing Data Profile&quot;) 44.4.2 Distribution DataExplorer provides bar chart, histogram, density plot, scatterplot and boxplot, etc for the exploration of the data distribution. Those function call on the whole dataset would only plot on the according discrete/continuous variables. Thus we do not need to specify the columns for plotting. Other than that, the arguments for the plot functions are mostly consistent with those in the ggplot functions that makes it easy to use. For example, the plot_bar function plots the distribution only for the discrete variables. ##`Life` distribution of all discrete variables plot_bar(sleep1, with = &quot;Life&quot;,title=&quot;Bar distribution in response to Life span&quot;) The plot_histogram function plots the distribution only for the continuous variables. plot_histogram(sleep1,title= &quot;Histogram Distribution of sleep data&quot;) The plot_qq function plots the qq-plot only for the continuous variables. ##View quantile-quantile plot of all continuous variables by feature `Life` plot_qq(sleep1,by=&#39;Life&#39;,title=&quot;QQ-plot in response to Life span&quot;) 44.4.3 Correlation Analysis DataExplorer provides correlation heatmap plot for all non-missing features. The heatmap can also be set to plot for continuous or discrete variables only. plot_correlation(na.omit(sleep1),title=&quot;Correlation heatmap of the sleep data&quot;) plot_correlation(na.omit(sleep1),type = &quot;c&quot;,,title=&quot;Correlation heatmap of the sleep data, continuous only&quot;) * For the discrete variable, the heatmap automatically done one hot encoding that provides further insights. plot_correlation(na.omit(sleep1),type = &quot;d&quot;,,title=&quot;Correlation heatmap of the sleep data,discrete only&quot;) 44.5 Feature Engineering Feature engineering is often needed in the data analysis process to transform data into better representative features. DataExplorer provides mutiple functions for feature engineering including missing value filling, sparse categories grouping, one hot encoding and feature transformation. The set_missing function can fill both the discrete and continuous variable with designated values in one line of code. 44.5.1 Missing value final_sleep &lt;- set_missing(sleep1, list(0L, &quot;unknown&quot;)) plot_missing(final_sleep) One hot encoding allows the categorical data to be more expressive. It can be done by the dummify function. 44.5.2 Dummy variable plot_str( list( &quot;original&quot; = final_sleep, &quot;dummified&quot; = dummify(final_sleep) ) ) 44.6 Data Report All the summary statistic and visualization plots of the data set can be organized into a data report in 1 step. The report automatically generates most visualization plots above. It is indeed a rough data profile but very useful for initial analysis and user-friendly for beginners #create_report(final_sleep) Data report sample This function is very powerful that it provides user the ability to configure based on the needs. Each section and the arguments can be rendered, reponse variable can also be added. For example, we can add boxplot and scatterplot to the report, set number of sampled row in the qq-plot, and set response variable to Life. config &lt;-configure_report( add_plot_boxplot = TRUE, add_plot_scatterplot = TRUE, &quot;plot_qq&quot; = list(sampled_rows = 1000L) ) #create_report(final_sleep,y =&quot;Life&quot;,config=config) 44.7 External Resources https://boxuancui.github.io/DataExplorer/index.html : DataExplorer Github Page https://rpubs.com/mark_sch7/DataExplorerPackage : package reflection "],["speed-up-in-r-programming.html", "Chapter 45 Speed up in r programming 45.1 Typical development cycle for computational statistics 45.2 Bytecode compilation 45.3 Rcpp 45.4 Parallel computing 45.5 Package development", " Chapter 45 Speed up in r programming Hongling Liu and Xinrui Zhang library(ggplot2) library(tidyverse) library(microbenchmark) library(compiler) library(Rcpp) library(parallel) It is well known that R is not a fast language, since R was deliberately designed to simplify our process of performing data analysis and statistics, rather than making life easier for our computers to process. While R is slow compared to some other programming languages, it’s fast enough for most of our purposes. The goal of our community contribution is to give you a deeper understanding of how we could yield the maximum efficiency when running R. We will first introduce the typical development cycle computational statistics with R and what is some of R’s performance characteristics, to help you understand how R codes are being executed. We will also show you two relatively easy ways to speed up code using the Rcpp package and parallel processing. 45.1 Typical development cycle for computational statistics First, let’s take a look at the usual steps of computational statistics with R, and what questions we need to ask ourselves for each step: Scientific planning: What experiments would verify/invalidate our hypotheses? What parameter settings should we consider? Code planning: What does the code need to do? How will the code fit together? What functions will be used? What are their inputs/outputs etc. Implementation: Prototype functions, classes, etc., partial documentation Write unit tests Implement code, run unit tests, debug Broader testing, more debugging Profile code, identify bottlenecks Optimize code Conduct experiments. Full documentation. 45.2 Bytecode compilation After profiling, what can we do to improve performance? Questions to ask ourselves: Are there obvious speedups? Are things being computed unnecessarily? Are you using a data.frame where you should be using a matrix etc. Look up your problem on internet (e.g., search for “lapply slow” or “speeding up lapply” etc.) Try the just-in-time (JIT) compiler. Consider re-writing some or all of the code in a compiled language (e.g., C/C++). Try parallelization. R typical execution: Since R 2.1.4, the compiler package by Luke Tierney is distributed with base R. compiler package compiles an R function into bytecode. 45.2.1 Example: summing a vector Brute-force for loop for summing a vector: sum_r &lt;- function(x) { sumx &lt;- 0.0 for (i in 1:length(x)) { sumx &lt;- sumx + x[i] } return(sumx) } sum_r ## function(x) { ## sumx &lt;- 0.0 ## for (i in 1:length(x)) { ## sumx &lt;- sumx + x[i] ## } ## return(sumx) ## } Run the code on 1e6 elements: library(microbenchmark) library(ggplot2) x = seq(from = 0, to = 100, by = 0.0001) microbenchmark(sum_r(x)) ## Unit: milliseconds ## expr min lq mean median uq max neval ## sum_r(x) 17.16882 18.28315 18.94692 18.72277 19.64432 23.61775 100 Let’s compile the function into bytecode sum_rc and benchmark again: library(compiler) sum_rc &lt;- cmpfun(sum_r) sum_rc ## function(x) { ## sumx &lt;- 0.0 ## for (i in 1:length(x)) { ## sumx &lt;- sumx + x[i] ## } ## return(sumx) ## } ## &lt;bytecode: 0x7fb8eb08f190&gt; Benchmark again: microbenchmark(sum_r(x), sum_rc(x)) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## sum_r(x) 17.13359 18.00938 18.72435 18.56270 19.52712 20.84430 100 a ## sum_rc(x) 16.99562 18.10622 18.68247 18.62024 19.32928 20.98843 100 a Before we look into the results, let’s look at the microbenchmark function. Here we use it as a more accurate replacement of the often seen system.time( ) expression to accurately measure the time it takes to evaluate expr. Since we are only measuring the performance of a very small piece of code, we could get the accurate running time in milliseconds(ms), microseconds (µs) or even nanoseconds (ns) by using microbenchmark. microbenchmark( ) runs each expression 100 times by default (it can be controlled by the times parameter). In the process, it also randomises the order of the expressions. It summarises the results with a minimum (min), lower quartile (lq), median, upper quartile (uq), and maximum (max). By default, microbenchmark( ) runs each expression 100 times (controlled by the times parameter). In the process, it also randomises the order of the expressions. It summarises the results with a minimum (min), lower quartile (lq), median, upper quartile (uq), and maximum (max). We could focus on the median and mean, and use the upper and lower quartiles (lq and uq) to get a feel for the variability. In our example, we can see that compiling into bytecode does not make a big difference, since according to microbenchmark(), the running time of our sum_r function is mostly (more than half) in the interquartile range(17.9565663,19.3889092), and after compiling, the interquartile range for sum_rc(x) is (17.828295,19.1148928), so we could say that compiling into bytecode does not help much. The reason behind this may be seen at the following code where we found out that the function sum_r is already compiled into bytecode before execution. sum_r ## function(x) { ## sumx &lt;- 0.0 ## for (i in 1:length(x)) { ## sumx &lt;- sumx + x[i] ## } ## return(sumx) ## } ## &lt;bytecode: 0x7fb92b4fcc18&gt; Let’s turn off JIT (just-in-time compilation), re-define the (same) sum_r function, and benchmark again: enableJIT(0) # set JIT leval to 0 ## [1] 3 sum_r &lt;- function(x) { sumx &lt;- 0.0 for (i in 1:length(x)) { sumx &lt;- sumx + x[i] } return(sumx) } microbenchmark(sum_r(x)) ## Unit: milliseconds ## expr min lq mean median uq max neval ## sum_r(x) 225.0473 232.7672 238.9171 236.0747 240.7473 362.7627 100 Now we witness the slowness of the un-compiled sum_r. Documentation of enableJIT: enableJIT enables or disables just-in-time (JIT) compilation. JIT is disabled if the argument is 0. If level is 1 then larger closures are compiled before their first use. If level is 2, then some small closures are also compiled before their second use. If level is 3 then in addition all top level loops are compiled before they are executed. JIT level 3 requires the compiler option optimize to be 2 or 3. The JIT level can also be selected by starting R with the environment variable R_ENABLE_JIT set to one of these values. Calling enableJIT with a negative argument returns the current JIT level. The default JIT level is 3. Since R 3.4.0 (Apr 2017), the JIT (‘Just In Time’) bytecode compiler is enabled by default at its level 3. If you create a package, then you automatically compile the package on installation by adding ByteCompile: true to the DESCRIPTION file. 45.3 Rcpp Now, we are going to introduce the package Rcpp which helps yield the maximum efficiency for R. And there is a learning source for whoever interested: Advanced R: https://adv-r.hadley.nz/rcpp.html We previously used compiler package to compile R code into bytecode, which is translated to machine code by interpreter during execution. However, A low-level language such as C, C++, and Fortran is compiled into machine code directly, making it easy for our computer to process the code, and Rcpp makes it very simple to connect C++ to R, where it allows us to write C++ functions in R, therefore yielding the maximum efficiency. 45.3.1 Use cppFunction Rcpp package provides a convenient way to embed C++ code in R code. library(Rcpp) cppFunction(&#39;double sum_c(NumericVector x) { int n = x.size(); double total = 0; for(int i = 0; i &lt; n; ++i) { total += x[i]; } return total; }&#39;) sum_c ## function (x) ## .Call(&lt;pointer: 0x10b252d20&gt;, x) Benchmark (1) compiled C++ function sum_c together with (2) R function sum_r, (3) compiled R function sum_rc, and (4) the sum function in base R: mbm &lt;- microbenchmark(sum_r(x), sum_rc(x), sum_c(x), sum(x)) mbm ## Unit: microseconds ## expr min lq mean median uq max neval cld ## sum_r(x) 223208.717 232181.391 239849.8980 238000.867 243239.4855 355158.096 100 c ## sum_rc(x) 16628.451 18010.128 18607.7822 18736.176 19229.1315 22454.226 100 b ## sum_c(x) 1005.198 1083.686 1150.0083 1165.044 1206.0155 1259.766 100 a ## sum(x) 780.448 916.362 952.2217 955.761 993.4495 1174.767 100 a autoplot(mbm) Remember we turned off JIT by enableGIT(0) earlier. This is a good example of where C++ is much more efficient than R. As shown by the above microbenchmark, sum_c() is competitive with the built-in (and highly optimised) sum(), while sum_r() is several orders of magnitude slower. Therefore,Rcpp package is definitely a great tool for us to improve the efficiency for our R code. 45.4 Parallel computing Now, we are going to discuss another way to get efficient with R – parallel computing: Fact: base R was designed to be single-threaded. Even you request a fancy instance with 96 vCPUs, running R code is just using 1/96th of its power. To perform multi-core computation in R: Option 1: Manually run multiple R sessions. Option 2: Make multiple system(\"Rscript\") calls. Typically automated by a scripting language (Python, Perl, shell script) or within R Option 3: Use package parallel. parallel package in R. Authors: Brian Ripley, Luke Tieney, Simon Urbanek. Included in base R since 2.14.0 (2011). Based on the snow (Luke Tierney) and multicore (Simon Urbanek) packages. To find the number of cores: library(parallel) detectCores() ## [1] 8 45.4.1 Simulation example Senario: Suppose we have a new method to calculate average, that is, to only choose primed-indexed number to calculate the average, and we would like to compute the average mean squared error (MSE) from both this new method and the classic method. Differently distributed random variables with different sample sizes will be tested. There are going to be many combinations of distribution and sample size. \\[ MSE = \\frac{\\sum_{r=1}^{\\text{rep}} (\\widehat \\mu_r - \\mu_{\\text{true}})^2}{\\text{rep}} \\] ## check if a given integer is prime isPrime = function(n) { if (n &lt;= 3) { return (TRUE) } if (any((n %% 2:floor(sqrt(n))) == 0)) { return (FALSE) } return (TRUE) } ## estimate mean only using observation with prime indices estMeanPrimes = function(x) { n &lt;- length(x) ind &lt;- sapply(1:n, isPrime) return (mean(x[ind])) } ## compare methods: sample avg and prime-indexed avg compare_methods &lt;- function(dist = &quot;gaussian&quot;, n = 100, reps = 100, seed = 123) { # set seed according to command argument `seed` set.seed(seed) # preallocate space to store estimators msePrimeAvg &lt;- 0.0 mseSamplAvg &lt;- 0.0 # loop over simulation replicates for (r in 1:reps) { # simulate data according to command arguments `n` and `distr` if (dist == &quot;gaussian&quot;) { x = rnorm(n) } else if (dist == &quot;t1&quot;) { x = rcauchy(n) } else if (dist == &quot;t5&quot;) { x = rt(n, 5) } else { stop(paste(&quot;unrecognized dist: &quot;, dist)) } # prime indexed mean estimator and classical sample average estimator msePrimeAvg &lt;- msePrimeAvg + estMeanPrimes(x)^2 mseSamplAvg &lt;- mseSamplAvg + mean(x)^2 } mseSamplAvg &lt;- mseSamplAvg / reps msePrimeAvg &lt;- msePrimeAvg / reps return(c(mseSamplAvg, msePrimeAvg)) } In order to indicate the option 2, we save the above codes in script [runSim.R]. 45.4.1.1 Option 2 this example is only good for linux system The [runSim.R] script to include arguments seed (random seed), n (sample size), dist (distribution) and rep (number of simulation replicates). When dist=\"gaussian\", generate data from standard normal; when dist=\"t1\", generate data from t-distribution with degree of freedom 1 (same as Cauchy distribution); when dist=\"t5\", generate data from t-distribution with degree of freedom 5. Calling runSim.R will (1) set random seed according to argument seed, (2) generate data according to argument dist, (3) compute the primed-indexed average estimator and the classical sample average estimator for each simulation replicate, (4) report the average mean squared error (MSE) The [autoSim.R] script to run simulations with combinations of sample sizes nVals = seq(100, 500, by=100) and distributions distTypes = c(\"gaussian\", \"t1\", \"t5\"). rep = 50 is used for MSE. cat resources/speed_up_in_r_programming/autoSim.R Download the [runSim.R] and [autoSim.R] scripts, and run the following codes, this will parallelly compute results from all combinations, and write output to appropriately named files. Rscript resources/speed_up_in_r_programming/autoSim.R seed=280 rep=50 45.4.1.2 Option 3 45.4.1.3 Serial code We need to loop over 3 generative models (distTypes) and 20 samples sizes (nVals). That are 60 embarssingly parallel tasks. seed = 280 reps = 500 nVals = seq(100, 1000, by = 50) distTypes = c(&quot;gaussian&quot;, &quot;t5&quot;, &quot;t1&quot;) This is the serial code that double-loop over combinations of distTypes and nVals: ## simulation study with combination of generative model `dist` and ## sample size `n` (serial code) simres1 = matrix(0.0, nrow = 2 * length(nVals), ncol = length(distTypes)) i = 1 # entry index system.time( for (dist in distTypes) { for (n in nVals) { #print(paste(&quot;n=&quot;, n, &quot; dist=&quot;, dist, &quot; seed=&quot;, seed, &quot; reps=&quot;, reps, sep=&quot;&quot;)) simres1[i:(i + 1)] = compare_methods(dist, n, reps, seed) i &lt;- i + 2 } } ) ## user system elapsed ## 26.774 0.050 26.840 simres1 ## [,1] [,2] [,3] ## [1,] 0.0103989436 0.017070603 312.4001 ## [2,] 0.0410217819 0.066503177 200.3237 ## [3,] 0.0065484669 0.011260420 173.9631 ## [4,] 0.0297390639 0.047465397 199.5330 ## [5,] 0.0056445593 0.007754855 68026.7023 ## [6,] 0.0215380206 0.040039145 1230343.1341 ## [7,] 0.0040803523 0.006871930 43609.7815 ## [8,] 0.0165144049 0.032353077 931755.3182 ## [9,] 0.0032566766 0.005417194 30283.1286 ## [10,] 0.0161191554 0.026330133 684726.8788 ## [11,] 0.0027565672 0.004444172 22306.1369 ## [12,] 0.0145039253 0.022820075 539105.3392 ## [13,] 0.0024915830 0.003798500 17119.0807 ## [14,] 0.0122801335 0.022788299 435528.4778 ## [15,] 0.0023706676 0.003360507 13531.4104 ## [16,] 0.0112703627 0.016674640 111.4673 ## [17,] 0.0020190283 0.003147367 10973.3489 ## [18,] 0.0106157492 0.016027485 278.9663 ## [19,] 0.0017567901 0.002863640 9069.6647 ## [20,] 0.0096185720 0.016444671 261373.7646 ## [21,] 0.0016441481 0.002637964 7629.9867 ## [22,] 0.0081784426 0.013710539 296.6235 ## [23,] 0.0015075246 0.002498450 6498.2362 ## [24,] 0.0088018140 0.012909942 191986.6388 ## [25,] 0.0014372130 0.002308089 5603.9395 ## [26,] 0.0077292632 0.012789483 171280.5448 ## [27,] 0.0012924543 0.002216936 4889.8739 ## [28,] 0.0069012154 0.011052562 170.3975 ## [29,] 0.0011994654 0.001987311 4299.6838 ## [30,] 0.0067559611 0.011291788 178.7454 ## [31,] 0.0011642413 0.001888637 3806.8472 ## [32,] 0.0070131993 0.010048327 140.2389 ## [33,] 0.0011566121 0.001873365 3401.9766 ## [34,] 0.0065066558 0.009020973 34.1644 ## [35,] 0.0010506067 0.001595430 3049.0432 ## [36,] 0.0060026682 0.010338424 103578.0598 ## [37,] 0.0009770234 0.001618095 2768.4517 ## [38,] 0.0054705674 0.009229294 143.5544 45.4.1.4 Using mcmapply Run the same task using mcmapply function (parallel analog of mapply) in the parallel package: ## simulation study with combination of generative model `dist` and ## sample size `n` (parallel code using mcmapply) library(parallel) system.time({ simres2 &lt;- mcmapply(compare_methods, rep(distTypes, each = length(nVals), times = 1), rep(nVals, each = 1, times = length(distTypes)), reps, seed, mc.cores = 4) }) ## user system elapsed ## 21.338 0.389 7.736 simres2 &lt;- matrix(unlist(simres2), ncol = length(distTypes)) simres2 ## [,1] [,2] [,3] ## [1,] 0.0103989436 0.017070603 312.4001 ## [2,] 0.0410217819 0.066503177 200.3237 ## [3,] 0.0065484669 0.011260420 173.9631 ## [4,] 0.0297390639 0.047465397 199.5330 ## [5,] 0.0056445593 0.007754855 68026.7023 ## [6,] 0.0215380206 0.040039145 1230343.1341 ## [7,] 0.0040803523 0.006871930 43609.7815 ## [8,] 0.0165144049 0.032353077 931755.3182 ## [9,] 0.0032566766 0.005417194 30283.1286 ## [10,] 0.0161191554 0.026330133 684726.8788 ## [11,] 0.0027565672 0.004444172 22306.1369 ## [12,] 0.0145039253 0.022820075 539105.3392 ## [13,] 0.0024915830 0.003798500 17119.0807 ## [14,] 0.0122801335 0.022788299 435528.4778 ## [15,] 0.0023706676 0.003360507 13531.4104 ## [16,] 0.0112703627 0.016674640 111.4673 ## [17,] 0.0020190283 0.003147367 10973.3489 ## [18,] 0.0106157492 0.016027485 278.9663 ## [19,] 0.0017567901 0.002863640 9069.6647 ## [20,] 0.0096185720 0.016444671 261373.7646 ## [21,] 0.0016441481 0.002637964 7629.9867 ## [22,] 0.0081784426 0.013710539 296.6235 ## [23,] 0.0015075246 0.002498450 6498.2362 ## [24,] 0.0088018140 0.012909942 191986.6388 ## [25,] 0.0014372130 0.002308089 5603.9395 ## [26,] 0.0077292632 0.012789483 171280.5448 ## [27,] 0.0012924543 0.002216936 4889.8739 ## [28,] 0.0069012154 0.011052562 170.3975 ## [29,] 0.0011994654 0.001987311 4299.6838 ## [30,] 0.0067559611 0.011291788 178.7454 ## [31,] 0.0011642413 0.001888637 3806.8472 ## [32,] 0.0070131993 0.010048327 140.2389 ## [33,] 0.0011566121 0.001873365 3401.9766 ## [34,] 0.0065066558 0.009020973 34.1644 ## [35,] 0.0010506067 0.001595430 3049.0432 ## [36,] 0.0060026682 0.010338424 103578.0598 ## [37,] 0.0009770234 0.001618095 2768.4517 ## [38,] 0.0054705674 0.009229294 143.5544 We see roughly 3x-4x speedup with mc.cores=4. mcmapply, mclapply and related functions rely on the forking capability of POSIX operating systems (e.g. Linux, MacOS) and is not available in Windows. parLapply, parApply, parCapply, parRapply, clusterApply, clusterMap, and related functions create a cluster of workers based on either socket (default) or forking. Socket is available on all platforms: Linux, MacOS, and Windows. 45.4.1.5 Using clusterMap The same simulation example using clusterMap function: cl &lt;- makeCluster(getOption(&quot;cl.cores&quot;, 4)) clusterExport(cl, c(&quot;isPrime&quot;, &quot;estMeanPrimes&quot;, &quot;compare_methods&quot;)) system.time({ simres3 &lt;- clusterMap(cl, compare_methods, rep(distTypes, each = length(nVals), times = 1), rep(nVals, each = 1, times = length(distTypes)), reps, seed, .scheduling = &quot;dynamic&quot;) }) ## user system elapsed ## 0.019 0.005 5.706 simres3 &lt;- matrix(unlist(simres3), ncol = length(distTypes)) stopCluster(cl) simres3 ## [,1] [,2] [,3] ## [1,] 0.0103989436 0.017070603 312.4001 ## [2,] 0.0410217819 0.066503177 200.3237 ## [3,] 0.0065484669 0.011260420 173.9631 ## [4,] 0.0297390639 0.047465397 199.5330 ## [5,] 0.0056445593 0.007754855 68026.7023 ## [6,] 0.0215380206 0.040039145 1230343.1341 ## [7,] 0.0040803523 0.006871930 43609.7815 ## [8,] 0.0165144049 0.032353077 931755.3182 ## [9,] 0.0032566766 0.005417194 30283.1286 ## [10,] 0.0161191554 0.026330133 684726.8788 ## [11,] 0.0027565672 0.004444172 22306.1369 ## [12,] 0.0145039253 0.022820075 539105.3392 ## [13,] 0.0024915830 0.003798500 17119.0807 ## [14,] 0.0122801335 0.022788299 435528.4778 ## [15,] 0.0023706676 0.003360507 13531.4104 ## [16,] 0.0112703627 0.016674640 111.4673 ## [17,] 0.0020190283 0.003147367 10973.3489 ## [18,] 0.0106157492 0.016027485 278.9663 ## [19,] 0.0017567901 0.002863640 9069.6647 ## [20,] 0.0096185720 0.016444671 261373.7646 ## [21,] 0.0016441481 0.002637964 7629.9867 ## [22,] 0.0081784426 0.013710539 296.6235 ## [23,] 0.0015075246 0.002498450 6498.2362 ## [24,] 0.0088018140 0.012909942 191986.6388 ## [25,] 0.0014372130 0.002308089 5603.9395 ## [26,] 0.0077292632 0.012789483 171280.5448 ## [27,] 0.0012924543 0.002216936 4889.8739 ## [28,] 0.0069012154 0.011052562 170.3975 ## [29,] 0.0011994654 0.001987311 4299.6838 ## [30,] 0.0067559611 0.011291788 178.7454 ## [31,] 0.0011642413 0.001888637 3806.8472 ## [32,] 0.0070131993 0.010048327 140.2389 ## [33,] 0.0011566121 0.001873365 3401.9766 ## [34,] 0.0065066558 0.009020973 34.1644 ## [35,] 0.0010506067 0.001595430 3049.0432 ## [36,] 0.0060026682 0.010338424 103578.0598 ## [37,] 0.0009770234 0.001618095 2768.4517 ## [38,] 0.0054705674 0.009229294 143.5544 Again we see roughly 3x-4x speedup by using 4 cores. clusterExport copies environment of master to slaves. 45.5 Package development Learning resources: Book _R Packages_ by Hadley Wickham RStudio tutorial: https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio "],["data-visualization-in-python-using-different-plotting-packages.html", "Chapter 46 Data Visualization in Python using different plotting packages", " Chapter 46 Data Visualization in Python using different plotting packages Yuxin Zhou and Siyu Duan https://nbviewer.jupyter.org/github/yuxinzhou0312/cc/blob/main/EDA_CC11.ipynb "],["final-project-teammate-finder.html", "Chapter 47 Final project teammate finder", " Chapter 47 Final project teammate finder Lei Liu To help overcome communication challenges with online learning, I designed a survey called “EDAV Final Project Teammate Finder” (see link below), which served as an information-sharing platform for students to find their teammates and form final project groups based on their interest. Survey: https://github.com/superpowergirl/EDAVtest/blob/main/EDAV%20Final%20Project%20Teammate%20Finder%20Survey.pdf "],["among-us-player-statistics.html", "Chapter 48 Among Us Player Statistics", " Chapter 48 Among Us Player Statistics Myles Ingram This R code runs player level and community level statistics on Among Us Player data (which can be found on the home page of the app). To use this script, add your name and a userid to the among_us_player_stats.csv along with your player stats. Then enter your username and userID into the code, run the script and take a look! This code will use my username as an example. plyr_stats &lt;- read_csv(&quot;resources/among_us_stats/among_us_player_stats.csv&quot;) Username = &quot;Big Cheese&quot; UserID = &quot;1&quot; The code calculates extra statistics from the base ones including win percentage as crewmate/imposter, how many tasks you complete per crew mate game, rate of kills per imposter game, etc. The player level statistics are broken up into three separate different bar graphs for the sake of interpretability. Count data, Rate data, and Percentage data have been separated so that they can be viewed in the right perspective. In each graph, the data associated with imposter and crewmate have been color-coded while the other variables are labeled NA because they are applicable to both roles. ggplot(tidy_df_num) + geom_bar(aes(x = variable, y=value, fill=role), stat=&quot;identity&quot;)+ geom_text(aes(variable, value, label=round(value, 3)), position=position_dodge(width=0.9), vjust=-.25, size=3) + theme(axis.text.x = element_text(angle=60, hjust=1)) + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;), na.value = &quot;purple&quot;, name=&quot;Role&quot;, breaks=c(&quot;crw&quot;, &quot;imp&quot;, NA), labels=c(&quot;Crewmate&quot;, &quot;Imposter&quot;, &quot;NA&quot;)) + ggtitle(paste(user_df$Username,&quot;Count Player Data&quot;)) ggplot(tidy_df_rate) + geom_bar(aes(x = variable, y=value, fill=role), stat=&quot;identity&quot;)+ geom_text(aes(variable, value, label=round(value, 3)), position=position_dodge(width=0.9), vjust=-.25, size=3) + theme(axis.text.x = element_text(angle=60, hjust=1)) + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;), na.value = &quot;purple&quot;, name=&quot;Role&quot;, breaks=c(&quot;crw&quot;, &quot;imp&quot;, NA), labels=c(&quot;Crewmate&quot;, &quot;Imposter&quot;, &quot;NA&quot;)) + ggtitle(paste(user_df$Username,&quot;Rate Player Data&quot;)) ggplot(tidy_df_perc) + geom_bar(aes(x = variable, y=value, fill=role), stat=&quot;identity&quot;)+ geom_text(aes(variable, value, label=round(value, 3)), position=position_dodge(width=0.9), vjust=-.25, size=3) + theme(axis.text.x = element_text(angle=60, hjust=1)) + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;), na.value = &quot;purple&quot;, name=&quot;Role&quot;, breaks=c(&quot;crw&quot;, &quot;imp&quot;, NA), labels=c(&quot;Crewmate&quot;, &quot;Imposter&quot;, &quot;NA&quot;))+ ggtitle(paste(user_df$Username,&quot;Percentage Player Data&quot;)) The following graphs are for the population level data. The graphs in this section explore the relationship between the variables in the dataset on a population wide level through barcharts and scatterplots. # for population level statistics plyr_stats_2 &lt;- plyr_stats %&gt;% mutate(`Mobile or CPU?`=recode(`Mobile or CPU?`, &quot;Mobile&quot;=1, &quot;CPU&quot; = 2)) plyr_stats_2 &lt;- plyr_stats_2 %&gt;% dplyr::rename( TDRG = `Tasks Done Per Game`, CTP = `Completed Tasks Percentage`, IWP = `Imposter Win Percentage`, CWP = `Crewmate Win Percentage`, CP = `Crewmate Percentage`, IP = `Imposter Percentage`, KPR = `Kills Per Round`, WP = `Win Percentage` ) crw_scatmat &lt;- plyr_stats_2 %&gt;% dplyr::select(`Mobile or CPU?`, `TDRG`, CTP, `Death Percentage`, `CP`, `CWP`, `Bodies Reported`, `WP`, `Games Finished`) pairs(crw_scatmat[, 2:9], col=ifelse(crw_scatmat$`Mobile or CPU?`== 1, &quot;black&quot;, &quot;red&quot;)) #plot(crw_scatmat) imp_scatmat &lt;- plyr_stats_2 %&gt;% dplyr::select(`Mobile or CPU?`, `IWP`, `IP`, `WP`, `Emergencies Called`, `CWP`, `Bodies Reported`, `Games Finished`) pairs(imp_scatmat[, 2:8], col=ifelse(imp_scatmat$`Mobile or CPU?`== 1, &quot;black&quot;, &quot;red&quot;)) #plot(imp_scatmat) TDRG = Tasks Done Per Game, CTP = Completed Tasks Percentage, IWP = Imposter Win Percentage, CWP = Crewmate Win Percentage, CP = Crewmate Percentage, IP = Imposter Percentage, KPR = Kills Per Round, WP = Win Percentage, Black = CPU, Red = Mobile While more data is needed to draw strong conclusions about these scatterplots, a couple of interesting observations include: Players that play on mobile have a higher crewmate win percentage than CPU players Crewmate win percentage seems correlated with overall win percentage Bodies reported also seems to have a weak correlation with emergencies called Mobile players also have a higher tasks done per game and completed tasks per game CPU players and Mobile players are equally likely to be imposter/crewmate ggplot(data=plyr_stats, aes(`Mobile or CPU?`, fill=`Mobile or CPU?`))+ geom_bar() + ggtitle(&quot;Mobile vs CPU Players&quot;) ggplot(data=plyr_stats, aes(Color))+ geom_bar() + ggtitle(&quot;Player Colors&quot;) More data to come for both of these graphs Github for the project https://github.com/Ingrammyles8/among_us_cc "],["ipywidget-example-walkthrough-video.html", "Chapter 49 Ipywidget example walkthrough video", " Chapter 49 Ipywidget example walkthrough video Weiyao Xie Ipywidget can be used to implement html widgets in jupyter notebook which enable easier interactions with dataset. I want to do this video tutorial because I found the official documentation was not very intuitive and lacked example demonstration. The video tutorial can be found here. Hope this can be helpful. "],["github-initial-setup.html", "Chapter 50 Github Initial Setup", " Chapter 50 Github Initial Setup Create a repo called “CC20” by following the instructions written in the bookdown-template. In index.Rmd, edit the title, github-repo and add description. In _bookdown.yml, edit relevant github information. In index.Rmd, include “Chapter 1: Instructions” similar to CC19. Add “Chapter 2: The Sample Project” by creating sample_project.Rmd. In _bookdown.yml, define our own order of Rmd files for the book in a field named rmd_files. Without rmd_files, the table of content of the book would be ordered alphabetically by the title of each Rmd file. Please read bookdown Usage for further reference. "],["tutorial-for-pull-request-mergers.html", "Chapter 51 Tutorial for Pull Request Mergers 51.1 Check branch 51.2 Examine files that were added or modified 51.3 Check .Rmd filename 51.4 Check .Rmd file contents 51.5 Request changes 51.6 Merge the pull request", " Chapter 51 Tutorial for Pull Request Mergers 51.1 Check branch PR should be submitted from a non-master branch. If PR was submitted from the master branch, you can link to this explanation of what to do to fix it: https://edav.info/github#fixing-mistakes. 51.2 Examine files that were added or modified There should be only ONE .Rmd file. All of the additional resources should be in the resources/&lt;project_name&gt;/ folder. There should be no other files in the root directory besides the .Rmd file. 51.3 Check .Rmd filename The .Rmd filename should be words only and joined with underscores, no white space. (Update: It does not need to be the same as the branch name.) The .Rmd filename can only contain lowercase letters. (Otherwise the filenames do not sort nicely on the repo home page.) 51.4 Check .Rmd file contents The file should not contain a YAML header nor a --- line. The second line should be blank, followed by the author name(s). The first line should start with a single hashtag #, followed by a single whitespace, and then the title. There should be no additional single hashtag headers in the chapter. (If there are, new chapters will be created.) NEW: Other hashtag headers should not be followed by numbers since the hashtags will create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading. If the file contains a setup chunk in .Rmd file, it should not contain a setup label. (The bookdown render will fail if there are duplicate chunk labels.) i.e. use {r, include=FALSE} instead of {r setup, include=FALSE}. See sample .Rmd Links to internal files must contain resources/&lt;project_name&gt;/ in the path, such as: ![Test Photo](resources/sample_project/election.jpg) The file should not contain any install.packages(), write functions, setwd(), or getwd(). If there’s anything else that looks odd but you’re not sure, assign jtr13 to review and explain the issue. 51.5 Request changes If there are problems with any of the checks listed above, explain why the pull request cannot be merged and request changes by following these steps: Then, add a changes requested label to this pull request. Your job for this pull request is done for now. Once contributors fix their requests, review again and either move forward with the merge or explain what changes still need to be made. 51.6 Merge the pull request If all is good to go, it’s time to merge the pull request. There are several steps. 51.6.1 Add chapter filename to _bookdown.yml in PR’s branch To access the PR branch: Make sure you are on the PR branch by checking that the PR branch name is shown (not master): Open the _bookdown.yml file. NEW: delete everything in the file beginning with rmd_files: [ and then add the name of the new file in single quotes followed by a comma: Why? Because it will be easier to fix the merge conflicts this way. (A better way to do this is to merge master into the PR branch before adding the new file but this can’t be done on GitHub. If there’s interest I will explain how to do this locally.) Save the edited version. Click the resolve conflicts button: Cut the new filename and paste it into the proper location. Then delete the lines with &lt;&lt;&lt;&lt;&lt;&lt;&lt; xxxx, ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; master. In short, the file should look correct when you’re done. Click the “Marked as resolved” button and then the green “Commit merge” button. 51.6.2 Add chapter names to .Rmd for every first article in each chapter (Chapter Organization) Only do this if you are adding the first chapter in a PART. For every first article of each part, add the chapter name on the top of the .Rmd file, then propose changes. The example is like this. 51.6.3 Merge PR and leave a comment Now comes the final step. If you’re not sure that you did things correctly, assign one of the PR merge leaders or @jtr13 to review before you merge the PR. Go back to the conversation tab of the pull requests page, for example: https://github.com/jtr13/cc20/pull/23#issuecomment-728506101 Leave comments for congratulations 🎉 (type :tada:) and then click on the green button for merge. "],["chapter-organization.html", "Chapter 52 Chapter Organization 52.1 Process for Organizing Chapters 52.2 Initial Book Parts", " Chapter 52 Chapter Organization Yibai Liu and Colin Payne-Rogers This appendix contains information on how chapters in the cc20 book are organized. 52.1 Process for Organizing Chapters Each pull request that contains a chapter will be marked with at least one label prefixed with chap: #. This identifies which part of the book it should be added to. Additional labels may be added by chapter organizers or pull request reviewers. The goal for these is to add additional context to each chapter pull request. For instance, a “Python vs. R” cheat sheet and a “Mosaic Plot” cheat sheet will have different chap: labels, but the same cheatsheet label. These labels may help the chapter organizers re-organize chapters at a later date, if any initial chapters become bloated with contributions :) The chapter organizers will request that pull requests are updated so that the _bookdown.yml chapter list is in accordance with the chapter organization. 52.2 Initial Book Parts These are subject to change based on the content of the classes contributions. They are: Data Processing and Wrangling Data Visualization Reporting Results Complete Analyses Translations Live Tutorials Other Topics 52.2.1 Motivation The chapters are organized with a real-world data pipeline in mind. First, an analyst needs to obtain and organize the data they hope to explore and visualize (“data processing and wrangling”). This may include contributions related to tidying data, handling missing data, web scraping, and more. Data in hand, our analyst will want to lean on data visualization tools, like those from this class, first for the exploratory data analysis and later for visualization aimed at their audience, customer, etc. These contributions are captured in “data visualization” but may become two sections if enough pull requests fall into each category. The “final” step in a data analysis pipeline is to share the results of an analysis with interested third parties. This might be through a website build in R with Shiny, or using D3, or with a platform like Tableau. Community contributions that focus on how to format charts for an audience, rather than how to create them in R, may belong here in “Reporting Results.” At this point, readers of our “book” may be ready to appreciate the hard work that goes into “Complete Analyses” - those community contributions that find or import data, tidy and format it, explore and analyze it, and present conclusions to readers belong in the fourth section. 52.2.2 Additional Categories A careful reader will notice that sections 5 and 6 are missing from the chapter organization motivation. That’s because these categories, “Translations,” “Live Tutorials,” and “Other Topics,” don’t fit neatly into the data pipeline heuristic. Chapters that are labeled and initially organized into those sections may find that that’s where they belong, or they may have a new home when future pull requests update the chapter organization based on the community contributions we’ve received. "]]
